# Configuration Reference

`finetune-cli` uses YAML (or JSON) config files for reproducible, shareable training runs. Every flag available on the CLI maps to a field in the config file.

---

## Full config structure

```yaml
model:
  name: "gpt2"                 # HuggingFace model id
  device: "auto"               # auto | cpu | cuda | mps
  torch_dtype: "float32"       # float32 | float16 | bfloat16 | auto
  load_in_4bit: false          # QLoRA — quantize base model
  load_in_8bit: false          # 8-bit inference mode
  trust_remote_code: false

dataset:
  source: "local_file"         # local_file | huggingface_hub
  path: "./data/train.jsonl"   # file path or HF dataset id
  split: "train"
  max_samples: 1000            # null = use all
  shuffle: true
  seed: 42
  text_columns: null           # null = auto-detect

tokenization:
  max_length: 512
  truncation: true
  padding: "max_length"        # max_length | longest | do_not_pad
  add_special_tokens: true
  return_attention_mask: true

training:
  method: "lora"               # lora | qlora
  output_dir: "./output"
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 0.0002
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  fp16: false
  bf16: false
  logging_steps: 10
  save_strategy: "epoch"       # no | epoch | steps
  evaluation_strategy: "no"    # no | epoch | steps
  gradient_checkpointing: false
  seed: 42

lora:
  r: 8                         # rank — higher = more capacity + VRAM
  lora_alpha: 32               # scaling factor (typical: 2× r)
  lora_dropout: 0.1
  target_modules: null         # null = auto-detect
  bias: "none"                 # none | all | lora_only
  init_lora_weights: true

evaluation:                    # optional
  metrics:
    - "rougeL"
    - "bleu"
  batch_size: 8
  num_samples: 100
  generation_max_length: 100
  generation_temperature: 0.7
  generation_top_p: 0.9
  generation_do_sample: true
```

---

## LoRA parameter guide

### Rank (`r`)

Controls how many parameters LoRA adds. Higher rank = more expressive but more VRAM.

| Rank | Parameters added | VRAM overhead | Best for |
|------|-----------------|---------------|----------|
| 4 | ~0.1% | Minimal | Quick experiments |
| 8 | ~0.2% | Low | General purpose (default) |
| 16 | ~0.4% | Medium | Domain adaptation |
| 32+ | ~0.8%+ | High | Maximum adaptation |

### Alpha (`lora_alpha`)

Scaling factor for the LoRA update. Rule of thumb: **alpha = 2× r**.

```yaml
# Conservative (less aggressive update)
lora_alpha: 16  # with r: 8

# Standard
lora_alpha: 32  # with r: 8

# Aggressive (more aggressive update, may need lower lr)
lora_alpha: 64  # with r: 8
```

### Target modules

Set to `null` for auto-detection. Override when auto-detection misses layers:

```yaml
# GPT-2
target_modules: ["c_attn", "c_proj"]

# LLaMA / Mistral
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

# BERT
target_modules: ["query", "value"]
```

---

## Configuration recipes

### Quick experiment (CPU-friendly)

```yaml
model:
  name: "gpt2"
  torch_dtype: "float32"
dataset:
  max_samples: 500
tokenization:
  max_length: 256
training:
  num_epochs: 1
  batch_size: 2
  gradient_accumulation_steps: 2
lora:
  r: 4
  lora_alpha: 8
```

### Balanced quality (8GB GPU)

```yaml
model:
  name: "gpt2-medium"
  torch_dtype: "float16"
training:
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  fp16: true
lora:
  r: 8
  lora_alpha: 32
```

### Large model on limited VRAM (QLoRA)

```yaml
model:
  name: "meta-llama/Llama-3.2-1B"
  load_in_4bit: true
  torch_dtype: "float16"
training:
  method: "qlora"
  batch_size: 2
  gradient_accumulation_steps: 8
  fp16: true
  gradient_checkpointing: true
lora:
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
```

---

---

## DPO configuration

DPO (Direct Preference Optimization) requires an extra `beta` parameter and a dataset with `prompt`, `chosen`, and `rejected` columns.

```yaml
dataset:
  source: local_file
  path: ./data/dpo_sample.jsonl  # generated by examples/generate_sample_data.py
  max_samples: 200

training:
  method: dpo
  output_dir: ./outputs/gpt2_dpo
  num_epochs: 1
  batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 5.0e-5

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: null   # auto-detect
```

### `beta` parameter

`beta` is not in the YAML config — it uses the DPOTrainer Python default of `0.1`. To customise it, use the Python API:

```python
from finetune_cli.trainers import DPOTrainer
trainer = DPOTrainer(model, tokenizer, training_config, lora_config, beta=0.2)
```

Lower beta (0.05–0.1) keeps the model close to the reference; higher (0.3–0.5) applies stronger preference shaping.

### Dataset columns

| Column | Type | Required |
|--------|------|---------|
| `prompt` | string | Yes |
| `chosen` | string | Yes |
| `rejected` | string | Yes |

Generate offline sample data:
```bash
python examples/generate_sample_data.py
# Creates data/dpo_sample.jsonl (200 rows)
```

**Requires:** `pip install "finetune-cli[dpo]"` or `pip install trl>=0.7.0`

## Generating a config file

Use the built-in examples as a starting point:

```bash
# Copy and edit
cp examples/configs/lora_gpt2.yaml my_config.yaml

# Or generate from the CLI (coming soon)
finetune-cli train --model gpt2 --dataset ./data.jsonl --output ./output
# Training args are logged — copy them into a config file
```

---

## JSON vs YAML

Both formats are supported. YAML is recommended for readability. JSON is useful for programmatic generation.

```bash
finetune-cli train --config config.yaml
finetune-cli train --config config.json
```