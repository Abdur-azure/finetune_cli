# finetune-cli

**Production-grade LLM fine-tuning from the command line.**

[![CI](https://github.com/Abdur-azure/finetune_cli/actions/workflows/ci.yml/badge.svg)](https://github.com/Abdur-azure/finetune_cli/actions)
[![Python](https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12-blue)](https://www.python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## What it does

`finetune-cli` is a modular Python framework for fine-tuning large language models. It wraps HuggingFace Transformers + PEFT in a clean CLI, a validated config system, and a composable trainer stack — all fully tested.

---

## Install

```bash
git clone https://github.com/Abdur-azure/finetune_cli.git
cd finetune_cli
pip install -e .
```

---

## 5-minute quickstart

```bash
# 1. Generate sample training data (no network required)
python examples/generate_sample_data.py

# 2. Not sure which method to use? Ask
finetune-cli recommend gpt2 --output my_config.yaml

# 3. Train
finetune-cli train --config my_config.yaml

# or use a ready-made example config
finetune-cli train --config examples/configs/lora_gpt2.yaml
```

---

## Commands

| Command | What it does |
|---------|-------------|
| `finetune-cli train` | Fine-tune using a YAML config or inline flags |
| `finetune-cli evaluate` | Score a saved checkpoint (ROUGE, BLEU, Perplexity) |
| `finetune-cli benchmark` | Before/after comparison: base vs fine-tuned |
| `finetune-cli upload` | Push adapter or merged model to HuggingFace Hub |
| `finetune-cli merge` | Merge LoRA adapter into base model → standalone model |
| `finetune-cli recommend` | Inspect model size + VRAM, output optimal YAML config |

---

## Training methods

| Method | Flag | VRAM | Best for |
|--------|------|------|---------|
| LoRA | `--method lora` | Medium | General purpose (default) |
| QLoRA | `--method qlora` | Low | Large models on consumer GPU |
| Instruction tuning | `--method instruction_tuning` | Medium | Alpaca-style `{instruction, input, response}` data |
| Full fine-tuning | `--method full_finetuning` | High | Small models, maximum adaptation |

---

## Usage examples

### Train with flags
```bash
finetune-cli train \
  --model gpt2 \
  --dataset ./data/sample.jsonl \
  --method lora \
  --epochs 3 \
  --output ./outputs/gpt2_lora
```

### Train with a config file (recommended for reproducibility)
```bash
finetune-cli train --config examples/configs/lora_gpt2.yaml
```

### Instruction tuning (alpaca-style data)
```bash
finetune-cli train --config examples/configs/instruction_tuning.yaml
```

### Full fine-tuning (small models only)
```bash
finetune-cli train --config examples/configs/full_finetuning.yaml
```

### Evaluate a trained model
```bash
finetune-cli evaluate ./outputs/gpt2_lora \
  --dataset ./data/sample.jsonl \
  --metrics rougeL,bleu
```

### Merge adapter into standalone model
```bash
finetune-cli merge ./outputs/gpt2_lora ./outputs/gpt2_merged \
  --base-model gpt2 \
  --dtype float16
```

### Upload to HuggingFace Hub
```bash
# Upload adapter only
finetune-cli upload ./outputs/gpt2_lora my-username/gpt2-lora

# Upload merged standalone model
finetune-cli upload ./outputs/gpt2_lora my-username/gpt2-merged \
  --merge-adapter --base-model gpt2
```

---

## Example configs

All configs in `examples/configs/` point to data generated by `generate_sample_data.py` and run out of the box.

| Config | Method | Model | Data |
|--------|--------|-------|------|
| `lora_gpt2.yaml` | LoRA | GPT-2 | `data/sample.jsonl` |
| `instruction_tuning.yaml` | Instruction | GPT-2 | `data/instructions.jsonl` |
| `full_finetuning.yaml` | Full | GPT-2 | `data/sample.jsonl` |
| `qlora_llama.yaml` | QLoRA | LLaMA-3.2-1B | HF Hub (needs token) |

---

## Python API

```python
from finetune_cli.core.config import ConfigBuilder
from finetune_cli.core.types import TrainingMethod, DatasetSource
from finetune_cli.models.loader import load_model_and_tokenizer
from finetune_cli.data import prepare_dataset
from finetune_cli.trainers import TrainerFactory

config = (
    ConfigBuilder()
    .with_model("gpt2")
    .with_dataset("./data/sample.jsonl", source=DatasetSource.LOCAL_FILE)
    .with_tokenization(max_length=256)
    .with_training(TrainingMethod.LORA, "./output", num_epochs=3)
    .with_lora(r=8, lora_alpha=16)
    .build()
)

model, tokenizer = load_model_and_tokenizer(config.model.to_config())
dataset = prepare_dataset(config.dataset.to_config(), config.tokenization.to_config(), tokenizer)
result = TrainerFactory.train(model, tokenizer, dataset, config.training.to_config(), config.lora.to_config())
print(f"Done. Loss: {result.train_loss:.4f}  →  {result.output_dir}")
```

---

## Docs

- [Usage Guide](docs/usage.md) — all commands with examples
- [Configuration Reference](docs/configuration.md) — YAML config fields
- [API Reference](docs/api.md) — Python API
- [Architecture](docs/ARCHITECTURE.md) — module design
- [Contributing](CONTRIBUTING.md) — how to add trainers or commands

---

## Tests

```bash
# Unit tests (no GPU needed)
pytest tests/ -v --ignore=tests/test_integration.py

# Integration tests (CPU ok, ~30s)
pytest tests/test_integration.py -v -s

# Full suite
pytest tests/ -v
```

---

## Project status

| Aspect | Status |
|--------|--------|
| Tests | 70+ unit + integration, all green |
| CI | pytest on Python 3.10 / 3.11 / 3.12 |
| Platform | Windows / macOS / Linux |
| Version | 2.3.0 |