# ğŸ¤– Finetune CLI Tool

A comprehensive command-line tool for **fine-tuning** and **distilling** Large Language Models. Supports LoRA, QLoRA, AdaLoRA fine-tuning methods and Knowledge Distillation for model compression.

![Build](https://img.shields.io/github/actions/workflow/status/Abdur-azure/finetune_cli/deploy_docs.yml)
![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Stars](https://img.shields.io/github/stars/Abdur-azure/finetune_cli)
![Issues](https://img.shields.io/github/issues/Abdur-azure/finetune_cli)

---

## âœ¨ Features

### **Fine-Tuning Methods**
- ğŸ¯ **LoRA**: Efficient parameter-efficient fine-tuning (~50% memory savings)
- ğŸ”¥ **QLoRA**: Quantized LoRA for large models (7B+ on consumer GPUs)
- ğŸ§  **AdaLoRA**: Adaptive rank allocation for optimal performance

### **Knowledge Distillation** ğŸ†•
- ğŸ« **Teacher-Student Training**: Compress large models into smaller ones
- ğŸ“‰ **6-10x Model Compression**: Maintain 82-95% performance
- âš¡ **Faster Inference**: Deploy on mobile/edge devices

### **Core Capabilities**
- ğŸ“Š **Auto-benchmarking**: ROUGE score comparison before/after training
- ğŸ” **Smart Dataset Loading**: Auto-detect columns, handle multiple formats
- ğŸ“ **Flexible Data Sources**: Local files (JSON, JSONL, CSV, TXT) or HuggingFace
- ğŸ›ï¸ **Selective Loading**: Load specific files from large repositories
- ğŸš€ **HuggingFace Upload**: Push models directly to Hub
- ğŸ§  **Auto-detection**: Automatically finds target modules

---

## ğŸ†• What's New in v2.1

### **Knowledge Distillation Support**

Train smaller, faster models by transferring knowledge from large teacher models:

```bash
python finetune_cli.py

# Step 0: Select Approach
> 2  # Knowledge Distillation

# Configure teacher (large) and student (small) models
Teacher: gpt2-medium (355M params)
Student: gpt2 (124M params)

# Result: 3x smaller model with ~90% retained performance
```

**Use Cases:**
- Mobile/edge deployment
- Faster inference requirements
- Model compression for bandwidth/storage
- Cost reduction for inference

See [DISTILLATION_UPDATE.md](DISTILLATION_UPDATE.md) for complete details.

---

## ğŸ”§ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-capable GPU (optional but recommended)

### Install Dependencies
```bash
pip install -r requirements.txt
```

**Requirements:**
```
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0
peft>=0.7.0
rouge-score>=0.1.2
huggingface-hub>=0.19.0
tqdm>=4.65.0
pandas>=2.0.0
```

---

## ğŸš€ Quick Start

```bash
python finetune_cli.py
```

The tool guides you through:

### **Step 0: Choose Approach**
- Fine-Tuning (adapt model to task)
- Knowledge Distillation (compress model)

### **For Fine-Tuning:**
1. **Method Selection**: LoRA, QLoRA, or AdaLoRA
2. **Model Configuration**: Choose model and output directory
3. **Dataset Loading**: Local or HuggingFace datasets
4. **Pre-training Benchmark**: ROUGE scoring on base model
5. **Method Configuration**: Set rank, alpha, dropout parameters
6. **Training**: Fine-tune with custom hyperparameters
7. **Post-training Benchmark**: Compare improvements
8. **Upload**: Push to HuggingFace Hub (optional)

### **For Distillation:**
1. **Model Selection**: Teacher (large) and Student (small)
2. **Dataset Loading**: Same flexible options
3. **Distillation Config**: Temperature and alpha parameters
4. **Training**: Transfer knowledge from teacher to student
5. **Evaluation**: Benchmark compressed model
6. **Upload**: Share on HuggingFace Hub (optional)

---

## ğŸ“š Usage Examples

### **Example 1: LoRA Fine-tuning on Local Data**

```bash
python finetune_cli.py

# Step 0: Approach
> 1  # Fine-Tuning

# Step 1: Method
> 1  # LoRA

# Step 2: Model Config
Model name: gpt2
Output directory: ./my_model

# Step 3: Dataset
Dataset path: ./my_data.jsonl
Limit samples: yes
Number of samples: 5000

# Step 4: Benchmark
Run pre-training benchmark: yes

# Step 5: LoRA Config
LoRA r: 8
LoRA alpha: 32
LoRA dropout: 0.1

# Step 6: Training
Epochs: 3
Batch size: 4
Learning rate: 2e-4
```

### **Example 2: QLoRA for Large Models**

```bash
python finetune_cli.py

# Step 0: Approach
> 1  # Fine-Tuning

# Step 1: Method
> 2  # QLoRA

# Step 2: Model Config
Model name: meta-llama/Llama-2-7b-hf
# (Model loaded in 4-bit quantization automatically)

# Rest of configuration...
# Can run 7B models on 12GB GPU!
```

### **Example 3: Knowledge Distillation** ğŸ†•

```bash
python finetune_cli.py

# Step 0: Approach
> 2  # Distillation

# Step 1: Model Config
Teacher model: gpt2-large
Student model: gpt2
Output directory: ./distilled_model

# Step 2: Dataset
Dataset: wikitext
Samples: 5000

# Step 3: Distillation Config
Temperature: 2.0  # Softness of probability distribution
Alpha: 0.5        # Balance between hard/soft targets

# Step 4: Training
Epochs: 3
Batch size: 4
Learning rate: 2e-4

# Result: 6x smaller model, 3x faster inference
```

### **Example 4: HuggingFace Dataset with Selective Loading**

```bash
# Step 3: Dataset Config
Dataset name: HuggingFaceH4/ultrachat_200k
Load specific file: yes
File path: data/train_sft-00000-of-00004.parquet
Number of samples: 2000
```

---

## ğŸ“Š Supported Dataset Formats

### **Local Files**
- **JSON**: Standard JSON format
- **JSONL**: JSON Lines (one object per line)
- **CSV**: Comma-separated values
- **TXT**: Plain text (one sample per line)

### **HuggingFace Datasets**
- Any public HuggingFace dataset
- Specific file selection from large repositories
- Multiple split support (train/test/validation)

---

## ğŸ¯ Training Method Comparison

| Method | Memory | Trainable Params | GPU Requirement | Best For |
|--------|--------|------------------|-----------------|----------|
| **LoRA** | ~50% of full FT | 0.1-1% | 8GB+ | Balanced efficiency |
| **QLoRA** | ~12-25% of full FT | 0.1-1% | 6GB+ | Large models (7B+) |
| **AdaLoRA** | ~50% of full FT | 0.1-1% | 8GB+ | Optimal performance |
| **Distillation** ğŸ†• | Teacher+Student | 100% (student) | 12GB+ | Model compression |

---

## ğŸ§  Knowledge Distillation Details

### **How It Works**

1. **Teacher Model**: Large, well-performing model
2. **Student Model**: Smaller model to be trained
3. **Soft Targets**: Teacher's probability distributions (temperature-scaled)
4. **Combined Loss**: Hard labels + Soft targets from teacher

**Loss Function:**
```
Loss = Î± Ã— CrossEntropy(student, labels) 
     + (1-Î±) Ã— KL_Divergence(student || teacher)
```

### **Parameters**

| Parameter | Range | Default | Description |
|-----------|-------|---------|-------------|
| **Temperature** | 1.0-5.0 | 2.0 | Controls softness of distributions |
| **Alpha** | 0.0-1.0 | 0.5 | Weight for hard label loss |

### **Compression Results**

| Teacher â†’ Student | Size Reduction | Performance Retained |
|-------------------|----------------|---------------------|
| GPT-2 Large â†’ Medium | 2.2x | 92-95% |
| GPT-2 Medium â†’ Small | 3x | 88-92% |
| GPT-2 Large â†’ Small | 6x | 82-88% |

---

## ğŸ›ï¸ Parameter Guide

### **LoRA Parameters**

| Parameter | Range | Recommended | Effect |
|-----------|-------|-------------|--------|
| **r (rank)** | 1-256 | 4-16 | Adapter capacity |
| **alpha** | 1-256 | 2-4Ã— rank | Scaling factor |
| **dropout** | 0.0-0.5 | 0.1 | Regularization |

**Guidelines:**
- **r=4**: Light adaptation, fast training
- **r=8**: Balanced (recommended)
- **r=16**: Strong adaptation for complex tasks
- **r=32**: Maximum quality for specialized domains

### **Distillation Parameters** ğŸ†•

| Parameter | Range | Recommended | Effect |
|-----------|-------|-------------|--------|
| **Temperature** | 1.0-5.0 | 2.0 | Softness of knowledge transfer |
| **Alpha** | 0.0-1.0 | 0.5 | Hard vs soft target balance |

**Guidelines:**
- **High T (3-5)**: More knowledge transfer, better for very different sizes
- **Low T (1-2)**: Less smoothing, better for similar sizes
- **High Î± (0.6-0.8)**: Focus on hard labels
- **Low Î± (0.2-0.4)**: Focus on teacher knowledge

---

## ğŸ”‘ HuggingFace Integration

### **Upload Models**

```bash
# During workflow (Step 8/Final Step)
Upload to HuggingFace? yes
Repository name: username/my-model
Create new repository? yes
Make repository private? no
```

### **Authentication**

**Option 1: Token during upload**
```bash
HuggingFace token: hf_xxxxxxxxxxxxx
```

**Option 2: Pre-login via CLI**
```bash
huggingface-cli login
```

Get your token from: https://huggingface.co/settings/tokens

---

## ğŸ“ Project Structure

```
finetune_cli/
â”œâ”€â”€ finetune_cli.py           # Main application (with distillation)
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ DISTILLATION_UPDATE.md    # Distillation documentation
â”œâ”€â”€ LICENSE                   # MIT License
â””â”€â”€ .gitignore               # Git ignore rules
```

---

## ğŸ› Troubleshooting

### **CUDA Out of Memory**
**Solutions:**
- Reduce batch size
- Decrease max sequence length
- Use lower LoRA rank
- Enable gradient checkpointing
- For large models: Use QLoRA with 4-bit quantization

### **Module Not Found**
```bash
pip install --upgrade -r requirements.txt
```

### **Authentication Error (HuggingFace)**
```bash
huggingface-cli login
```

### **Distillation: Teacher Too Large**
**Solutions:**
- Use quantized teacher (load in 8-bit)
- Choose smaller teacher model
- Train on smaller batches
- Use gradient accumulation

---

## ğŸ“ˆ Performance Tips

### **General**
1. **Start Small**: Test with 1000 samples before full training
2. **Monitor Memory**: Watch GPU usage with `nvidia-smi`
3. **Adjust LoRA Rank**: Higher rank = better but more memory
4. **Use Gradient Accumulation**: Simulate larger batch sizes

### **For Distillation** ğŸ†•
1. **Teacher Size**: Don't go more than 10x larger than student
2. **Temperature**: Start with 2.0, increase if performance is poor
3. **Dataset Size**: Need 2-5x more data than fine-tuning
4. **Training Time**: Budget 1.5-2x longer than fine-tuning

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a Pull Request

**Areas for Contribution:**
- Additional distillation methods (multi-teacher, progressive)
- More fine-tuning methods (Prefix Tuning, IA3)
- Improved benchmarking metrics
- Better error handling
- Documentation improvements

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details.

Free to use for any purpose.

---

## ğŸ™ Acknowledgments

### **Libraries**
- [Transformers](https://github.com/huggingface/transformers) - HuggingFace
- [PEFT](https://github.com/huggingface/peft) - Parameter-Efficient Fine-Tuning
- [ROUGE Score](https://github.com/google-research/google-research/tree/master/rouge) - Evaluation metrics

### **Research**
- **LoRA**: Hu et al. - "LoRA: Low-Rank Adaptation of Large Language Models"
- **QLoRA**: Dettmers et al. - "QLoRA: Efficient Finetuning of Quantized LLMs"
- **AdaLoRA**: Zhang et al. - "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
- **Knowledge Distillation**: Hinton et al. - "Distilling the Knowledge in a Neural Network" ğŸ†•

---

## ğŸ“§ Support

**Issues & Questions:**
- GitHub Issues: https://github.com/Abdur-azure/finetune_cli/issues
- Discussions: https://github.com/Abdur-azure/finetune_cli/discussions

**Documentation:**
- [Installation Guide](docs/installation.md)
- [Usage Guide](docs/usage.md)
- [Distillation Guide](DISTILLATION_UPDATE.md) ğŸ†•
- [API Reference](docs/api.md)

---

## ğŸ”® Roadmap

### **v2.2 (Planned)**
- [ ] Multi-teacher distillation
- [ ] Progressive distillation (iterative compression)
- [ ] Quantization-aware training
- [ ] Web UI interface

### **v2.3 (Future)**
- [ ] DPO/RLHF alignment methods
- [ ] Prefix Tuning and IA3
- [ ] Distributed training support
- [ ] Advanced evaluation metrics

---

## ğŸ“Š Version History

### **v2.1.0** (2025-01-29) - Current ğŸ†•
- âœ¨ Added Knowledge Distillation support
- ğŸ« Teacher-Student training workflow
- ğŸ“‰ Model compression (6-10x size reduction)
- ğŸ¯ Temperature and alpha parameter tuning
- ğŸ“š Comprehensive distillation documentation

### **v2.0.0** (2025-01-27)
- ğŸ¯ LoRA fine-tuning
- ğŸ”¥ QLoRA for large models
- ğŸ§  AdaLoRA adaptive rank allocation
- ğŸ“Š Automatic ROUGE benchmarking
- ğŸš€ HuggingFace Hub integration

---

**Made with â¤ï¸ for the AI community**

**Star â­ this repo if you find it useful!**