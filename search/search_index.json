{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udd16 Finetune CLI","text":"<p>A comprehensive command-line tool for fine-tuning Large Language Models using LoRA (Low-Rank Adaptation), with automatic ROUGE benchmarking and HuggingFace integration.</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>This tool simplifies the process of fine-tuning large language models by providing an interactive CLI interface with built-in benchmarking capabilities. Whether you're working with local datasets or HuggingFace repositories, this tool handles the complexity of LoRA configuration, training, and evaluation.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfaf LoRA Fine-tuning: Efficient parameter-efficient fine-tuning with automatic target module detection</li> <li>\ud83d\udcca Auto-benchmarking: ROUGE score comparison before and after training to measure improvements</li> <li>\ud83d\udd0d Smart Dataset Loading: Automatically detects text columns and handles multiple data formats</li> <li>\ud83d\udcc1 Flexible Data Sources: Support for local files (JSON, JSONL, CSV, TXT) and HuggingFace datasets</li> <li>\ud83c\udf9b\ufe0f Selective Loading: Load specific files from large repositories to optimize memory usage</li> <li>\ud83d\ude80 HuggingFace Integration: Push fine-tuned models directly to HuggingFace Hub</li> <li>\ud83e\udde0 Auto-detection: Automatically identifies target modules for any model architecture</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install dependencies\npip install -r requirements.txt\n\n# Run the interactive CLI\npython finetune_cli.py\n</code></pre> <p>The tool will guide you through:</p> <ol> <li>Model selection from HuggingFace</li> <li>Dataset loading and preparation</li> <li>Pre-training benchmark</li> <li>LoRA configuration</li> <li>Training process</li> <li>Post-training evaluation</li> <li>Optional upload to HuggingFace Hub</li> </ol>"},{"location":"#why-use-this-tool","title":"Why Use This Tool?","text":"<ul> <li>Simplified Workflow: No need to write complex training scripts</li> <li>Best Practices Built-in: Automatically handles tokenization, padding, and data collation</li> <li>Memory Efficient: LoRA reduces memory requirements significantly</li> <li>Reproducible: Consistent configuration and benchmarking across experiments</li> <li>Educational: Learn fine-tuning concepts through interactive prompts</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the Installation Guide to set up your environment, then follow the Usage Guide to start fine-tuning your first model.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Installation: Setup instructions and prerequisites</li> <li>Usage Guide: Detailed walkthrough of all features</li> <li>Configuration: Understanding LoRA parameters and training settings</li> <li>API Reference: Technical documentation of core classes and methods</li> <li>Examples: Common use cases and recipes</li> <li>Troubleshooting: Solutions to common issues</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>This tool is actively maintained and open for contributions. If you encounter any issues or have suggestions, please open an issue on GitHub.</p>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"ARCHITECTURE/","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<p>Version: 2.0 (FAANG-Grade Refactor) Last Updated: 2025-01-29</p>"},{"location":"ARCHITECTURE/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Overview</li> <li>Design Principles</li> <li>System Architecture</li> <li>Module Breakdown</li> <li>Data Flow</li> <li>Extension Points</li> <li>Migration from v1</li> </ol>"},{"location":"ARCHITECTURE/#overview","title":"\ud83c\udfaf Overview","text":"<p>This framework provides a production-grade, modular, and extensible system for fine-tuning Large Language Models. Built following FAANG engineering standards with:</p> <ul> <li>Type Safety: Full type hints with protocols and type checking</li> <li>Modularity: Clean separation of concerns across layers</li> <li>Extensibility: Plugin architecture for new methods and data sources</li> <li>Testability: Dependency injection and interface-based design</li> <li>Observability: Comprehensive logging and error handling</li> <li>Configuration: Pydantic-based config with validation</li> </ul>"},{"location":"ARCHITECTURE/#design-principles","title":"\ud83e\udded Design Principles","text":""},{"location":"ARCHITECTURE/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>Each module has a single, well-defined responsibility: - <code>core/</code> - Type definitions, configuration, exceptions - <code>models/</code> - Model loading and management - <code>data/</code> - Dataset loading and processing - <code>trainers/</code> - Training implementations - <code>evaluation/</code> - Metrics and benchmarking - <code>cli/</code> - User interface</p>"},{"location":"ARCHITECTURE/#2-composition-over-inheritance","title":"2. Composition Over Inheritance","text":"<p>Uses protocols (interfaces) rather than deep inheritance hierarchies:</p> <pre><code>class ModelLoader(Protocol):\n    def load_model(self, config: ModelConfig) -&gt; PreTrainedModel: ...\n</code></pre>"},{"location":"ARCHITECTURE/#3-dependency-injection","title":"3. Dependency Injection","text":"<p>Components receive dependencies explicitly:</p> <pre><code>def __init__(self, tokenizer: PreTrainedTokenizer, config: TokenizationConfig):\n    self.tokenizer = tokenizer\n    self.config = config\n</code></pre>"},{"location":"ARCHITECTURE/#4-factory-registry-patterns","title":"4. Factory &amp; Registry Patterns","text":"<p>Dynamic selection of implementations:</p> <pre><code>registry = DatasetLoaderRegistry()\nloader = registry.get_loader(config)  # Auto-selects based on config\n</code></pre>"},{"location":"ARCHITECTURE/#5-immutable-configuration","title":"5. Immutable Configuration","text":"<p>Config objects are frozen dataclasses preventing accidental mutation:</p> <pre><code>@dataclass(frozen=True)\nclass ModelConfig:\n    name: str\n    device: DeviceType\n</code></pre>"},{"location":"ARCHITECTURE/#6-fail-fast","title":"6. Fail Fast","text":"<p>Validate at config layer, not execution layer:</p> <pre><code>class ModelConfigModel(BaseModel):\n    @model_validator(mode='after')\n    def validate_quantization(self) -&gt; 'ModelConfigModel':\n        if self.load_in_8bit and self.load_in_4bit:\n            raise IncompatibleConfigError(...)\n</code></pre>"},{"location":"ARCHITECTURE/#system-architecture","title":"\ud83c\udfdb\ufe0f System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         CLI Layer                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  train   \u2502  \u2502evaluate  \u2502  \u2502benchmark \u2502  \u2502  upload  \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502             \u2502             \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Pipeline Layer                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502          Training Pipeline Orchestrator              \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502                \u2502                \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Models    \u2502  \u2502   Data   \u2502  \u2502  Trainers   \u2502  \u2502   Eval   \u2502\n\u2502              \u2502  \u2502          \u2502  \u2502             \u2502  \u2502          \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502  Loader  \u2502 \u2502  \u2502 \u2502Loader\u2502 \u2502  \u2502 \u2502  LoRA   \u2502 \u2502  \u2502 \u2502Metric\u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 Detector \u2502 \u2502  \u2502 \u2502Proces\u2502 \u2502  \u2502 \u2502  QLoRA  \u2502 \u2502  \u2502 \u2502Bench \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502                \u2502                \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Core Layer                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Types   \u2502  \u2502  Config  \u2502  \u2502Exception \u2502  \u2502 Logging  \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#module-breakdown","title":"\ud83d\udce6 Module Breakdown","text":""},{"location":"ARCHITECTURE/#core-core","title":"Core (<code>core/</code>)","text":""},{"location":"ARCHITECTURE/#typespy-type-system","title":"<code>types.py</code> - Type System","text":"<ul> <li>Enums: <code>TrainingMethod</code>, <code>DatasetSource</code>, <code>EvaluationMetric</code>, etc.</li> <li>Dataclasses: Immutable config objects (<code>ModelConfig</code>, <code>TrainingConfig</code>, etc.)</li> <li>Protocols: Interface definitions (<code>ModelLoader</code>, <code>Trainer</code>, <code>Evaluator</code>)</li> <li>Results: Structured output types (<code>TrainingResult</code>, <code>EvaluationResult</code>)</li> </ul> <p>Design: Centralized type definitions prevent duplication and ensure consistency.</p>"},{"location":"ARCHITECTURE/#exceptionspy-exception-hierarchy","title":"<code>exceptions.py</code> - Exception Hierarchy","text":"<pre><code>FineTuneError (base)\n\u251c\u2500\u2500 ConfigurationError\n\u2502   \u251c\u2500\u2500 InvalidConfigError\n\u2502   \u251c\u2500\u2500 MissingConfigError\n\u2502   \u2514\u2500\u2500 IncompatibleConfigError\n\u251c\u2500\u2500 ModelError\n\u2502   \u251c\u2500\u2500 ModelLoadError\n\u2502   \u251c\u2500\u2500 ModelNotFoundError\n\u2502   \u2514\u2500\u2500 UnsupportedModelError\n\u251c\u2500\u2500 DatasetError\n\u2502   \u251c\u2500\u2500 DatasetLoadError\n\u2502   \u251c\u2500\u2500 NoTextColumnsError\n\u2502   \u2514\u2500\u2500 EmptyDatasetError\n\u251c\u2500\u2500 TrainingError\n\u2502   \u251c\u2500\u2500 OutOfMemoryError\n\u2502   \u251c\u2500\u2500 NaNLossError\n\u2502   \u2514\u2500\u2500 CheckpointError\n\u2514\u2500\u2500 EvaluationError\n    \u2514\u2500\u2500 MetricComputationError\n</code></pre> <p>Design: Specific exceptions enable precise error handling and actionable messages.</p>"},{"location":"ARCHITECTURE/#configpy-configuration-management","title":"<code>config.py</code> - Configuration Management","text":"<ul> <li>Pydantic Models: Validation + serialization (<code>ModelConfigModel</code>, etc.)</li> <li>Builders: Programmatic config construction</li> <li>I/O: JSON/YAML loading and saving</li> <li>Validation: Cross-field validation and type conversion</li> </ul> <p>Example:</p> <pre><code>config = ConfigBuilder() \\\n    .with_model(\"gpt2\") \\\n    .with_dataset(\"./data.jsonl\", source=DatasetSource.LOCAL_FILE) \\\n    .with_training(TrainingMethod.LORA, \"./output\") \\\n    .with_lora(r=8, lora_alpha=32) \\\n    .build()\n</code></pre>"},{"location":"ARCHITECTURE/#models-models","title":"Models (<code>models/</code>)","text":""},{"location":"ARCHITECTURE/#loaderpy-model-loading","title":"<code>loader.py</code> - Model Loading","text":"<p>Components: - <code>ModelLoader</code>: Load models with quantization, device mapping, Flash Attention - <code>TargetModuleDetector</code>: Auto-detect LoRA target modules</p> <p>Features: - Device auto-detection (CUDA/MPS/CPU) - 4-bit/8-bit quantization - Flash Attention 2 support - Gradient checkpointing - Memory-efficient loading</p> <p>Example:</p> <pre><code>loader = ModelLoader()\nmodel, tokenizer = loader.load(model_config)\n\ndetector = TargetModuleDetector(model)\ntarget_modules = detector.detect()  # Auto-detect for LoRA\n</code></pre>"},{"location":"ARCHITECTURE/#data-data","title":"Data (<code>data/</code>)","text":""},{"location":"ARCHITECTURE/#basepy-abstract-interfaces","title":"<code>base.py</code> - Abstract Interfaces","text":"<ul> <li><code>DatasetLoader</code> protocol</li> <li><code>DatasetProcessor</code> protocol</li> <li><code>DatasetStatistics</code>, <code>DatasetAnalyzer</code>, <code>DatasetFilter</code></li> </ul>"},{"location":"ARCHITECTURE/#loaderspy-loading-implementations","title":"<code>loaders.py</code> - Loading Implementations","text":"<p>Loaders: - <code>LocalFileLoader</code>: JSON, JSONL, CSV, Parquet, TXT - <code>HuggingFaceLoader</code>: Hub datasets with streaming</p> <p>Registry:</p> <pre><code>registry = DatasetLoaderRegistry()\nloader = registry.get_loader(config)  # Auto-selects\ndataset = loader.load(config)\n</code></pre>"},{"location":"ARCHITECTURE/#processorspy-processing-tokenization","title":"<code>processors.py</code> - Processing &amp; Tokenization","text":"<p>Strategies: - <code>SingleColumnStrategy</code>: One text column - <code>MultiColumnStrategy</code>: Multiple columns combined - <code>InstructionStrategy</code>: Instruction-response format</p> <p>Auto-detection:</p> <pre><code>detector = TextColumnDetector()\ncolumns = detector.detect(dataset)  # Finds text columns\n</code></pre>"},{"location":"ARCHITECTURE/#pipelinepy-complete-pipeline","title":"<code>pipeline.py</code> - Complete Pipeline","text":"<p>DataPipeline:</p> <pre><code>pipeline = DataPipeline(dataset_config, tokenization_config, tokenizer)\ndataset = pipeline.run(split_for_validation=True, validation_ratio=0.1)\n# Returns: {'train': Dataset, 'validation': Dataset}\n</code></pre> <p>Quick functions:</p> <pre><code># Minimal config\ndataset = quick_load(\"./data.jsonl\", tokenizer, max_samples=1000)\n\n# Full config\ndataset = prepare_dataset(dataset_config, tokenization_config, tokenizer)\n</code></pre>"},{"location":"ARCHITECTURE/#utils-utils","title":"Utils (<code>utils/</code>)","text":""},{"location":"ARCHITECTURE/#loggingpy-logging-infrastructure","title":"<code>logging.py</code> - Logging Infrastructure","text":"<ul> <li>Colored console output</li> <li>File logging</li> <li>Context managers (<code>LogProgress</code>, <code>LogContext</code>)</li> <li>Specialized loggers (<code>log_model_info</code>, <code>log_dataset_info</code>)</li> </ul> <p>Example:</p> <pre><code>logger = setup_logger(\"my_module\", level=LogLevel.INFO, log_file=Path(\"log.txt\"))\n\nwith LogProgress(logger, \"Training model\"):\n    train()  # Automatically logs start/end time\n</code></pre>"},{"location":"ARCHITECTURE/#data-flow","title":"\ud83d\udd04 Data Flow","text":""},{"location":"ARCHITECTURE/#training-pipeline-flow","title":"Training Pipeline Flow:","text":"<pre><code>1. Configuration\n   \u251c\u2500 Load config from file/CLI\n   \u251c\u2500 Validate with Pydantic\n   \u2514\u2500 Convert to immutable dataclasses\n\n2. Model Loading\n   \u251c\u2500 Load model from HuggingFace\n   \u251c\u2500 Apply quantization (optional)\n   \u251c\u2500 Setup device mapping\n   \u2514\u2500 Detect target modules\n\n3. Data Pipeline\n   \u251c\u2500 Load dataset (local/HF)\n   \u251c\u2500 Detect text columns\n   \u251c\u2500 Filter invalid samples\n   \u251c\u2500 Tokenize\n   \u2514\u2500 Split train/val\n\n4. Training\n   \u251c\u2500 Initialize trainer (LoRA/QLoRA/Full)\n   \u251c\u2500 Setup LoRA adapters\n   \u251c\u2500 Train with HF Trainer\n   \u2514\u2500 Save checkpoints\n\n5. Evaluation\n   \u251c\u2500 Compute metrics (ROUGE/BLEU/etc)\n   \u251c\u2500 Compare base vs fine-tuned\n   \u2514\u2500 Generate report\n\n6. Upload (optional)\n   \u2514\u2500 Push to HuggingFace Hub\n</code></pre>"},{"location":"ARCHITECTURE/#extension-points","title":"\ud83d\udd0c Extension Points","text":""},{"location":"ARCHITECTURE/#adding-a-new-data-source","title":"Adding a New Data Source:","text":"<pre><code>class CustomLoader(DatasetLoader):\n    def can_handle(self, config: DatasetConfig) -&gt; bool:\n        return config.source == DatasetSource.CUSTOM\n\n    def load(self, config: DatasetConfig) -&gt; Dataset:\n        # Your loading logic\n        return dataset\n\n# Register\nfrom finetune_cli.data import register_loader\nregister_loader(CustomLoader())\n</code></pre>"},{"location":"ARCHITECTURE/#adding-a-new-training-method","title":"Adding a New Training Method:","text":"<pre><code>class CustomTrainer(Trainer):\n    def train(self, model, dataset, config) -&gt; TrainingResult:\n        # Your training logic\n        return result\n\n# Register in factory\nTrainerFactory.register(TrainingMethod.CUSTOM, CustomTrainer)\n</code></pre>"},{"location":"ARCHITECTURE/#adding-a-new-metric","title":"Adding a New Metric:","text":"<pre><code>class CustomMetric(Metric):\n    def compute(self, predictions, references) -&gt; float:\n        # Your metric logic\n        return score\n\n# Register\nMetricRegistry.register(EvaluationMetric.CUSTOM, CustomMetric)\n</code></pre>"},{"location":"ARCHITECTURE/#migration-from-v1","title":"\ud83d\udd04 Migration from v1","text":""},{"location":"ARCHITECTURE/#old-monolithic","title":"Old (Monolithic):","text":"<pre><code># Single 400-line file\nfinetuner = LLMFineTuner(\"gpt2\", \"./output\")\nfinetuner.load_model()\ndataset = finetuner.load_dataset_from_source(\"./data.json\")\ntokenized = finetuner.prepare_dataset(dataset)\nfinetuner.setup_lora(r=8)\nfinetuner.train(tokenized)\n</code></pre>"},{"location":"ARCHITECTURE/#new-modular","title":"New (Modular):","text":"<pre><code># Separate concerns\nfrom finetune_cli.core.config import ConfigBuilder\nfrom finetune_cli.core.types import DatasetSource, TrainingMethod\nfrom finetune_cli.models.loader import load_model_and_tokenizer\nfrom finetune_cli.data import prepare_dataset\nfrom finetune_cli.trainers import LoRATrainer\n\n# 1. Configuration (validated)\nconfig = ConfigBuilder() \\\n    .with_model(\"gpt2\") \\\n    .with_dataset(\"./data.json\", source=DatasetSource.LOCAL_FILE) \\\n    .with_training(TrainingMethod.LORA, \"./output\", num_epochs=3) \\\n    .with_lora(r=8, lora_alpha=32) \\\n    .build()\n\n# 2. Load model (with auto-detection)\nmodel, tokenizer = load_model_and_tokenizer(config.model.to_config())\n\n# 3. Prepare data (with pipeline)\ndataset = prepare_dataset(\n    config.dataset.to_config(),\n    config.tokenization.to_config(),\n    tokenizer\n)\n\n# 4. Train (with specific trainer)\ntrainer = LoRATrainer(model, tokenizer, config.training.to_config(), config.lora.to_config())\nresult = trainer.train(dataset)\n</code></pre>"},{"location":"ARCHITECTURE/#benefits-of-new-architecture","title":"Benefits of New Architecture:","text":"<ul> <li>\u2705 Testable: Each component can be tested independently</li> <li>\u2705 Type-safe: Full type checking with mypy</li> <li>\u2705 Extensible: Add new methods without modifying core</li> <li>\u2705 Maintainable: Clear responsibilities, no 400-line files</li> <li>\u2705 Documented: Self-documenting with types and protocols</li> <li>\u2705 Production-ready: Error handling, logging, validation</li> </ul>"},{"location":"ARCHITECTURE/#comparison-v1-vs-v2","title":"\ud83d\udcca Comparison: v1 vs v2","text":"Aspect v1 (Monolithic) v2 (Modular) Lines per file 400+ &lt;200 Type hints None Complete Testing Impossible Unit + Integration Extensibility Modify core Plugin system Error handling Generic Specific exceptions Configuration Hardcoded Validated configs Logging Print statements Structured logging Methods 1 (LoRA) 20+ (planned) Maintainability Poor Excellent"},{"location":"ARCHITECTURE/#next-steps","title":"\ud83d\ude80 Next Steps","text":""},{"location":"ARCHITECTURE/#phase-3-trainer-system-next-priority","title":"Phase 3: Trainer System (Next Priority)","text":"<ul> <li>Abstract <code>Trainer</code> base class</li> <li><code>LoRATrainer</code> implementation</li> <li><code>QLoRATrainer</code> implementation</li> <li><code>FullFineTuner</code> implementation</li> <li><code>TrainerFactory</code> for method selection</li> </ul>"},{"location":"ARCHITECTURE/#phase-4-evaluation-system","title":"Phase 4: Evaluation System","text":"<ul> <li>Metric implementations (ROUGE, BLEU, Perplexity)</li> <li>Benchmarking pipeline</li> <li>Comparison reports</li> </ul>"},{"location":"ARCHITECTURE/#phase-5-cli-interface","title":"Phase 5: CLI Interface","text":"<ul> <li>Typer-based CLI</li> <li>Subcommands for each operation</li> <li>Interactive mode</li> <li>Config file support</li> </ul>"},{"location":"ARCHITECTURE/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Configuration Guide: See <code>docs/configuration.md</code></li> <li>API Reference: See <code>docs/api.md</code></li> <li>Examples: See <code>examples/</code> directory</li> <li>Contributing: See <code>CONTRIBUTING.md</code></li> </ul> <p>Last Updated: 2025-01-29 Architecture Version: 2.0.0 Status: Phase 2 Complete (Data Pipeline)</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete reference for the <code>LLMFineTuner</code> class and its methods.</p>"},{"location":"api/#llmfinetuner-class","title":"LLMFineTuner Class","text":"<p>The main class for fine-tuning language models with LoRA.</p>"},{"location":"api/#constructor","title":"Constructor","text":"<pre><code>LLMFineTuner(model_name: str, output_dir: str = \"./finetuned_model\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>model_name</code> (str): HuggingFace model identifier (e.g., \"gpt2\", \"facebook/opt-125m\")</li> <li><code>output_dir</code> (str, optional): Directory to save fine-tuned model. Default: \"./finetuned_model\"</li> </ul> <p>Attributes:</p> <ul> <li><code>model_name</code> (str): Name of the base model</li> <li><code>output_dir</code> (str): Output directory path</li> <li><code>device</code> (str): Device for training (\"cuda\" or \"cpu\")</li> <li><code>tokenizer</code> (AutoTokenizer): HuggingFace tokenizer instance</li> <li><code>model</code> (AutoModelForCausalLM): Base model instance</li> <li><code>peft_model</code> (PeftModel): LoRA-adapted model instance</li> </ul> <p>Example:</p> <pre><code>from finetune_cli import LLMFineTuner\n\nfinetuner = LLMFineTuner(\n    model_name=\"gpt2\",\n    output_dir=\"./my_model\"\n)\n</code></pre>"},{"location":"api/#methods","title":"Methods","text":""},{"location":"api/#load_model","title":"load_model()","text":"<p>Load the base model and tokenizer from HuggingFace.</p> <pre><code>def load_model() -&gt; None\n</code></pre> <p>Returns: None</p> <p>Side Effects:</p> <ul> <li>Initializes <code>self.tokenizer</code></li> <li>Initializes <code>self.model</code></li> <li>Sets pad_token if not present</li> </ul> <p>Example:</p> <pre><code>finetuner = LLMFineTuner(\"gpt2\")\nfinetuner.load_model()\n</code></pre> <p>Notes:</p> <ul> <li>Automatically uses FP16 on CUDA devices</li> <li>Sets device_map=\"auto\" for multi-GPU support</li> <li>Uses low_cpu_mem_usage for efficient loading</li> </ul>"},{"location":"api/#load_dataset_from_source","title":"load_dataset_from_source()","text":"<p>Load dataset from local file or HuggingFace Hub.</p> <pre><code>def load_dataset_from_source(\n    dataset_source: str,\n    dataset_config: Optional[str] = None,\n    split: str = \"train\",\n    num_samples: Optional[int] = None,\n    data_files: Optional[str] = None\n) -&gt; Dataset\n</code></pre> <p>Parameters:</p> <ul> <li><code>dataset_source</code> (str): Local file path or HuggingFace dataset name</li> <li><code>dataset_config</code> (str, optional): Dataset configuration/subset name</li> <li><code>split</code> (str): Dataset split to load. Default: \"train\"</li> <li><code>num_samples</code> (int, optional): Limit number of samples to load</li> <li><code>data_files</code> (str, optional): Specific files to load from repository</li> </ul> <p>Returns: <code>Dataset</code> object</p> <p>Supported Formats:</p> <ul> <li>Local: <code>.json</code>, <code>.jsonl</code>, <code>.csv</code>, <code>.txt</code></li> <li>HuggingFace: Any public dataset</li> </ul> <p>Example:</p> <pre><code># Load local file\ndataset = finetuner.load_dataset_from_source(\n    dataset_source=\"./data.jsonl\",\n    num_samples=1000\n)\n\n# Load HuggingFace dataset\ndataset = finetuner.load_dataset_from_source(\n    dataset_source=\"wikitext\",\n    dataset_config=\"wikitext-2-raw-v1\",\n    split=\"train\",\n    num_samples=5000\n)\n\n# Load specific file from large repo\ndataset = finetuner.load_dataset_from_source(\n    dataset_source=\"HuggingFaceH4/ultrachat_200k\",\n    data_files=\"data/train_sft-00000-of-00004.parquet\",\n    num_samples=2000\n)\n</code></pre>"},{"location":"api/#detect_text_columns","title":"detect_text_columns()","text":"<p>Automatically detect text columns in a dataset.</p> <pre><code>def detect_text_columns(dataset: Dataset) -&gt; List[str]\n</code></pre> <p>Parameters:</p> <ul> <li><code>dataset</code> (Dataset): Dataset to analyze</li> </ul> <p>Returns: List of column names containing text data</p> <p>Detection Strategy:</p> <ol> <li>Checks for common text column names</li> <li>Inspects data types of columns</li> <li>Returns all string-type columns</li> </ol> <p>Common Names Detected:</p> <ul> <li>text, content, input, output</li> <li>prompt, response, instruction</li> <li>question, answer</li> </ul> <p>Example:</p> <pre><code>dataset = finetuner.load_dataset_from_source(\"./data.jsonl\")\ntext_cols = finetuner.detect_text_columns(dataset)\nprint(f\"Found columns: {text_cols}\")\n# Output: Found columns: ['prompt', 'response']\n</code></pre>"},{"location":"api/#prepare_dataset","title":"prepare_dataset()","text":"<p>Tokenize and prepare dataset for training.</p> <pre><code>def prepare_dataset(\n    dataset: Dataset,\n    text_columns: Optional[List[str]] = None,\n    max_length: int = 512\n) -&gt; Tuple[Dataset, List[str]]\n</code></pre> <p>Parameters:</p> <ul> <li><code>dataset</code> (Dataset): Raw dataset to prepare</li> <li><code>text_columns</code> (List[str], optional): Columns to use. Auto-detects if None</li> <li><code>max_length</code> (int): Maximum sequence length. Default: 512</li> </ul> <p>Returns: Tuple of (tokenized_dataset, text_columns_used)</p> <p>Processing Steps:</p> <ol> <li>Auto-detects text columns if not provided</li> <li>Combines multiple columns if present</li> <li>Tokenizes with truncation and padding</li> <li>Removes original columns</li> </ol> <p>Example:</p> <pre><code>dataset = finetuner.load_dataset_from_source(\"./data.jsonl\")\n\n# Auto-detect columns\ntokenized, cols = finetuner.prepare_dataset(dataset, max_length=512)\n\n# Manual column specification\ntokenized, cols = finetuner.prepare_dataset(\n    dataset,\n    text_columns=[\"prompt\", \"response\"],\n    max_length=256\n)\n</code></pre>"},{"location":"api/#get_target_modules","title":"get_target_modules()","text":"<p>Automatically detect target modules for LoRA based on model architecture.</p> <pre><code>def get_target_modules() -&gt; Union[List[str], str]\n</code></pre> <p>Returns: List of module names or \"all-linear\" as fallback</p> <p>Detection Patterns:</p> <ul> <li>GPT-2 style: <code>[\"c_attn\", \"c_proj\"]</code></li> <li>Transformer style: <code>[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]</code></li> <li>Alternative: <code>[\"query\", \"value\", \"key\", \"dense\"]</code></li> </ul> <p>Example:</p> <pre><code>finetuner.load_model()\nmodules = finetuner.get_target_modules()\nprint(f\"Detected modules: {modules}\")\n# Output: Detected modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj']\n</code></pre> <p>Notes:</p> <ul> <li>Automatically called by <code>setup_lora()</code></li> <li>Fallback to \"all-linear\" if no pattern matches</li> </ul>"},{"location":"api/#setup_lora","title":"setup_lora()","text":"<p>Configure and apply LoRA to the model.</p> <pre><code>def setup_lora(\n    r: int = 8,\n    lora_alpha: int = 32,\n    lora_dropout: float = 0.1,\n    target_modules: Optional[List[str]] = None\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>r</code> (int): LoRA rank. Default: 8</li> <li><code>lora_alpha</code> (int): LoRA alpha scaling. Default: 32</li> <li><code>lora_dropout</code> (float): Dropout probability. Default: 0.1</li> <li><code>target_modules</code> (List[str], optional): Modules to apply LoRA. Auto-detects if None</li> </ul> <p>Returns: None</p> <p>Side Effects:</p> <ul> <li>Creates <code>self.peft_model</code> with LoRA adapters</li> <li>Prints trainable parameter statistics</li> </ul> <p>Example:</p> <pre><code>finetuner.load_model()\n\n# Default configuration\nfinetuner.setup_lora()\n\n# Custom configuration\nfinetuner.setup_lora(\n    r=16,\n    lora_alpha=64,\n    lora_dropout=0.15,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n</code></pre> <p>Output:</p> <pre><code>trainable params: 294,912 || all params: 124,439,808 || trainable%: 0.2370\n</code></pre>"},{"location":"api/#train","title":"train()","text":"<p>Train the model with LoRA adapters.</p> <pre><code>def train(\n    train_dataset: Dataset,\n    num_epochs: int = 3,\n    batch_size: int = 4,\n    learning_rate: float = 2e-4\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>train_dataset</code> (Dataset): Tokenized training dataset</li> <li><code>num_epochs</code> (int): Number of training epochs. Default: 3</li> <li><code>batch_size</code> (int): Per-device batch size. Default: 4</li> <li><code>learning_rate</code> (float): Learning rate. Default: 2e-4</li> </ul> <p>Returns: None</p> <p>Side Effects:</p> <ul> <li>Trains the model</li> <li>Saves checkpoints to output_dir</li> <li>Saves final model and tokenizer</li> </ul> <p>Training Configuration:</p> <ul> <li>Gradient accumulation: 4 steps</li> <li>FP16: Enabled on CUDA</li> <li>Logging: Every 10 steps</li> <li>Save strategy: Per epoch</li> </ul> <p>Example:</p> <pre><code>finetuner.load_model()\ndataset = finetuner.load_dataset_from_source(\"./data.jsonl\")\ntokenized, _ = finetuner.prepare_dataset(dataset)\nfinetuner.setup_lora()\n\nfinetuner.train(\n    train_dataset=tokenized,\n    num_epochs=3,\n    batch_size=8,\n    learning_rate=2e-4\n)\n</code></pre>"},{"location":"api/#benchmark","title":"benchmark()","text":"<p>Benchmark model performance using ROUGE scores.</p> <pre><code>def benchmark(\n    test_prompts: List[str],\n    use_finetuned: bool = False\n) -&gt; Dict[str, float]\n</code></pre> <p>Parameters:</p> <ul> <li><code>test_prompts</code> (List[str]): List of prompts to evaluate</li> <li><code>use_finetuned</code> (bool): Use fine-tuned model. Default: False (uses base model)</li> </ul> <p>Returns: Dictionary with ROUGE scores</p> <p>Metrics Computed:</p> <ul> <li>ROUGE-1: Unigram overlap</li> <li>ROUGE-2: Bigram overlap</li> <li>ROUGE-L: Longest common subsequence</li> </ul> <p>Example:</p> <pre><code>test_prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks.\",\n    \"Define artificial intelligence.\"\n]\n\n# Benchmark base model\nbase_scores = finetuner.benchmark(test_prompts, use_finetuned=False)\n\n# After training\nfinetuned_scores = finetuner.benchmark(test_prompts, use_finetuned=True)\n\n# Compare\nfor metric in base_scores:\n    improvement = (finetuned_scores[metric] - base_scores[metric]) / base_scores[metric] * 100\n    print(f\"{metric}: {improvement:+.2f}% improvement\")\n</code></pre> <p>Output Format:</p> <pre><code>{\n    'rouge1': 0.3245,\n    'rouge2': 0.2134,\n    'rougeL': 0.2987\n}\n</code></pre>"},{"location":"api/#upload_to_huggingface","title":"upload_to_huggingface()","text":"<p>Upload fine-tuned model to HuggingFace Hub.</p> <pre><code>def upload_to_huggingface(\n    repo_name: str,\n    token: Optional[str] = None,\n    create_new: bool = False,\n    private: bool = False\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>repo_name</code> (str): Repository name (format: \"username/repo-name\")</li> <li><code>token</code> (str, optional): HuggingFace API token. Uses cached login if None</li> <li><code>create_new</code> (bool): Create new repository. Default: False</li> <li><code>private</code> (bool): Make repository private. Default: False</li> </ul> <p>Returns: None</p> <p>Requirements:</p> <ul> <li>Valid HuggingFace token with write permissions</li> <li>Trained model in output_dir</li> </ul> <p>Example:</p> <pre><code># After training\nfinetuner.upload_to_huggingface(\n    repo_name=\"myusername/gpt2-finetuned\",\n    token=\"hf_xxxxxxxxxxxxx\",\n    create_new=True,\n    private=False\n)\n</code></pre> <p>Successful Upload:</p> <pre><code>\u2705 Model uploaded successfully to: https://huggingface.co/myusername/gpt2-finetuned\n</code></pre>"},{"location":"api/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/#complete-training-pipeline","title":"Complete Training Pipeline","text":"<pre><code>from finetune_cli import LLMFineTuner\n\n# Initialize\nfinetuner = LLMFineTuner(\"gpt2\", \"./my_model\")\n\n# Load model\nfinetuner.load_model()\n\n# Load and prepare data\ndataset = finetuner.load_dataset_from_source(\n    \"./data.jsonl\",\n    num_samples=5000\n)\ntokenized, cols = finetuner.prepare_dataset(dataset, max_length=512)\n\n# Pre-training benchmark\ntest_prompts = [\"Sample prompt 1\", \"Sample prompt 2\"]\nbase_scores = finetuner.benchmark(test_prompts, use_finetuned=False)\n\n# Setup LoRA\nfinetuner.setup_lora(r=8, lora_alpha=32, lora_dropout=0.1)\n\n# Train\nfinetuner.train(tokenized, num_epochs=3, batch_size=4, learning_rate=2e-4)\n\n# Post-training benchmark\nfinetuned_scores = finetuner.benchmark(test_prompts, use_finetuned=True)\n\n# Upload\nfinetuner.upload_to_huggingface(\n    \"username/model-name\",\n    create_new=True\n)\n</code></pre>"},{"location":"api/#loading-fine-tuned-model","title":"Loading Fine-tuned Model","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Load LoRA adapters\nmodel = PeftModel.from_pretrained(base_model, \"./my_model\")\n\n# Inference\nprompt = \"Hello, how are you?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>"},{"location":"api/#merging-adapters-optional","title":"Merging Adapters (Optional)","text":"<pre><code>from peft import PeftModel\n\n# Load model with adapters\nmodel = PeftModel.from_pretrained(base_model, \"./my_model\")\n\n# Merge adapters into base model\nmerged_model = model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(\"./merged_model\")\ntokenizer.save_pretrained(\"./merged_model\")\n</code></pre>"},{"location":"api/#helper-functions","title":"Helper Functions","text":""},{"location":"api/#get_user_input","title":"get_user_input()","text":"<p>Interactive prompt with optional default value.</p> <pre><code>def get_user_input(prompt: str, default: Optional[str] = None) -&gt; str\n</code></pre> <p>Example:</p> <pre><code>model_name = get_user_input(\"Enter model name\", \"gpt2\")\n# Prompt: Enter model name [gpt2]:\n</code></pre>"},{"location":"api/#type-definitions","title":"Type Definitions","text":"<pre><code>from typing import Optional, Dict, List, Tuple\nfrom datasets import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>The tool includes comprehensive error handling:</p> <pre><code>try:\n    finetuner.train(dataset)\nexcept RuntimeError as e:\n    if \"out of memory\" in str(e):\n        print(\"Reduce batch size or sequence length\")\n    raise\nexcept KeyboardInterrupt:\n    print(\"Training interrupted\")\n    sys.exit(0)\n</code></pre>"},{"location":"api/#next-steps","title":"Next Steps","text":"<ul> <li>See Usage Guide for CLI workflow</li> <li>Check Examples for practical use cases</li> <li>Review Configuration for parameter tuning</li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>This guide explains all configuration parameters and how to optimize them for your use case.</p>"},{"location":"configuration/#lora-parameters","title":"LoRA Parameters","text":"<p>LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds trainable rank decomposition matrices to existing weights.</p>"},{"location":"configuration/#rank-r","title":"Rank (r)","text":"<p>The rank of the low-rank matrices added to model layers.</p> <p>What it controls: The capacity of the adapter to learn new patterns.</p> <p>Values and Use Cases:</p> Rank Trainable Params Memory Use Case 4 ~0.1-0.5M Low Quick experiments, simple tasks 8 ~0.5-2M Moderate General purpose, balanced performance 16 ~2-8M Higher Complex tasks, significant adaptation 32 ~8-32M High Maximum quality, specialized domains <p>Choosing rank:</p> <pre><code># Simple classification or entity extraction\nr = 4\n\n# General text generation or summarization\nr = 8\n\n# Complex reasoning or domain adaptation\nr = 16\n\n# Specialized medical/legal/technical domains\nr = 32\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Higher rank: Better adaptation, handles complex patterns</li> <li>\u274c Higher rank: More memory, longer training, risk of overfitting</li> </ul>"},{"location":"configuration/#alpha","title":"Alpha (\u03b1)","text":"<p>Scaling factor applied to LoRA weights.</p> <p>What it controls: The influence of LoRA updates relative to pre-trained weights.</p> <p>Formula: <code>scaling = alpha / r</code></p> <p>Recommended values:</p> <ul> <li>Standard: <code>alpha = 2 \u00d7 r</code> (e.g., r=8, alpha=16)</li> <li>Conservative: <code>alpha = r</code> (less aggressive updates)</li> <li>Aggressive: <code>alpha = 4 \u00d7 r</code> (stronger adaptation)</li> </ul> <p>Examples:</p> <pre><code># Conservative (maintains more of base model)\nr = 8, alpha = 8    # scaling = 1.0\n\n# Standard (recommended)\nr = 8, alpha = 16   # scaling = 2.0\n\n# Aggressive (stronger fine-tuning)\nr = 8, alpha = 32   # scaling = 4.0\n</code></pre> <p>When to adjust:</p> <ul> <li>Increase alpha if model isn't adapting enough</li> <li>Decrease alpha if model forgets pre-trained knowledge</li> </ul>"},{"location":"configuration/#dropout","title":"Dropout","text":"<p>Regularization technique to prevent overfitting.</p> <p>What it controls: Probability of randomly disabling LoRA parameters during training.</p> <p>Values:</p> <ul> <li><code>0.0</code>: No dropout (risk of overfitting on small datasets)</li> <li><code>0.05</code>: Light regularization (large, diverse datasets)</li> <li><code>0.1</code>: Standard regularization (general purpose)</li> <li><code>0.2</code>: Heavy regularization (small or noisy datasets)</li> </ul> <p>Choosing dropout:</p> <pre><code># Large dataset (&gt; 50k samples), clean data\ndropout = 0.05\n\n# Medium dataset (5k-50k samples)\ndropout = 0.1\n\n# Small dataset (&lt; 5k samples) or noisy data\ndropout = 0.2\n</code></pre>"},{"location":"configuration/#target-modules","title":"Target Modules","text":"<p>Specifies which model layers to apply LoRA to.</p> <p>Auto-detection: The tool automatically identifies optimal target modules.</p> <p>Common patterns:</p> <pre><code># Attention layers only (memory efficient)\n[\"q_proj\", \"v_proj\"]\n\n# Full attention (recommended)\n[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n# Attention + MLP (maximum adaptation)\n[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"fc1\", \"fc2\"]\n</code></pre> <p>Manual override (advanced):</p> <p>You can modify the code to specify custom targets:</p> <pre><code>target_modules = [\"q_proj\", \"v_proj\"]  # Attention queries and values only\n</code></pre>"},{"location":"configuration/#training-parameters","title":"Training Parameters","text":""},{"location":"configuration/#number-of-epochs","title":"Number of Epochs","text":"<p>Complete passes through the training dataset.</p> <p>Guidelines by dataset size:</p> Dataset Size Recommended Epochs &lt; 1,000 samples 5-10 1,000-5,000 3-7 5,000-50,000 3-5 &gt; 50,000 1-3 <p>Signs of:</p> <ul> <li>Underfitting: Loss still decreasing, ROUGE scores improving</li> <li> <p>Solution: Increase epochs</p> </li> <li> <p>Overfitting: Training loss decreases but validation loss increases</p> </li> <li>Solution: Decrease epochs, increase dropout</li> </ul>"},{"location":"configuration/#batch-size","title":"Batch Size","text":"<p>Number of samples processed before updating model weights.</p> <p>Memory constraints:</p> GPU VRAM Model Size Max Batch Size 8GB Small (&lt; 500M params) 2-4 12GB Small-Medium 4-8 16GB Medium (1-3B params) 4-8 24GB Large (7B params) 8-16 <p>Effective batch size with gradient accumulation:</p> <pre><code># Config in training_args\nper_device_train_batch_size = 4\ngradient_accumulation_steps = 4\n# Effective batch size = 4 \u00d7 4 = 16\n</code></pre> <p>Tips:</p> <ul> <li>Start with batch_size=4 and adjust based on memory</li> <li>Smaller batches = more frequent updates, noisier gradients</li> <li>Larger batches = more stable gradients, better generalization</li> </ul>"},{"location":"configuration/#learning-rate","title":"Learning Rate","text":"<p>Step size for weight updates.</p> <p>Common values:</p> Learning Rate Use Case 1e-5 Very conservative, large models 5e-5 Conservative, stable training 1e-4 Moderate, good starting point 2e-4 Standard for LoRA (recommended) 5e-4 Aggressive, small models 1e-3 Very aggressive, risk of instability <p>Learning rate schedule:</p> <p>The tool uses a constant learning rate. For advanced use, you can modify to use:</p> <ul> <li>Linear decay</li> <li>Cosine decay</li> <li>Warmup + decay</li> </ul> <p>Signs of poor learning rate:</p> <ul> <li>Too high: Loss oscillates or diverges, NaN values</li> <li> <p>Solution: Reduce by 50% (e.g., 2e-4 \u2192 1e-4)</p> </li> <li> <p>Too low: Loss decreases very slowly</p> </li> <li>Solution: Increase by 2x (e.g., 1e-4 \u2192 2e-4)</li> </ul>"},{"location":"configuration/#maximum-sequence-length","title":"Maximum Sequence Length","text":"<p>Maximum number of tokens per training sample.</p> <p>Choosing max length:</p> <pre><code># Short texts (tweets, titles, Q&amp;A)\nmax_length = 128\n\n# Medium texts (paragraphs, summaries)\nmax_length = 512\n\n# Long texts (articles, documents)\nmax_length = 1024\n\n# Very long texts (research papers)\nmax_length = 2048\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Longer sequences: Better context understanding</li> <li>\u274c Longer sequences: Quadratic memory increase, slower training</li> </ul> <p>Memory impact:</p> <pre><code>Memory \u221d batch_size \u00d7 max_length\u00b2\n</code></pre> <p>Doubling max_length quadruples memory usage!</p>"},{"location":"configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"configuration/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Simulate larger batch sizes without more memory.</p> <p>In the code (line 222):</p> <pre><code>gradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n</code></pre> <p>Calculation:</p> <pre><code>Effective Batch Size = batch_size \u00d7 gradient_accumulation_steps \u00d7 num_gpus\n</code></pre>"},{"location":"configuration/#mixed-precision-fp16","title":"Mixed Precision (FP16)","text":"<p>Use 16-bit floating point for faster training and less memory.</p> <p>Automatically enabled when CUDA is available:</p> <pre><code>fp16 = self.device == \"cuda\"  # Line 228\n</code></pre> <p>Benefits:</p> <ul> <li>50% less memory</li> <li>2-3x faster training</li> <li>Minimal accuracy loss</li> </ul>"},{"location":"configuration/#model-quantization","title":"Model Quantization","text":"<p>For very large models, you can enable quantization:</p> <pre><code># Add to model loading (line 59)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_8bit=True,  # Quantize to 8-bit\n    device_map=\"auto\"\n)\n</code></pre>"},{"location":"configuration/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"configuration/#recipe-1-quick-experimentation","title":"Recipe 1: Quick Experimentation","text":"<pre><code>Samples: 1000\nMax Length: 256\nLoRA r: 4\nLoRA alpha: 16\nDropout: 0.1\nEpochs: 3\nBatch Size: 4\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Testing ideas, rapid iteration</p>"},{"location":"configuration/#recipe-2-balanced-quality","title":"Recipe 2: Balanced Quality","text":"<pre><code>Samples: 10000\nMax Length: 512\nLoRA r: 8\nLoRA alpha: 32\nDropout: 0.1\nEpochs: 3\nBatch Size: 8\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Production models, general tasks</p>"},{"location":"configuration/#recipe-3-maximum-quality","title":"Recipe 3: Maximum Quality","text":"<pre><code>Samples: 50000+\nMax Length: 1024\nLoRA r: 16\nLoRA alpha: 64\nDropout: 0.1\nEpochs: 3\nBatch Size: 8\nLearning Rate: 1e-4\n</code></pre> <p>Best for: Specialized domains, publication-quality results</p>"},{"location":"configuration/#recipe-4-memory-constrained","title":"Recipe 4: Memory-Constrained","text":"<pre><code>Samples: 5000\nMax Length: 256\nLoRA r: 4\nLoRA alpha: 16\nDropout: 0.1\nEpochs: 5\nBatch Size: 2\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Limited GPU memory (&lt; 8GB)</p>"},{"location":"configuration/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Start conservative: Use lower rank, smaller batch, fewer epochs</li> <li>Monitor metrics: Watch loss curves and ROUGE scores</li> <li>Iterate gradually: Increase one parameter at a time</li> <li>Save checkpoints: Keep best performing configurations</li> <li>Profile memory: Use <code>nvidia-smi</code> to track GPU usage</li> </ol>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>See practical Examples</li> <li>Understand Troubleshooting common issues</li> <li>Explore API Reference for programmatic usage</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Practical examples for common fine-tuning scenarios.</p>"},{"location":"examples/#example-1-fine-tune-gpt-2-on-custom-dialogue-data","title":"Example 1: Fine-tune GPT-2 on Custom Dialogue Data","text":""},{"location":"examples/#scenario","title":"Scenario","text":"<p>You have a JSONL file with conversational data and want to create a chatbot.</p>"},{"location":"examples/#dataset-format-dialoguejsonl","title":"Dataset Format (<code>dialogue.jsonl</code>)","text":"<pre><code>{\"prompt\": \"Hello, how are you?\", \"response\": \"I'm doing great! How can I help you today?\"}\n{\"prompt\": \"What's the weather like?\", \"response\": \"I don't have access to weather data, but you can check weather.com\"}\n</code></pre>"},{"location":"examples/#configuration","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: gpt2\nOutput directory: ./dialogue_model\nDataset path: ./dialogue.jsonl\nLimit samples: yes\nNumber of samples: 5000\nMax sequence length: 256\nLoRA r: 8\nLoRA alpha: 32\nLoRA dropout: 0.1\nEpochs: 5\nBatch size: 8\nLearning rate: 2e-4\nUpload to HuggingFace: no\n</code></pre>"},{"location":"examples/#expected-results","title":"Expected Results","text":"<pre><code>\ud83d\udcca PERFORMANCE COMPARISON\nMetric       Base Model      Fine-tuned      Improvement\nROUGE1       0.1823          0.3245          +78.03%\nROUGE2       0.0912          0.2134          +133.99%\nROUGEL       0.1645          0.2987          +81.58%\n</code></pre>"},{"location":"examples/#using-the-fine-tuned-model","title":"Using the Fine-tuned Model","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"./dialogue_model\")\n\n# Generate response\nprompt = \"Hello, how are you?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n</code></pre>"},{"location":"examples/#example-2-domain-adaptation-for-medical-text","title":"Example 2: Domain Adaptation for Medical Text","text":""},{"location":"examples/#scenario_1","title":"Scenario","text":"<p>Adapt a model to medical terminology and clinical notes.</p>"},{"location":"examples/#dataset","title":"Dataset","text":"<p>Using HuggingFace medical dataset.</p>"},{"location":"examples/#configuration_1","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: facebook/opt-350m\nOutput directory: ./medical_model\nDataset name: medical_meadow_medical_flashcards\nDataset config: default\nSplit: train\nLimit samples: yes\nNumber of samples: 20000\nMax sequence length: 512\nLoRA r: 16\nLoRA alpha: 64\nLoRA dropout: 0.15\nEpochs: 3\nBatch size: 4\nLearning rate: 1e-4\nUpload to HuggingFace: yes\nRepo name: myusername/opt-medical-350m\n</code></pre>"},{"location":"examples/#why-these-settings","title":"Why These Settings?","text":"<ul> <li>Higher rank (16): Medical domain requires learning specialized terminology</li> <li>Higher alpha (64): Stronger adaptation to domain-specific patterns</li> <li>More dropout (0.15): Medical text can be noisy, prevent overfitting</li> <li>Lower learning rate (1e-4): Conservative to preserve general knowledge</li> </ul>"},{"location":"examples/#example-3-summarization-task","title":"Example 3: Summarization Task","text":""},{"location":"examples/#scenario_2","title":"Scenario","text":"<p>Fine-tune for news article summarization.</p>"},{"location":"examples/#dataset-format-summariescsv","title":"Dataset Format (<code>summaries.csv</code>)","text":"<pre><code>article,summary\n\"Long article text here...\",\"Brief summary here...\"\n\"Another article...\",\"Another summary...\"\n</code></pre>"},{"location":"examples/#configuration_2","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: google/flan-t5-base\nOutput directory: ./summarization_model\nDataset path: ./summaries.csv\nLimit samples: yes\nNumber of samples: 15000\nMax sequence length: 1024\nLoRA r: 8\nLoRA alpha: 32\nLoRA dropout: 0.1\nEpochs: 3\nBatch size: 4\nLearning rate: 2e-4\n</code></pre>"},{"location":"examples/#data-preparation-tips","title":"Data Preparation Tips","text":"<p>The tool auto-detects columns. For best results:</p> <ol> <li>Name columns clearly: <code>article</code>, <code>text</code>, <code>summary</code>, <code>content</code></li> <li>Clean your data: Remove HTML tags, special characters</li> <li>Balance length: Keep articles similar length when possible</li> </ol>"},{"location":"examples/#example-4-code-generation","title":"Example 4: Code Generation","text":""},{"location":"examples/#scenario_3","title":"Scenario","text":"<p>Fine-tune on code examples for Python code generation.</p>"},{"location":"examples/#dataset_1","title":"Dataset","text":"<p>Using HuggingFace code dataset with specific file selection.</p>"},{"location":"examples/#configuration_3","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: Salesforce/codegen-350M-mono\nOutput directory: ./code_model\nDataset name: codeparrot/github-code\nDataset config: python\nLoad specific file: yes\nFile path: train-00000-of-00200.parquet\nNumber of samples: 10000\nMax sequence length: 512\nLoRA r: 8\nLoRA alpha: 32\nLoRA dropout: 0.05\nEpochs: 2\nBatch size: 8\nLearning rate: 2e-4\n</code></pre>"},{"location":"examples/#why-these-settings_1","title":"Why These Settings?","text":"<ul> <li>Lower dropout (0.05): Code has consistent structure, less noise</li> <li>Fewer epochs (2): Code datasets are large, less overfitting risk</li> <li>Specific file selection: Avoids loading entire 200-file dataset</li> </ul>"},{"location":"examples/#example-5-question-answering","title":"Example 5: Question Answering","text":""},{"location":"examples/#scenario_4","title":"Scenario","text":"<p>Fine-tune on SQuAD-style Q&amp;A data.</p>"},{"location":"examples/#configuration_4","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: distilbert-base-uncased\nOutput directory: ./qa_model\nDataset name: squad\nDataset config: plain_text\nSplit: train\nLimit samples: yes\nNumber of samples: 30000\nMax sequence length: 384\nLoRA r: 8\nLoRA alpha: 32\nLoRA dropout: 0.1\nEpochs: 3\nBatch size: 16\nLearning rate: 3e-4\n</code></pre>"},{"location":"examples/#inference-example","title":"Inference Example","text":"<pre><code>from transformers import pipeline\nfrom peft import PeftModel, AutoModelForQuestionAnswering\n\n# Load fine-tuned model\nbase_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\nmodel = PeftModel.from_pretrained(base_model, \"./qa_model\")\n\n# Create QA pipeline\nqa = pipeline(\"question-answering\", model=model, tokenizer=\"distilbert-base-uncased\")\n\n# Ask question\ncontext = \"Paris is the capital of France. It is known for the Eiffel Tower.\"\nquestion = \"What is the capital of France?\"\nresult = qa(question=question, context=context)\nprint(result['answer'])  # \"Paris\"\n</code></pre>"},{"location":"examples/#example-6-multi-language-fine-tuning","title":"Example 6: Multi-language Fine-tuning","text":""},{"location":"examples/#scenario_5","title":"Scenario","text":"<p>Fine-tune multilingual model for specific languages.</p>"},{"location":"examples/#configuration_5","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: xlm-roberta-base\nOutput directory: ./multilingual_model\nDataset name: Helsinki-NLP/tatoeba\nDataset config: eng-spa\nSplit: train\nLimit samples: yes\nNumber of samples: 25000\nMax sequence length: 128\nLoRA r: 16\nLoRA alpha: 32\nLoRA dropout: 0.1\nEpochs: 4\nBatch size: 8\nLearning rate: 2e-4\n</code></pre>"},{"location":"examples/#example-7-sentiment-analysis","title":"Example 7: Sentiment Analysis","text":""},{"location":"examples/#scenario_6","title":"Scenario","text":"<p>Adapt model for sentiment classification.</p>"},{"location":"examples/#dataset-format-sentimentjsonl","title":"Dataset Format (<code>sentiment.jsonl</code>)","text":"<pre><code>{\"text\": \"This product is amazing!\", \"sentiment\": \"positive\"}\n{\"text\": \"Terrible experience, would not recommend.\", \"sentiment\": \"negative\"}\n</code></pre>"},{"location":"examples/#configuration_6","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: roberta-base\nOutput directory: ./sentiment_model\nDataset path: ./sentiment.jsonl\nLimit samples: yes\nNumber of samples: 10000\nMax sequence length: 256\nLoRA r: 4\nLoRA alpha: 16\nLoRA dropout: 0.1\nEpochs: 5\nBatch size: 16\nLearning rate: 3e-4\n</code></pre>"},{"location":"examples/#why-these-settings_2","title":"Why These Settings?","text":"<ul> <li>Lower rank (4): Sentiment is relatively simple classification</li> <li>Higher batch size (16): Shorter sequences allow larger batches</li> <li>More epochs (5): Smaller dataset benefits from more iterations</li> </ul>"},{"location":"examples/#example-8-instruction-following","title":"Example 8: Instruction Following","text":""},{"location":"examples/#scenario_7","title":"Scenario","text":"<p>Fine-tune on instruction-response pairs.</p>"},{"location":"examples/#dataset_2","title":"Dataset","text":"<p>Using large instruction dataset with selective loading.</p>"},{"location":"examples/#configuration_7","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: EleutherAI/pythia-410m\nOutput directory: ./instruction_model\nDataset name: HuggingFaceH4/ultrachat_200k\nLoad specific file: yes\nFile path: data/train_sft-00000-of-00004.parquet\nNumber of samples: 15000\nMax sequence length: 1024\nLoRA r: 16\nLoRA alpha: 64\nLoRA dropout: 0.1\nEpochs: 2\nBatch size: 4\nLearning rate: 1e-4\nUpload to HuggingFace: yes\nRepo name: myusername/pythia-instruction-410m\nPrivate: no\n</code></pre>"},{"location":"examples/#best-practices-from-examples","title":"Best Practices from Examples","text":""},{"location":"examples/#dataset-size-guidelines","title":"Dataset Size Guidelines","text":"<ul> <li>Experiments: 1,000-5,000 samples</li> <li>Development: 5,000-20,000 samples</li> <li>Production: 20,000+ samples</li> </ul>"},{"location":"examples/#model-selection-tips","title":"Model Selection Tips","text":"<ol> <li>Start small: Test with GPT-2 or OPT-125M</li> <li>Match architecture: Use decoder models (GPT) for generation</li> <li>Consider license: Check model licensing for commercial use</li> <li>Resource awareness: Larger models need more VRAM</li> </ol>"},{"location":"examples/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ol> <li>\u274c Training on too few samples (&lt; 500)</li> <li>\u274c Using max_length longer than needed (wastes memory)</li> <li>\u274c Setting rank too high for simple tasks (overfitting)</li> <li>\u274c Forgetting to validate results before uploading</li> <li>\u274c Not monitoring GPU memory usage</li> </ol>"},{"location":"examples/#performance-optimization","title":"Performance Optimization","text":"<pre><code># For faster iteration during development:\n- Use smaller model variants\n- Limit samples to 1000-5000\n- Reduce max_length\n- Use rank 4-8\n\n# For production quality:\n- Use full dataset\n- Increase rank to 16\n- Train for more epochs\n- Validate on held-out test set\n</code></pre>"},{"location":"examples/#recipes-summary","title":"Recipes Summary","text":"Use Case Model Rank Samples Max Length Chatbot GPT-2 8 5k 256 Medical OPT-350M 16 20k 512 Summarization FLAN-T5 8 15k 1024 Code Gen CodeGen 8 10k 512 QA DistilBERT 8 30k 384 Multilingual XLM-R 16 25k 128 Sentiment RoBERTa 4 10k 256 Instructions Pythia 16 15k 1024"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration Options</li> <li>Check Troubleshooting for common issues</li> <li>Review API Reference for programmatic usage</li> </ul>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the LLM Fine-Tuning CLI Tool, ensure you have the following:</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>GPU: CUDA-capable GPU (optional but highly recommended)</li> <li>Minimum 8GB VRAM for small models (GPT-2, OPT-125M)</li> <li>16GB+ VRAM recommended for larger models</li> <li>RAM: 16GB minimum, 32GB recommended</li> <li>Storage: 10GB+ free space for model downloads and checkpoints</li> </ul>"},{"location":"installation/#software-dependencies","title":"Software Dependencies","text":"<ul> <li>CUDA Toolkit 11.8+ (for GPU acceleration)</li> <li>pip package manager</li> <li>Git (for cloning the repository)</li> </ul>"},{"location":"installation/#installation-steps","title":"Installation Steps","text":""},{"location":"installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/Abdur-azure/finetune_cli.git\ncd finetune_cli\n</code></pre>"},{"location":"installation/#2-create-virtual-environment-recommended","title":"2. Create Virtual Environment (Recommended)","text":"<p>Using <code>venv</code>:</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre> <p>Using <code>conda</code>:</p> <pre><code>conda create -n finetune python=3.10\nconda activate finetune\n</code></pre>"},{"location":"installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>This will install all required packages:</p> <ul> <li><code>torch&gt;=2.0.0</code> - PyTorch deep learning framework</li> <li><code>transformers&gt;=4.35.0</code> - HuggingFace Transformers library</li> <li><code>datasets&gt;=2.14.0</code> - HuggingFace Datasets library</li> <li><code>peft&gt;=0.7.0</code> - Parameter-Efficient Fine-Tuning library</li> <li><code>rouge-score&gt;=0.1.2</code> - ROUGE metric for evaluation</li> <li><code>huggingface-hub&gt;=0.19.0</code> - HuggingFace Hub integration</li> <li><code>accelerate&gt;=0.24.0</code> - Distributed training utilities</li> <li>Additional utilities (tqdm, pandas, sentencepiece, protobuf)</li> </ul>"},{"location":"installation/#4-verify-installation","title":"4. Verify Installation","text":"<p>Test that everything is installed correctly:</p> <pre><code>python -c \"import torch; print(f'PyTorch: {torch.__version__}')\"\npython -c \"import transformers; print(f'Transformers: {transformers.__version__}')\"\npython -c \"import torch; print(f'CUDA Available: {torch.cuda.is_available()}')\"\n</code></pre> <p>Expected output:</p> <pre><code>PyTorch: 2.x.x\nTransformers: 4.x.x\nCUDA Available: True  # or False if no GPU\n</code></pre>"},{"location":"installation/#gpu-setup-optional-but-recommended","title":"GPU Setup (Optional but Recommended)","text":""},{"location":"installation/#nvidia-gpu-with-cuda","title":"NVIDIA GPU with CUDA","text":"<ol> <li>Install NVIDIA drivers for your GPU</li> <li>Install CUDA Toolkit (11.8 or later):</li> <li>Download from NVIDIA CUDA Downloads</li> <li>Verify CUDA installation:</li> </ol> <pre><code>nvcc --version\nnvidia-smi\n</code></pre>"},{"location":"installation/#pytorch-with-cuda","title":"PyTorch with CUDA","text":"<p>If you need a specific CUDA version, install PyTorch separately:</p> <pre><code># For CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# For CUDA 12.1\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"installation/#huggingface-setup-optional","title":"HuggingFace Setup (Optional)","text":"<p>To upload models to HuggingFace Hub, you'll need an account and token:</p> <ol> <li>Create account at HuggingFace</li> <li>Generate token at Settings &gt; Access Tokens</li> <li>Login via CLI:</li> </ol> <pre><code>huggingface-cli login\n</code></pre> <p>Or set environment variable:</p> <pre><code>export HUGGING_FACE_HUB_TOKEN=\"your_token_here\"\n</code></pre>"},{"location":"installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":""},{"location":"installation/#issue-cuda-out-of-memory","title":"Issue: CUDA Out of Memory","text":"<p>Solution: Install CPU-only PyTorch or use a smaller model:</p> <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"installation/#issue-module-not-found-errors","title":"Issue: Module Not Found Errors","text":"<p>Solution: Upgrade pip and reinstall dependencies:</p> <pre><code>pip install --upgrade pip\npip install --upgrade -r requirements.txt\n</code></pre>"},{"location":"installation/#issue-slow-installation","title":"Issue: Slow Installation","text":"<p>Solution: Use a different PyPI mirror:</p> <pre><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre>"},{"location":"installation/#issue-permission-denied","title":"Issue: Permission Denied","text":"<p>Solution: Install in user space:</p> <pre><code>pip install --user -r requirements.txt\n</code></pre>"},{"location":"installation/#docker-installation-alternative","title":"Docker Installation (Alternative)","text":"<p>For a containerized environment:</p> <pre><code># Build image\ndocker build -t finetune-cli .\n\n# Run container\ndocker run -it --gpus all -v $(pwd):/workspace finetune-cli\n</code></pre> <p>Create a <code>Dockerfile</code>:</p> <pre><code>FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime\n\nWORKDIR /workspace\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"finetune_cli.py\"]\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete, proceed to the Usage Guide to start fine-tuning your first model.</p>"},{"location":"installation/#updating","title":"Updating","text":"<p>To update to the latest version:</p> <pre><code>git pull origin main\npip install --upgrade -r requirements.txt\n</code></pre>"},{"location":"training_and_evaluation/","title":"Training and Evaluation Guide","text":"<p>Complete guide to training and evaluating LLMs with the fine-tuning framework</p>"},{"location":"training_and_evaluation/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Overview</li> <li>Training Methods</li> <li>Training Configuration</li> <li>Training Examples</li> <li>Evaluation Metrics</li> <li>Benchmarking</li> <li>Complete Workflows</li> <li>Best Practices</li> </ol>"},{"location":"training_and_evaluation/#overview","title":"\ud83c\udfaf Overview","text":"<p>The framework provides three main training methods and comprehensive evaluation capabilities:</p>"},{"location":"training_and_evaluation/#training-methods","title":"Training Methods","text":"<ul> <li>LoRA - Parameter-efficient fine-tuning (0.1-1% parameters)</li> <li>QLoRA - Quantized LoRA for memory efficiency</li> <li>Full Fine-tuning - Train all parameters</li> </ul>"},{"location":"training_and_evaluation/#evaluation-features","title":"Evaluation Features","text":"<ul> <li>7 Metrics - ROUGE, BLEU, Perplexity, F1, Exact Match, Accuracy</li> <li>Benchmarking - Compare base vs fine-tuned models</li> <li>Reports - Generate Markdown, JSON, or HTML reports</li> </ul>"},{"location":"training_and_evaluation/#training-methods_1","title":"\ud83d\ude80 Training Methods","text":""},{"location":"training_and_evaluation/#1-lora-low-rank-adaptation","title":"1. LoRA (Low-Rank Adaptation)","text":"<p>Overview: - Trains only 0.1-1% of parameters - Adds lightweight adapter layers - ~50% memory savings vs full fine-tuning - Can merge adapters back into base model</p> <p>When to Use: - Most general-purpose fine-tuning tasks - Medium to large models (1B+ parameters) - When you need multiple task-specific adapters - Balance between quality and efficiency</p> <p>Configuration:</p> <pre><code>from finetune_cli.core.config import ConfigBuilder\nfrom finetune_cli.core.types import TrainingMethod\n\nconfig = ConfigBuilder() \\\n    .with_model(\"gpt2\") \\\n    .with_training(TrainingMethod.LORA, \"./output\", num_epochs=3) \\\n    .with_lora(\n        r=8,                    # Rank (4, 8, 16, 32)\n        lora_alpha=32,          # Scaling (typically 2-4x rank)\n        lora_dropout=0.1        # Dropout (0.05-0.2)\n    ) \\\n    .build()\n</code></pre> <p>LoRA Parameters:</p> Parameter Range Recommended Effect r (rank) 1-256 8-16 Adapter capacity; higher = more parameters alpha 1-256 2-4\u00d7 rank Scaling factor; affects learning strength dropout 0.0-0.5 0.1 Regularization; prevents overfitting <p>Example:</p> <pre><code>from finetune_cli.trainers import train_with_lora\n\nresult = train_with_lora(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_data,\n    training_config=training_config,\n    lora_config=lora_config,\n    eval_dataset=val_data\n)\n\nprint(f\"Final Loss: {result.final_loss:.4f}\")\nprint(f\"Training Time: {result.training_time_seconds:.2f}s\")\n</code></pre> <p>Advanced: Merge Adapters</p> <pre><code># After training\ntrainer = LoRATrainer(model, tokenizer, training_config, lora_config)\ntrainer.train(dataset)\n\n# Merge adapters into base model\ntrainer.merge_and_save(\"./merged_model\")\n# Now you have a standalone model without PEFT dependency\n</code></pre>"},{"location":"training_and_evaluation/#2-qlora-quantized-lora","title":"2. QLoRA (Quantized LoRA)","text":"<p>Overview: - LoRA on quantized models (4-bit or 8-bit) - ~75-88% memory savings vs full fine-tuning - Enables 7B+ models on consumer GPUs - Uses paged_adamw optimizer</p> <p>When to Use: - Large models (7B+ parameters) - Consumer GPUs with limited VRAM - Memory-constrained environments - When 4-bit quantization is acceptable</p> <p>Configuration:</p> <pre><code>config = ConfigBuilder() \\\n    .with_model(\n        \"meta-llama/Llama-2-7b-hf\",\n        load_in_4bit=True,           # Enable 4-bit quantization\n        device=DeviceType.AUTO\n    ) \\\n    .with_training(\n        TrainingMethod.QLORA,\n        \"./output\",\n        gradient_checkpointing=True   # Required for QLoRA\n    ) \\\n    .with_lora(\n        r=16,                         # Higher rank for quantized models\n        lora_alpha=64\n    ) \\\n    .build()\n</code></pre> <p>Memory Requirements (7B Model):</p> Method VRAM Required Memory Savings Full FT ~28GB - LoRA ~14GB 50% QLoRA (8-bit) ~7GB 75% QLoRA (4-bit) ~4GB 86% <p>Example:</p> <pre><code>from finetune_cli.trainers import train_with_qlora\n\nresult = train_with_qlora(\n    model=quantized_model,\n    tokenizer=tokenizer,\n    train_dataset=train_data,\n    training_config=training_config,\n    lora_config=lora_config,\n    model_config=model_config  # Contains quantization info\n)\n</code></pre> <p>Best Practices:</p> <pre><code>from finetune_cli.trainers import get_qlora_best_practices\n\npractices = get_qlora_best_practices()\n# Returns comprehensive recommendations for QLoRA training\n</code></pre>"},{"location":"training_and_evaluation/#3-full-fine-tuning","title":"3. Full Fine-tuning","text":"<p>Overview: - Trains all model parameters - Maximum adaptation capability - Highest memory requirements - Best for small models or unlimited GPU</p> <p>When to Use: - Small models (&lt;1B parameters) - Tasks requiring substantial model changes - When GPU memory is abundant - Maximum quality is priority</p> <p>Configuration:</p> <pre><code>config = ConfigBuilder() \\\n    .with_model(\"gpt2\") \\\n    .with_training(\n        TrainingMethod.FULL_FINETUNING,\n        \"./output\",\n        num_epochs=3,\n        batch_size=2,                # Smaller batch for memory\n        gradient_checkpointing=True  # Recommended\n    ) \\\n    .build()\n</code></pre> <p>Memory Estimation:</p> <pre><code>from finetune_cli.trainers import FullFineTuner\n\ntrainer = FullFineTuner(model, tokenizer, config)\nmemory = trainer.estimate_memory_usage()\n\nprint(f\"Parameters: {memory['parameters_gb']:.2f} GB\")\nprint(f\"Gradients: {memory['gradients_gb']:.2f} GB\")\nprint(f\"Optimizer: {memory['optimizer_gb']:.2f} GB\")\nprint(f\"Total: {memory['total_estimated_gb']:.2f} GB\")\n</code></pre> <p>Example:</p> <pre><code>from finetune_cli.trainers import train_full_finetuning\n\nresult = train_full_finetuning(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_data,\n    training_config=training_config,\n    eval_dataset=val_data\n)\n</code></pre>"},{"location":"training_and_evaluation/#training-configuration","title":"\u2699\ufe0f Training Configuration","text":""},{"location":"training_and_evaluation/#core-training-parameters","title":"Core Training Parameters","text":"<pre><code>training_config = TrainingConfig(\n    method=TrainingMethod.LORA,\n    output_dir=Path(\"./outputs/model\"),\n\n    # Training schedule\n    num_epochs=3,                    # Number of passes through data\n    batch_size=4,                    # Samples per GPU\n    gradient_accumulation_steps=4,   # Effective batch = 4 \u00d7 4 = 16\n\n    # Optimization\n    learning_rate=2e-4,              # Step size (1e-5 to 5e-4)\n    weight_decay=0.01,               # Regularization\n    warmup_ratio=0.1,                # 10% warmup\n    lr_scheduler_type=\"cosine\",      # Learning rate schedule\n    max_grad_norm=1.0,               # Gradient clipping\n\n    # Mixed precision\n    fp16=False,                      # FP16 training (NVIDIA GPUs)\n    bf16=False,                      # BF16 training (newer GPUs)\n\n    # Checkpointing\n    save_strategy=\"epoch\",           # When to save checkpoints\n    evaluation_strategy=\"epoch\",     # When to evaluate\n    load_best_model_at_end=True,    # Load best checkpoint after training\n\n    # Optimization\n    gradient_checkpointing=False,    # Trade compute for memory\n\n    # Reproducibility\n    seed=42\n)\n</code></pre>"},{"location":"training_and_evaluation/#learning-rate-selection","title":"Learning Rate Selection","text":"Model Size Recommended LR Range Small (&lt;500M) 2e-4 1e-4 to 5e-4 Medium (500M-3B) 1e-4 5e-5 to 2e-4 Large (3B+) 5e-5 1e-5 to 1e-4 <p>Learning Rate Schedulers: - <code>linear</code> - Linear decay to 0 - <code>cosine</code> - Cosine annealing (recommended) - <code>constant</code> - Fixed learning rate - <code>polynomial</code> - Polynomial decay</p>"},{"location":"training_and_evaluation/#batch-size-guidelines","title":"Batch Size Guidelines","text":"<p>Effective Batch Size = <code>batch_size \u00d7 gradient_accumulation_steps \u00d7 num_gpus</code></p> GPU VRAM Batch Size Accumulation Effective 8GB 2 4 8 12GB 4 4 16 16GB 8 4 32 24GB 16 4 64 <p>Tips: - Use gradient accumulation to simulate larger batches - Larger effective batches = more stable training - Smaller batches = faster iterations but noisier</p>"},{"location":"training_and_evaluation/#training-examples","title":"\ud83d\udcdd Training Examples","text":""},{"location":"training_and_evaluation/#example-1-quick-lora-training","title":"Example 1: Quick LoRA Training","text":"<pre><code>from finetune_cli.core.config import ConfigBuilder\nfrom finetune_cli.core.types import TrainingMethod, DatasetSource\nfrom finetune_cli.models.loader import load_model_and_tokenizer\nfrom finetune_cli.data import prepare_dataset\nfrom finetune_cli.trainers import train_model\n\n# 1. Configuration\nconfig = ConfigBuilder() \\\n    .with_model(\"gpt2\") \\\n    .with_dataset(\"./data.jsonl\", source=DatasetSource.LOCAL_FILE) \\\n    .with_tokenization(max_length=512) \\\n    .with_training(TrainingMethod.LORA, \"./output\") \\\n    .with_lora(r=8, lora_alpha=32) \\\n    .build()\n\n# 2. Load model\nmodel, tokenizer = load_model_and_tokenizer(config.model.to_config())\n\n# 3. Prepare data\ndataset = prepare_dataset(\n    config.dataset.to_config(),\n    config.tokenization.to_config(),\n    tokenizer\n)\n\n# 4. Train\nresult = train_model(\n    model, tokenizer, dataset,\n    config.training.to_config(),\n    config.lora.to_config()\n)\n\nprint(f\"Training complete! Loss: {result.final_loss:.4f}\")\n</code></pre>"},{"location":"training_and_evaluation/#example-2-qlora-on-large-model","title":"Example 2: QLoRA on Large Model","text":"<pre><code># Configure for 7B model on 12GB GPU\nconfig = ConfigBuilder() \\\n    .with_model(\n        \"meta-llama/Llama-2-7b-hf\",\n        load_in_4bit=True,\n        device=DeviceType.CUDA\n    ) \\\n    .with_dataset(\"HuggingFaceH4/ultrachat_200k\", max_samples=10000) \\\n    .with_training(\n        TrainingMethod.QLORA,\n        \"./outputs/llama-qlora\",\n        num_epochs=2,\n        batch_size=2,\n        gradient_accumulation_steps=8\n    ) \\\n    .with_lora(r=16, lora_alpha=64, lora_dropout=0.1) \\\n    .build()\n\n# Train\nmodel, tokenizer = load_model_and_tokenizer(config.model.to_config())\ndataset = prepare_dataset(...)\nresult = train_model(model, tokenizer, dataset, ...)\n</code></pre>"},{"location":"training_and_evaluation/#example-3-resume-from-checkpoint","title":"Example 3: Resume from Checkpoint","text":"<pre><code>from pathlib import Path\n\n# Original training\nresult = train_model(\n    model, tokenizer, dataset,\n    training_config,\n    lora_config\n)\n\n# Resume from checkpoint\ncheckpoint_path = Path(\"./outputs/model/checkpoint-100\")\nresult = train_model(\n    model, tokenizer, dataset,\n    training_config,\n    lora_config,\n    resume_from_checkpoint=checkpoint_path\n)\n</code></pre>"},{"location":"training_and_evaluation/#example-4-training-with-validation","title":"Example 4: Training with Validation","text":"<pre><code># Prepare data with splits\nsplits = prepare_dataset(\n    dataset_config,\n    tokenization_config,\n    tokenizer,\n    split_for_validation=True,\n    validation_ratio=0.1\n)\n\n# Train with validation\nresult = train_model(\n    model, tokenizer,\n    train_dataset=splits['train'],\n    eval_dataset=splits['validation'],\n    training_config=training_config,\n    lora_config=lora_config\n)\n</code></pre>"},{"location":"training_and_evaluation/#evaluation-metrics","title":"\ud83d\udcca Evaluation Metrics","text":""},{"location":"training_and_evaluation/#available-metrics","title":"Available Metrics","text":"Metric Description Range Higher is Better ROUGE-1 Unigram overlap 0-1 \u2713 ROUGE-2 Bigram overlap 0-1 \u2713 ROUGE-L Longest common subsequence 0-1 \u2713 BLEU N-gram precision 0-1 \u2713 Perplexity Prediction quality 1-\u221e \u2717 (lower is better) F1 Token-level F1 score 0-1 \u2713 Exact Match Exact string match 0-1 \u2713"},{"location":"training_and_evaluation/#rouge-metrics","title":"ROUGE Metrics","text":"<p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</p> <p>Measures overlap between generated and reference text.</p> <pre><code>from finetune_cli.evaluation import evaluate_model\nfrom finetune_cli.core.types import EvaluationMetric\n\nconfig = EvaluationConfig(\n    metrics=[\n        EvaluationMetric.ROUGE_1,  # Unigram overlap\n        EvaluationMetric.ROUGE_2,  # Bigram overlap\n        EvaluationMetric.ROUGE_L   # Longest common subsequence\n    ],\n    batch_size=8\n)\n\nresult = evaluate_model(model, tokenizer, test_dataset, config)\nprint(f\"ROUGE-1: {result.metrics['rouge1']:.4f}\")\nprint(f\"ROUGE-2: {result.metrics['rouge2']:.4f}\")\nprint(f\"ROUGE-L: {result.metrics['rougeL']:.4f}\")\n</code></pre> <p>When to Use: - Summarization tasks - Text generation quality - Content preservation</p>"},{"location":"training_and_evaluation/#bleu-score","title":"BLEU Score","text":"<p>BLEU (Bilingual Evaluation Understudy)</p> <p>Measures n-gram precision between generated and reference.</p> <pre><code>config = EvaluationConfig(\n    metrics=[EvaluationMetric.BLEU],\n    batch_size=8\n)\n\nresult = evaluate_model(model, tokenizer, test_dataset, config)\nprint(f\"BLEU: {result.metrics['bleu']:.4f}\")\n</code></pre> <p>When to Use: - Translation tasks - Paraphrase generation - When precision matters more than recall</p>"},{"location":"training_and_evaluation/#perplexity","title":"Perplexity","text":"<p>Measures how well the model predicts text. Lower is better.</p> <pre><code>config = EvaluationConfig(\n    metrics=[EvaluationMetric.PERPLEXITY],\n    batch_size=8\n)\n\nresult = evaluate_model(model, tokenizer, test_dataset, config)\nprint(f\"Perplexity: {result.metrics['perplexity']:.2f}\")\n</code></pre> <p>When to Use: - Language modeling quality - Model comparison - Measuring fluency</p> <p>Interpretation: - <code>1.0</code> - Perfect prediction - <code>10-50</code> - Excellent - <code>50-100</code> - Good - <code>100+</code> - Poor</p>"},{"location":"training_and_evaluation/#f1-score","title":"F1 Score","text":"<p>Token-level precision and recall balance.</p> <pre><code>config = EvaluationConfig(\n    metrics=[EvaluationMetric.F1],\n    batch_size=8\n)\n\nresult = evaluate_model(model, tokenizer, test_dataset, config)\nprint(f\"F1: {result.metrics['f1']:.4f}\")\n</code></pre> <p>When to Use: - Information extraction - Question answering - When both precision and recall matter</p>"},{"location":"training_and_evaluation/#exact-match","title":"Exact Match","text":"<p>Percentage of predictions that exactly match reference.</p> <pre><code>from finetune_cli.evaluation.metrics import ExactMatchMetric\n\n# With normalization\nmetric = ExactMatchMetric(\n    ignore_case=True,\n    ignore_punctuation=True\n)\n\n# Without normalization\nmetric_strict = ExactMatchMetric(\n    ignore_case=False,\n    ignore_punctuation=False\n)\n</code></pre> <p>When to Use: - Classification tasks - Structured output generation - Strict correctness requirements</p>"},{"location":"training_and_evaluation/#benchmarking","title":"\ud83d\udd2c Benchmarking","text":""},{"location":"training_and_evaluation/#basic-benchmarking","title":"Basic Benchmarking","text":"<p>Compare base model vs fine-tuned model:</p> <pre><code>from finetune_cli.evaluation import benchmark_models\n\nresult = benchmark_models(\n    base_model=base_model,\n    finetuned_model=finetuned_model,\n    tokenizer=tokenizer,\n    dataset=test_dataset,\n    config=eval_config\n)\n\nprint(f\"Average Improvement: {result.get_average_improvement():.2f}%\")\n</code></pre> <p>Output:</p> <pre><code>BENCHMARK RESULTS\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nMetric          Base         Fine-tuned   Improvement    \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrouge1          0.2345       0.3421       +45.86%\nrouge2          0.1234       0.2156       +74.71%\nrougeL          0.2123       0.3089       +45.50%\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nAverage Improvement: +55.36%\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n</code></pre>"},{"location":"training_and_evaluation/#generate-reports","title":"Generate Reports","text":"<pre><code>from finetune_cli.evaluation import ReportGenerator\nfrom pathlib import Path\n\n# Markdown report\nReportGenerator.save_report(\n    result,\n    Path(\"./reports/benchmark.md\"),\n    format=\"markdown\",\n    title=\"GPT-2 LoRA Fine-tuning Results\"\n)\n\n# HTML report\nReportGenerator.save_report(\n    result,\n    Path(\"./reports/benchmark.html\"),\n    format=\"html\"\n)\n\n# JSON report\nReportGenerator.save_report(\n    result,\n    Path(\"./reports/benchmark.json\"),\n    format=\"json\"\n)\n</code></pre>"},{"location":"training_and_evaluation/#compare-pre-computed-metrics","title":"Compare Pre-computed Metrics","text":"<pre><code>from finetune_cli.evaluation import compare_metrics\n\nbase_metrics = {\n    'rouge1': 0.25,\n    'rouge2': 0.15,\n    'rougeL': 0.22\n}\n\nfinetuned_metrics = {\n    'rouge1': 0.35,\n    'rouge2': 0.23,\n    'rougeL': 0.31\n}\n\nresult = compare_metrics(base_metrics, finetuned_metrics)\nprint(result.improvements)\n# Output: {'rouge1': 40.0, 'rouge2': 53.33, 'rougeL': 40.91}\n</code></pre>"},{"location":"training_and_evaluation/#complete-workflows","title":"\ud83d\udd04 Complete Workflows","text":""},{"location":"training_and_evaluation/#workflow-1-train-and-evaluate","title":"Workflow 1: Train and Evaluate","text":"<pre><code>from finetune_cli.core.config import ConfigBuilder\nfrom finetune_cli.core.types import TrainingMethod, EvaluationMetric\nfrom finetune_cli.models.loader import load_model_and_tokenizer\nfrom finetune_cli.data import prepare_dataset\nfrom finetune_cli.trainers import train_model\nfrom finetune_cli.evaluation import evaluate_model\n\n# 1. Configuration\nconfig = ConfigBuilder() \\\n    .with_model(\"gpt2\") \\\n    .with_dataset(\"./data.jsonl\", max_samples=5000) \\\n    .with_tokenization(max_length=512) \\\n    .with_training(TrainingMethod.LORA, \"./output\") \\\n    .with_lora(r=8, lora_alpha=32) \\\n    .with_evaluation(\n        metrics=[\n            EvaluationMetric.ROUGE_1,\n            EvaluationMetric.ROUGE_L,\n            EvaluationMetric.PERPLEXITY\n        ]\n    ) \\\n    .build()\n\n# 2. Load model\nmodel, tokenizer = load_model_and_tokenizer(config.model.to_config())\n\n# 3. Prepare data with splits\nsplits = prepare_dataset(\n    config.dataset.to_config(),\n    config.tokenization.to_config(),\n    tokenizer,\n    split_for_validation=True,\n    validation_ratio=0.2\n)\n\n# 4. Train\ntrain_result = train_model(\n    model, tokenizer,\n    train_dataset=splits['train'],\n    eval_dataset=splits['validation'],\n    training_config=config.training.to_config(),\n    lora_config=config.lora.to_config()\n)\n\n# 5. Evaluate\neval_result = evaluate_model(\n    model, tokenizer,\n    splits['validation'],\n    config.evaluation.to_config()\n)\n\nprint(f\"Training Loss: {train_result.final_loss:.4f}\")\nprint(f\"ROUGE-1: {eval_result.metrics['rouge1']:.4f}\")\n</code></pre>"},{"location":"training_and_evaluation/#workflow-2-beforeafter-comparison","title":"Workflow 2: Before/After Comparison","text":"<pre><code>from finetune_cli.evaluation import benchmark_models\nfrom pathlib import Path\n\n# Load base model\nbase_model, tokenizer = load_model_and_tokenizer(model_config)\n\n# Train\nfinetuned_model = base_model  # Will be modified by training\ntrain_result = train_model(...)\n\n# Reload base model for fair comparison\nbase_model_fresh, _ = load_model_and_tokenizer(model_config)\n\n# Benchmark\nbenchmark_result = benchmark_models(\n    base_model=base_model_fresh,\n    finetuned_model=finetuned_model,\n    tokenizer=tokenizer,\n    dataset=test_dataset,\n    config=eval_config,\n    save_report=Path(\"./benchmark_report.md\")\n)\n\nprint(f\"Average Improvement: {benchmark_result.get_average_improvement():.2f}%\")\n</code></pre>"},{"location":"training_and_evaluation/#workflow-3-quick-evaluation","title":"Workflow 3: Quick Evaluation","text":"<pre><code>from finetune_cli.evaluation import quick_evaluate\n\n# Fast evaluation without configuration\ntest_inputs = [\n    \"What is machine learning?\",\n    \"Explain neural networks.\",\n    \"Define artificial intelligence.\"\n]\n\ntest_references = [\n    \"Machine learning is a subset of AI...\",\n    \"Neural networks are computing systems...\",\n    \"Artificial intelligence is the simulation...\"\n]\n\nscores = quick_evaluate(model, tokenizer, test_inputs, test_references)\nprint(f\"ROUGE-1: {scores['rouge1']:.4f}\")\nprint(f\"ROUGE-L: {scores['rougeL']:.4f}\")\n</code></pre>"},{"location":"training_and_evaluation/#best-practices","title":"\ud83d\udca1 Best Practices","text":""},{"location":"training_and_evaluation/#training-best-practices","title":"Training Best Practices","text":"<p>1. Start Small, Scale Up</p> <pre><code># Development\nconfig = ConfigBuilder() \\\n    .with_dataset(\"./data.jsonl\", max_samples=1000) \\\n    .with_training(num_epochs=1, batch_size=2) \\\n    .with_lora(r=4) \\\n    .build()\n\n# Production\nconfig = ConfigBuilder() \\\n    .with_dataset(\"./data.jsonl\", max_samples=None) \\\n    .with_training(num_epochs=3, batch_size=8) \\\n    .with_lora(r=16) \\\n    .build()\n</code></pre> <p>2. Monitor Training</p> <pre><code># Enable logging\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Save training state\nfrom finetune_cli.trainers import LoRATrainer\n\ntrainer = LoRATrainer(...)\nresult = trainer.train(dataset)\n\n# Access training state\nprint(f\"Best loss: {trainer.state.best_loss:.4f}\")\nprint(f\"Best epoch: {trainer.state.best_epoch}\")\nprint(f\"Loss history: {trainer.state.loss_history}\")\n</code></pre> <p>3. Use Validation Sets</p> <pre><code># Always split data\nsplits = prepare_dataset(..., split_for_validation=True)\n\n# Train with validation\nresult = train_model(\n    train_dataset=splits['train'],\n    eval_dataset=splits['validation'],  # Important!\n    ...\n)\n</code></pre> <p>4. Handle OOM Errors</p> <pre><code>try:\n    result = train_model(...)\nexcept OutOfMemoryError as e:\n    print(f\"OOM: {e}\")\n    print(\"Suggestions:\")\n    print(\"- Reduce batch_size\")\n    print(\"- Reduce max_length\")\n    print(\"- Enable gradient_checkpointing\")\n    print(\"- Use lower LoRA rank\")\n</code></pre>"},{"location":"training_and_evaluation/#evaluation-best-practices","title":"Evaluation Best Practices","text":"<p>1. Use Multiple Metrics</p> <pre><code>config = EvaluationConfig(\n    metrics=[\n        EvaluationMetric.ROUGE_1,\n        EvaluationMetric.ROUGE_2,\n        EvaluationMetric.ROUGE_L,\n        EvaluationMetric.BLEU,\n        EvaluationMetric.F1\n    ]\n)\n</code></pre> <p>2. Separate Test Set</p> <pre><code># Never evaluate on training data!\nsplits = prepare_dataset(..., validation_ratio=0.2)\n\n# Use held-out validation set\neval_result = evaluate_model(\n    model, tokenizer,\n    splits['validation'],  # Not training set!\n    eval_config\n)\n</code></pre> <p>3. Consistent Evaluation</p> <pre><code># Use same config for fair comparison\neval_config = EvaluationConfig(\n    metrics=[...],\n    batch_size=8,\n    generation_max_length=100,\n    generation_temperature=0.7\n)\n\nbase_result = evaluate_model(base_model, ..., eval_config)\nft_result = evaluate_model(finetuned_model, ..., eval_config)\n</code></pre>"},{"location":"training_and_evaluation/#method-selection","title":"Method Selection","text":"<p>Use the Method Recommender:</p> <pre><code>from finetune_cli.trainers import MethodRecommender\n\nrecommendation = MethodRecommender.recommend(\n    model_size_params=124e6,   # GPT-2 small\n    available_vram_gb=8.0,\n    task_complexity=\"medium\"\n)\n\nprint(f\"Recommended: {recommendation['recommendation'].value}\")\nprint(f\"Reason: {recommendation['reason']}\")\n</code></pre> <p>Decision Tree:</p> <pre><code>\u250c\u2500 Memory abundant (24GB+) ?\n\u2502  \u2514\u2500 Yes \u2192 Full Fine-tuning\n\u2502  \u2514\u2500 No \u2192 Continue\n\u2502\n\u251c\u2500 Model size &lt; 1B ?\n\u2502  \u2514\u2500 Yes \u2192 LoRA or Full Fine-tuning\n\u2502  \u2514\u2500 No \u2192 Continue\n\u2502\n\u251c\u2500 Model size &lt; 7B ?\n\u2502  \u2514\u2500 Yes \u2192 LoRA\n\u2502  \u2514\u2500 No \u2192 QLoRA\n\u2502\n\u2514\u2500 Model size 7B+ ?\n   \u2514\u2500 Yes \u2192 QLoRA (4-bit)\n</code></pre>"},{"location":"training_and_evaluation/#additional-resources","title":"\ud83d\udcda Additional Resources","text":""},{"location":"training_and_evaluation/#configuration-examples","title":"Configuration Examples","text":"<p>See <code>examples/complete_training_pipeline.py</code> for: - LoRA training - Full fine-tuning - Method recommendation - Checkpoint resumption - Config file usage</p>"},{"location":"training_and_evaluation/#api-reference","title":"API Reference","text":"<ul> <li>Trainers API</li> <li>Evaluation API</li> <li>Configuration API</li> </ul>"},{"location":"training_and_evaluation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Training Issues</li> <li>Memory Problems</li> <li>Evaluation Issues</li> </ul>"},{"location":"training_and_evaluation/#summary","title":"\ud83c\udf93 Summary","text":""},{"location":"training_and_evaluation/#quick-reference","title":"Quick Reference","text":"Task Command Train with LoRA <code>train_model(..., TrainingMethod.LORA, lora_config)</code> Train with QLoRA <code>train_model(..., TrainingMethod.QLORA, lora_config, model_config)</code> Full fine-tuning <code>train_model(..., TrainingMethod.FULL_FINETUNING)</code> Evaluate model <code>evaluate_model(model, tokenizer, dataset, config)</code> Benchmark models <code>benchmark_models(base, finetuned, tokenizer, dataset, config)</code> Quick eval <code>quick_evaluate(model, tokenizer, inputs, references)</code>"},{"location":"training_and_evaluation/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Start with LoRA for most use cases</li> <li>Use QLoRA for large models on consumer GPUs</li> <li>Monitor training with validation sets</li> <li>Evaluate comprehensively with multiple metrics</li> <li>Benchmark before/after to measure improvement</li> <li>Save checkpoints frequently</li> <li>Use gradient checkpointing if memory constrained</li> </ol> <p>Last Updated: 2025-01-29 Version: 2.0.0</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Solutions to common issues when using the fine-tuning tool.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#issue-cuda-not-available","title":"Issue: CUDA Not Available","text":"<p>Symptoms:</p> <pre><code>CUDA Available: False\nDevice: cpu\n</code></pre> <p>Causes:</p> <ol> <li>No NVIDIA GPU</li> <li>CUDA drivers not installed</li> <li>PyTorch installed without CUDA support</li> </ol> <p>Solutions:</p> <p>Check GPU:</p> <pre><code>nvidia-smi\n</code></pre> <p>Reinstall PyTorch with CUDA:</p> <pre><code>pip uninstall torch\npip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre>"},{"location":"troubleshooting/#issue-module-not-found-error","title":"Issue: Module Not Found Error","text":"<p>Symptoms:</p> <pre><code>ModuleNotFoundError: No module named 'peft'\n</code></pre> <p>Solution:</p> <pre><code>pip install --upgrade -r requirements.txt\n</code></pre> <p>If issue persists:</p> <pre><code>pip install peft transformers datasets --upgrade\n</code></pre>"},{"location":"troubleshooting/#issue-version-conflicts","title":"Issue: Version Conflicts","text":"<p>Symptoms:</p> <pre><code>ERROR: pip's dependency resolver...\n</code></pre> <p>Solution:</p> <p>Create fresh virtual environment:</p> <pre><code>python -m venv fresh_env\nsource fresh_env/bin/activate  # Windows: fresh_env\\Scripts\\activate\npip install -r requirements.txt\n</code></pre>"},{"location":"troubleshooting/#memory-issues","title":"Memory Issues","text":""},{"location":"troubleshooting/#issue-cuda-out-of-memory-oom","title":"Issue: CUDA Out of Memory (OOM)","text":"<p>Symptoms:</p> <pre><code>RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB\n</code></pre> <p>Solutions (try in order):</p> <p>1. Reduce batch size:</p> <pre><code>Batch size: 2  # Instead of 4 or 8\n</code></pre> <p>2. Reduce max sequence length:</p> <pre><code>Max length: 256  # Instead of 512 or 1024\n</code></pre> <p>3. Reduce LoRA rank:</p> <pre><code>LoRA r: 4  # Instead of 8 or 16\n</code></pre> <p>4. Limit number of samples:</p> <pre><code>Number of samples: 1000  # For testing\n</code></pre> <p>5. Enable gradient checkpointing (edit code):</p> <pre><code># Add to model loading (line 59)\nmodel.gradient_checkpointing_enable()\n</code></pre> <p>6. Use smaller model:</p> <pre><code>Model name: gpt2  # Instead of gpt2-medium or gpt2-large\n</code></pre> <p>Memory calculation formula:</p> <pre><code>Required VRAM \u2248 batch_size \u00d7 max_length\u00b2 \u00d7 model_size / 1e9 GB\n</code></pre>"},{"location":"troubleshooting/#issue-cpu-out-of-memory","title":"Issue: CPU Out of Memory","text":"<p>Symptoms:</p> <pre><code>MemoryError: Unable to allocate array\n</code></pre> <p>Solutions:</p> <p>1. Limit dataset size:</p> <pre><code>Number of samples: 5000\n</code></pre> <p>2. Use streaming for large datasets:</p> <p>Modify code to add streaming:</p> <pre><code>dataset = load_dataset(dataset_source, split=split, streaming=True)\n</code></pre> <p>3. Increase system swap:</p> <pre><code># Linux\nsudo fallocate -l 16G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n</code></pre>"},{"location":"troubleshooting/#training-issues","title":"Training Issues","text":""},{"location":"troubleshooting/#issue-loss-not-decreasing","title":"Issue: Loss Not Decreasing","text":"<p>Symptoms:</p> <pre><code>Epoch 1: Loss 3.45\nEpoch 2: Loss 3.44\nEpoch 3: Loss 3.43\n</code></pre> <p>Causes &amp; Solutions:</p> <p>1. Learning rate too low:</p> <pre><code>Learning rate: 2e-4  # Increase from 1e-4\n</code></pre> <p>2. Model frozen: Check that LoRA is properly applied:</p> <pre><code>\ud83c\udfaf Setting up LoRA configuration...\ntrainable params: X || all params: Y || trainable%: Z\n</code></pre> <p>3. Insufficient training:</p> <pre><code>Epochs: 5  # Increase from 3\nSamples: 10000  # Increase from 1000\n</code></pre> <p>4. Data quality issues: - Check dataset has meaningful text - Verify columns are correctly detected - Ensure no empty or null values</p>"},{"location":"troubleshooting/#issue-loss-diverging-nan","title":"Issue: Loss Diverging (NaN)","text":"<p>Symptoms:</p> <pre><code>Epoch 1: Loss 2.34\nEpoch 2: Loss 12.45\nEpoch 3: Loss NaN\n</code></pre> <p>Causes &amp; Solutions:</p> <p>1. Learning rate too high:</p> <pre><code>Learning rate: 1e-4  # Reduce from 5e-4 or 1e-3\n</code></pre> <p>2. Gradient explosion:</p> <p>Add gradient clipping (edit code):</p> <pre><code># In TrainingArguments (line 223)\nmax_grad_norm=1.0\n</code></pre> <p>3. Data issues: - Remove extreme outliers - Check for special characters causing issues - Normalize text inputs</p>"},{"location":"troubleshooting/#issue-overfitting","title":"Issue: Overfitting","text":"<p>Symptoms:</p> <ul> <li>Training loss decreases but validation would increase</li> <li>ROUGE scores decrease on new data</li> <li>Model outputs repetitive text</li> </ul> <p>Solutions:</p> <p>1. Increase dropout:</p> <pre><code>LoRA dropout: 0.2  # Increase from 0.1\n</code></pre> <p>2. Reduce epochs:</p> <pre><code>Epochs: 2  # Reduce from 5\n</code></pre> <p>3. Add more training data:</p> <pre><code>Number of samples: 20000  # Increase from 5000\n</code></pre> <p>4. Reduce model capacity:</p> <pre><code>LoRA r: 4  # Reduce from 8 or 16\n</code></pre> <p>5. Early stopping (edit code):</p> <pre><code># Add to TrainingArguments\nearly_stopping_patience=2\n</code></pre>"},{"location":"troubleshooting/#dataset-issues","title":"Dataset Issues","text":""},{"location":"troubleshooting/#issue-no-text-columns-detected","title":"Issue: No Text Columns Detected","text":"<p>Symptoms:</p> <pre><code>ValueError: No text columns found in dataset\n</code></pre> <p>Solutions:</p> <p>Check dataset structure:</p> <pre><code>print(dataset.column_names)\nprint(dataset[0])\n</code></pre> <p>Manual column specification (edit code around line 120):</p> <pre><code>text_columns = [\"my_text_column\", \"my_content_column\"]\ntokenized_dataset, _ = finetuner.prepare_dataset(dataset, text_columns=text_columns)\n</code></pre>"},{"location":"troubleshooting/#issue-dataset-too-large","title":"Issue: Dataset Too Large","text":"<p>Symptoms: - Slow loading - Memory issues - Long preprocessing</p> <p>Solutions:</p> <p>1. Use selective file loading:</p> <pre><code>Load specific file: yes\nFile path: train-00000-of-00100.parquet  # Load only one shard\n</code></pre> <p>2. Limit samples aggressively:</p> <pre><code>Number of samples: 5000\n</code></pre> <p>3. Use streaming mode:</p> <p>Modify dataset loading:</p> <pre><code>dataset = load_dataset(dataset_source, streaming=True)\ndataset = dataset.take(num_samples)\n</code></pre>"},{"location":"troubleshooting/#issue-column-names-not-recognized","title":"Issue: Column Names Not Recognized","text":"<p>Symptoms:</p> <p>Tool doesn't detect your text columns properly.</p> <p>Common column names recognized:</p> <ul> <li><code>text</code>, <code>content</code>, <code>input</code>, <code>output</code></li> <li><code>prompt</code>, <code>response</code>, <code>instruction</code></li> <li><code>question</code>, <code>answer</code>, <code>summary</code></li> </ul> <p>Solution:</p> <p>Rename your columns or modify detection logic (line 103):</p> <pre><code>common_names = ['text', 'content', 'your_column_name']\n</code></pre>"},{"location":"troubleshooting/#model-issues","title":"Model Issues","text":""},{"location":"troubleshooting/#issue-model-not-found","title":"Issue: Model Not Found","text":"<p>Symptoms:</p> <pre><code>OSError: Can't find model 'xyz'\n</code></pre> <p>Solution:</p> <p>Verify model exists: - Check HuggingFace Models - Ensure exact name match (case-sensitive)</p> <p>Common model names:</p> <pre><code>\u2705 gpt2\n\u2705 facebook/opt-125m\n\u2705 EleutherAI/pythia-410m\n\u274c GPT-2 (wrong case)\n\u274c opt-125m (missing organization)\n</code></pre>"},{"location":"troubleshooting/#issue-model-architecture-not-supported","title":"Issue: Model Architecture Not Supported","text":"<p>Symptoms:</p> <pre><code>Target modules not found\nLoRA cannot be applied\n</code></pre> <p>Solution:</p> <p>Check supported architectures:</p> <ul> <li>\u2705 GPT-2, GPT-Neo, GPT-J</li> <li>\u2705 OPT, BLOOM, LLaMA</li> <li>\u2705 T5, FLAN-T5</li> <li>\u274c BERT (requires different task type)</li> </ul> <p>Manual target module specification:</p> <p>Find module names:</p> <pre><code>for name, module in model.named_modules():\n    print(name)\n</code></pre> <p>Specify manually in setup_lora call.</p>"},{"location":"troubleshooting/#issue-tokenizer-warnings","title":"Issue: Tokenizer Warnings","text":"<p>Symptoms:</p> <pre><code>Token indices sequence length is longer than specified maximum\n</code></pre> <p>Solution:</p> <p>This is informational. To suppress:</p> <pre><code>Max sequence length: 512  # Match your typical text length\n</code></pre> <p>Or truncate more aggressively.</p>"},{"location":"troubleshooting/#upload-issues","title":"Upload Issues","text":""},{"location":"troubleshooting/#issue-authentication-failed","title":"Issue: Authentication Failed","text":"<p>Symptoms:</p> <pre><code>HTTPError: 401 Client Error: Unauthorized\n</code></pre> <p>Solutions:</p> <p>1. Check token: - Get new token: https://huggingface.co/settings/tokens - Ensure \"Write\" permission enabled</p> <p>2. Login via CLI:</p> <pre><code>huggingface-cli login\n</code></pre> <p>3. Set environment variable:</p> <pre><code>export HUGGING_FACE_HUB_TOKEN=\"hf_xxxxxxxxxxxxx\"\n</code></pre>"},{"location":"troubleshooting/#issue-repository-already-exists","title":"Issue: Repository Already Exists","text":"<p>Symptoms:</p> <pre><code>HTTPError: 409 Conflict\n</code></pre> <p>Solutions:</p> <p>1. Use existing repository:</p> <pre><code>Create new repository: no\n</code></pre> <p>2. Choose different name:</p> <pre><code>Repo name: username/new-model-name-v2\n</code></pre> <p>3. Delete old repository: - Go to repository settings on HuggingFace - Delete repository - Try again</p>"},{"location":"troubleshooting/#issue-upload-failed","title":"Issue: Upload Failed","text":"<p>Symptoms:</p> <pre><code>Error uploading model: Connection timeout\n</code></pre> <p>Solutions:</p> <p>1. Check internet connection</p> <p>2. Retry upload: The tool supports resumable uploads.</p> <p>3. Manual upload:</p> <pre><code>huggingface-cli upload username/repo-name ./finetuned_model\n</code></pre>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#issue-training-too-slow","title":"Issue: Training Too Slow","text":"<p>Symptoms:</p> <ul> <li>&lt; 1 iteration/second</li> <li>Hours for small datasets</li> </ul> <p>Solutions:</p> <p>1. Use GPU: Verify CUDA is enabled:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> <p>2. Reduce sequence length:</p> <pre><code>Max length: 256  # Instead of 512 or 1024\n</code></pre> <p>3. Increase batch size:</p> <pre><code>Batch size: 8  # If memory allows\n</code></pre> <p>4. Use mixed precision: Automatically enabled on GPU (FP16).</p> <p>5. Reduce dataset size for testing:</p> <pre><code>Number of samples: 1000\n</code></pre>"},{"location":"troubleshooting/#issue-poor-fine-tuning-results","title":"Issue: Poor Fine-tuning Results","text":"<p>Symptoms:</p> <ul> <li>ROUGE scores barely improve</li> <li>Model outputs generic responses</li> </ul> <p>Solutions:</p> <p>1. Increase model capacity:</p> <pre><code>LoRA r: 16  # Increase from 8\nLoRA alpha: 64  # Increase from 32\n</code></pre> <p>2. Train longer:</p> <pre><code>Epochs: 5  # Increase from 3\n</code></pre> <p>3. Check data quality: - Ensure diverse, high-quality examples - Remove duplicates - Balance dataset</p> <p>4. Use larger base model:</p> <pre><code>Model name: facebook/opt-1.3b  # Instead of opt-125m\n</code></pre> <p>5. Increase training data:</p> <pre><code>Number of samples: 20000  # Instead of 5000\n</code></pre>"},{"location":"troubleshooting/#debugging-tips","title":"Debugging Tips","text":""},{"location":"troubleshooting/#enable-verbose-logging","title":"Enable Verbose Logging","text":"<pre><code>export TRANSFORMERS_VERBOSITY=debug\nexport PEFT_VERBOSITY=debug\npython finetune_cli.py\n</code></pre>"},{"location":"troubleshooting/#monitor-gpu-usage","title":"Monitor GPU Usage","text":"<pre><code># Real-time monitoring\nwatch -n 1 nvidia-smi\n\n# Log to file\nnvidia-smi --query-gpu=timestamp,memory.used,memory.free,utilization.gpu --format=csv -l 1 &gt; gpu_log.csv\n</code></pre>"},{"location":"troubleshooting/#check-model-size","title":"Check Model Size","text":"<pre><code>from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nprint(f\"Params: {model.num_parameters() / 1e6:.1f}M\")\n</code></pre>"},{"location":"troubleshooting/#validate-dataset","title":"Validate Dataset","text":"<pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"your_dataset\")\nprint(dataset)\nprint(dataset[0])\nprint(dataset.column_names)\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If issues persist:</p> <ol> <li>Check logs: Review error messages carefully</li> <li>Search issues: GitHub Issues</li> <li>Open new issue: Include:</li> <li>Error message</li> <li>Configuration used</li> <li>System info (GPU, Python version)</li> <li>Steps to reproduce</li> </ol>"},{"location":"troubleshooting/#common-error-messages-reference","title":"Common Error Messages Reference","text":"Error Likely Cause Quick Fix CUDA OOM Memory exceeded Reduce batch size NaN loss Learning rate too high Reduce learning rate No text columns Column names not recognized Check dataset structure 401 Unauthorized Invalid HF token Re-login to HuggingFace Connection timeout Network issue Retry upload Module not found Missing dependency Reinstall requirements Model not found Wrong model name Check spelling"},{"location":"troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>Review Configuration Guide for optimization</li> <li>Check Examples for working configurations</li> <li>See API Reference for programmatic usage</li> </ul>"},{"location":"usage/","title":"Configuration Guide","text":"<p>This guide explains all configuration parameters and how to optimize them for your use case.</p>"},{"location":"usage/#lora-parameters","title":"LoRA Parameters","text":"<p>LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds trainable rank decomposition matrices to existing weights.</p>"},{"location":"usage/#rank-r","title":"Rank (r)","text":"<p>The rank of the low-rank matrices added to model layers.</p> <p>What it controls: The capacity of the adapter to learn new patterns.</p> <p>Values and Use Cases:</p> Rank Trainable Params Memory Use Case 4 ~0.1-0.5M Low Quick experiments, simple tasks 8 ~0.5-2M Moderate General purpose, balanced performance 16 ~2-8M Higher Complex tasks, significant adaptation 32 ~8-32M High Maximum quality, specialized domains <p>Choosing rank:</p> <pre><code># Simple classification or entity extraction\nr = 4\n\n# General text generation or summarization\nr = 8\n\n# Complex reasoning or domain adaptation\nr = 16\n\n# Specialized medical/legal/technical domains\nr = 32\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Higher rank: Better adaptation, handles complex patterns</li> <li>\u274c Higher rank: More memory, longer training, risk of overfitting</li> </ul>"},{"location":"usage/#alpha","title":"Alpha (\u03b1)","text":"<p>Scaling factor applied to LoRA weights.</p> <p>What it controls: The influence of LoRA updates relative to pre-trained weights.</p> <p>Formula: <code>scaling = alpha / r</code></p> <p>Recommended values:</p> <ul> <li>Standard: <code>alpha = 2 \u00d7 r</code> (e.g., r=8, alpha=16)</li> <li>Conservative: <code>alpha = r</code> (less aggressive updates)</li> <li>Aggressive: <code>alpha = 4 \u00d7 r</code> (stronger adaptation)</li> </ul> <p>Examples:</p> <pre><code># Conservative (maintains more of base model)\nr = 8, alpha = 8    # scaling = 1.0\n\n# Standard (recommended)\nr = 8, alpha = 16   # scaling = 2.0\n\n# Aggressive (stronger fine-tuning)\nr = 8, alpha = 32   # scaling = 4.0\n</code></pre> <p>When to adjust:</p> <ul> <li>Increase alpha if model isn't adapting enough</li> <li>Decrease alpha if model forgets pre-trained knowledge</li> </ul>"},{"location":"usage/#dropout","title":"Dropout","text":"<p>Regularization technique to prevent overfitting.</p> <p>What it controls: Probability of randomly disabling LoRA parameters during training.</p> <p>Values:</p> <ul> <li><code>0.0</code>: No dropout (risk of overfitting on small datasets)</li> <li><code>0.05</code>: Light regularization (large, diverse datasets)</li> <li><code>0.1</code>: Standard regularization (general purpose)</li> <li><code>0.2</code>: Heavy regularization (small or noisy datasets)</li> </ul> <p>Choosing dropout:</p> <pre><code># Large dataset (&gt; 50k samples), clean data\ndropout = 0.05\n\n# Medium dataset (5k-50k samples)\ndropout = 0.1\n\n# Small dataset (&lt; 5k samples) or noisy data\ndropout = 0.2\n</code></pre>"},{"location":"usage/#target-modules","title":"Target Modules","text":"<p>Specifies which model layers to apply LoRA to.</p> <p>Auto-detection: The tool automatically identifies optimal target modules.</p> <p>Common patterns:</p> <pre><code># Attention layers only (memory efficient)\n[\"q_proj\", \"v_proj\"]\n\n# Full attention (recommended)\n[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n# Attention + MLP (maximum adaptation)\n[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"fc1\", \"fc2\"]\n</code></pre> <p>Manual override (advanced):</p> <p>You can modify the code to specify custom targets:</p> <pre><code>target_modules = [\"q_proj\", \"v_proj\"]  # Attention queries and values only\n</code></pre>"},{"location":"usage/#training-parameters","title":"Training Parameters","text":""},{"location":"usage/#number-of-epochs","title":"Number of Epochs","text":"<p>Complete passes through the training dataset.</p> <p>Guidelines by dataset size:</p> Dataset Size Recommended Epochs &lt; 1,000 samples 5-10 1,000-5,000 3-7 5,000-50,000 3-5 &gt; 50,000 1-3 <p>Signs of:</p> <ul> <li>Underfitting: Loss still decreasing, ROUGE scores improving</li> <li> <p>Solution: Increase epochs</p> </li> <li> <p>Overfitting: Training loss decreases but validation loss increases</p> </li> <li>Solution: Decrease epochs, increase dropout</li> </ul>"},{"location":"usage/#batch-size","title":"Batch Size","text":"<p>Number of samples processed before updating model weights.</p> <p>Memory constraints:</p> GPU VRAM Model Size Max Batch Size 8GB Small (&lt; 500M params) 2-4 12GB Small-Medium 4-8 16GB Medium (1-3B params) 4-8 24GB Large (7B params) 8-16 <p>Effective batch size with gradient accumulation:</p> <pre><code># Config in training_args\nper_device_train_batch_size = 4\ngradient_accumulation_steps = 4\n# Effective batch size = 4 \u00d7 4 = 16\n</code></pre> <p>Tips:</p> <ul> <li>Start with batch_size=4 and adjust based on memory</li> <li>Smaller batches = more frequent updates, noisier gradients</li> <li>Larger batches = more stable gradients, better generalization</li> </ul>"},{"location":"usage/#learning-rate","title":"Learning Rate","text":"<p>Step size for weight updates.</p> <p>Common values:</p> Learning Rate Use Case 1e-5 Very conservative, large models 5e-5 Conservative, stable training 1e-4 Moderate, good starting point 2e-4 Standard for LoRA (recommended) 5e-4 Aggressive, small models 1e-3 Very aggressive, risk of instability <p>Learning rate schedule:</p> <p>The tool uses a constant learning rate. For advanced use, you can modify to use:</p> <ul> <li>Linear decay</li> <li>Cosine decay</li> <li>Warmup + decay</li> </ul> <p>Signs of poor learning rate:</p> <ul> <li>Too high: Loss oscillates or diverges, NaN values</li> <li> <p>Solution: Reduce by 50% (e.g., 2e-4 \u2192 1e-4)</p> </li> <li> <p>Too low: Loss decreases very slowly</p> </li> <li>Solution: Increase by 2x (e.g., 1e-4 \u2192 2e-4)</li> </ul>"},{"location":"usage/#maximum-sequence-length","title":"Maximum Sequence Length","text":"<p>Maximum number of tokens per training sample.</p> <p>Choosing max length:</p> <pre><code># Short texts (tweets, titles, Q&amp;A)\nmax_length = 128\n\n# Medium texts (paragraphs, summaries)\nmax_length = 512\n\n# Long texts (articles, documents)\nmax_length = 1024\n\n# Very long texts (research papers)\nmax_length = 2048\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Longer sequences: Better context understanding</li> <li>\u274c Longer sequences: Quadratic memory increase, slower training</li> </ul> <p>Memory impact:</p> <pre><code>Memory \u221d batch_size \u00d7 max_length\u00b2\n</code></pre> <p>Doubling max_length quadruples memory usage!</p>"},{"location":"usage/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"usage/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Simulate larger batch sizes without more memory.</p> <p>In the code (line 222):</p> <pre><code>gradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n</code></pre> <p>Calculation:</p> <pre><code>Effective Batch Size = batch_size \u00d7 gradient_accumulation_steps \u00d7 num_gpus\n</code></pre>"},{"location":"usage/#mixed-precision-fp16","title":"Mixed Precision (FP16)","text":"<p>Use 16-bit floating point for faster training and less memory.</p> <p>Automatically enabled when CUDA is available:</p> <pre><code>fp16 = self.device == \"cuda\"  # Line 228\n</code></pre> <p>Benefits:</p> <ul> <li>50% less memory</li> <li>2-3x faster training</li> <li>Minimal accuracy loss</li> </ul>"},{"location":"usage/#model-quantization","title":"Model Quantization","text":"<p>For very large models, you can enable quantization:</p> <pre><code># Add to model loading (line 59)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_8bit=True,  # Quantize to 8-bit\n    device_map=\"auto\"\n)\n</code></pre>"},{"location":"usage/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"usage/#recipe-1-quick-experimentation","title":"Recipe 1: Quick Experimentation","text":"<pre><code>Samples: 1000\nMax Length: 256\nLoRA r: 4\nLoRA alpha: 16\nDropout: 0.1\nEpochs: 3\nBatch Size: 4\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Testing ideas, rapid iteration</p>"},{"location":"usage/#recipe-2-balanced-quality","title":"Recipe 2: Balanced Quality","text":"<pre><code>Samples: 10000\nMax Length: 512\nLoRA r: 8\nLoRA alpha: 32\nDropout: 0.1\nEpochs: 3\nBatch Size: 8\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Production models, general tasks</p>"},{"location":"usage/#recipe-3-maximum-quality","title":"Recipe 3: Maximum Quality","text":"<pre><code>Samples: 50000+\nMax Length: 1024\nLoRA r: 16\nLoRA alpha: 64\nDropout: 0.1\nEpochs: 3\nBatch Size: 8\nLearning Rate: 1e-4\n</code></pre> <p>Best for: Specialized domains, publication-quality results</p>"},{"location":"usage/#recipe-4-memory-constrained","title":"Recipe 4: Memory-Constrained","text":"<pre><code>Samples: 5000\nMax Length: 256\nLoRA r: 4\nLoRA alpha: 16\nDropout: 0.1\nEpochs: 5\nBatch Size: 2\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Limited GPU memory (&lt; 8GB)</p>"},{"location":"usage/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Start conservative: Use lower rank, smaller batch, fewer epochs</li> <li>Monitor metrics: Watch loss curves and ROUGE scores</li> <li>Iterate gradually: Increase one parameter at a time</li> <li>Save checkpoints: Keep best performing configurations</li> <li>Profile memory: Use <code>nvidia-smi</code> to track GPU usage</li> </ol>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>See practical Examples</li> <li>Understand Troubleshooting common issues</li> <li>Explore API Reference for programmatic usage</li> </ul>"},{"location":"vanilla_distillation/","title":"Knowledge Distillation Guide","text":"<p>Complete guide to Knowledge Distillation methods in the LLM Fine-Tuning Framework</p>"},{"location":"vanilla_distillation/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Overview</li> <li>Vanilla Distillation</li> <li>Feature Distillation</li> <li>Comparison Table</li> <li>Configuration Guide</li> <li>Use Cases</li> <li>Best Practices</li> <li>Examples</li> </ol>"},{"location":"vanilla_distillation/#overview","title":"\ud83c\udfaf Overview","text":"<p>Knowledge Distillation is a model compression technique where a smaller student model learns to mimic a larger, more powerful teacher model. This enables:</p> <ul> <li>Model Compression: Reduce model size by 2-10x</li> <li>Inference Speed: Faster predictions on resource-constrained devices</li> <li>Efficiency: Maintain most of the teacher's performance with fewer parameters</li> <li>Deployment: Enable edge deployment and mobile applications</li> </ul>"},{"location":"vanilla_distillation/#key-concepts","title":"Key Concepts","text":"Concept Description Teacher Model Large, pre-trained model with strong performance Student Model Smaller model that learns from teacher Knowledge Transfer Process of transferring teacher's knowledge to student Soft Targets Probability distributions (softer than hard labels) Temperature Controls softness of probability distributions"},{"location":"vanilla_distillation/#vanilla-distillation","title":"\ud83d\udd35 Vanilla Distillation","text":""},{"location":"vanilla_distillation/#overview_1","title":"Overview","text":"<p>Vanilla Distillation (also called Output Distillation) transfers knowledge through the teacher's output probability distributions.</p> <p>How It Works: 1. Teacher generates soft probability distributions using temperature scaling 2. Student learns to match these distributions via KL divergence 3. Combined loss: \u03b1 \u00d7 CrossEntropy + (1-\u03b1) \u00d7 KL_Divergence</p>"},{"location":"vanilla_distillation/#mathematical-formulation","title":"Mathematical Formulation","text":"<pre><code>Soft Targets: p_teacher = softmax(logits_teacher / T)\nStudent Predictions: p_student = log_softmax(logits_student / T)\n\nKL Loss = KL_div(p_student, p_teacher) \u00d7 T\u00b2\nTotal Loss = \u03b1 \u00d7 CE_loss + (1-\u03b1) \u00d7 KL_loss\n</code></pre> <p>Where: - T = Temperature (higher = softer distributions) - \u03b1 = Weight for standard cross-entropy loss - KL_div = Kullback-Leibler divergence</p>"},{"location":"vanilla_distillation/#advantages","title":"Advantages","text":"<p>\u2705 Simple: Only requires output logits \u2705 Effective: Proven to work well across domains \u2705 Fast: Minimal computational overhead \u2705 Universal: Works with any architecture  </p>"},{"location":"vanilla_distillation/#when-to-use","title":"When to Use","text":"<ul> <li>Model Compression: Reducing model size while maintaining performance</li> <li>Task Generalization: Teacher provides richer supervision than hard labels</li> <li>Transfer Learning: Transferring knowledge to a different architecture</li> <li>Edge Deployment: Creating smaller models for mobile/embedded devices</li> </ul>"},{"location":"vanilla_distillation/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Range Default Description <code>temperature</code> 1.0-10.0 2.0 Softness of probability distributions <code>alpha</code> 0.0-1.0 0.5 Weight for cross-entropy loss <p>Temperature Guide: - T = 1.0: Hard targets (no distillation benefit) - T = 2.0: Balanced softness (recommended) - T = 5.0: Very soft targets (for very different architectures) - T = 10.0: Maximum softness (rarely needed)</p> <p>Alpha Guide: - \u03b1 = 0.0: Pure distillation (no ground truth) - \u03b1 = 0.3: Mostly distillation (recommended for strong teachers) - \u03b1 = 0.5: Balanced (default, good starting point) - \u03b1 = 0.7: Mostly ground truth (weaker teachers) - \u03b1 = 1.0: No distillation (standard fine-tuning)</p>"},{"location":"vanilla_distillation/#example-usage","title":"Example Usage","text":"<pre><code>from finetune_cli import LLMFineTuner\n\n# Initialize\nfinetuner = LLMFineTuner(\"gpt2\", \"./distilled_model\")\n\n# Load student model\nfinetuner.load_model(method=\"vanilla_distillation\")\n\n# Load teacher model\nfinetuner.load_teacher_model(\"gpt2-medium\")\n\n# Setup vanilla distillation\nfinetuner.setup_vanilla_distillation(\n    temperature=2.0,\n    alpha=0.5\n)\n\n# Train\nfinetuner.train(dataset, num_epochs=3)\n</code></pre> <p>CLI Usage:</p> <pre><code>python finetune_cli.py\n\n# Select method: 4 (Vanilla Distillation)\n# Student model: gpt2\n# Teacher model: gpt2-medium\n# Temperature: 2.0\n# Alpha: 0.5\n</code></pre>"},{"location":"vanilla_distillation/#feature-distillation","title":"\ud83d\udd34 Feature Distillation","text":""},{"location":"vanilla_distillation/#overview_2","title":"Overview","text":"<p>Feature Distillation (also called Intermediate Layer Distillation) transfers knowledge through intermediate layer representations, not just final outputs.</p> <p>How It Works: 1. Extract hidden states from teacher's intermediate layers 2. Extract corresponding hidden states from student's layers 3. Minimize MSE between teacher and student representations 4. Combine with output distillation or standard loss</p>"},{"location":"vanilla_distillation/#mathematical-formulation_1","title":"Mathematical Formulation","text":"<pre><code>Feature Loss = (1/N) \u00d7 \u03a3 MSE(h_student[i], h_teacher[i])\n\nWhere:\n- h_student[i] = student's hidden states at layer i\n- h_teacher[i] = teacher's hidden states at layer i\n- N = number of layers being matched\n\nTotal Loss = \u03b1 \u00d7 CE_loss + (1-\u03b1) \u00d7 Feature_loss\n</code></pre>"},{"location":"vanilla_distillation/#advantages_1","title":"Advantages","text":"<p>\u2705 Richer Knowledge: Captures intermediate representations \u2705 Better Generalization: Learns feature hierarchies \u2705 Architectural Understanding: Transfers layer-wise knowledge \u2705 Improved Performance: Often outperforms vanilla distillation  </p>"},{"location":"vanilla_distillation/#when-to-use_1","title":"When to Use","text":"<ul> <li>Complex Tasks: Tasks requiring understanding of intermediate features</li> <li>Similar Architectures: Student and teacher have similar layer structures</li> <li>Maximum Performance: When you want the best possible student</li> <li>Feature Learning: When intermediate representations are important</li> </ul>"},{"location":"vanilla_distillation/#configuration-parameters_1","title":"Configuration Parameters","text":"Parameter Range Default Description <code>temperature</code> 1.0-10.0 2.0 For output distillation component <code>alpha</code> 0.0-1.0 0.3 Weight for CE loss (lower for feature distillation) <code>feature_layers</code> List[int] Auto Which layers to match <p>Layer Selection Strategies:</p> <ol> <li> <p>Auto (Default): Evenly spaced layers    <code>python    # If student has 12 layers, selects: [0, 3, 6, 9]    feature_layers = None  # Auto-detect</code></p> </li> <li> <p>All Layers: Maximum knowledge transfer    <code>python    feature_layers = list(range(12))  # All 12 layers</code></p> </li> <li> <p>Key Layers: Focus on important layers    <code>python    feature_layers = [0, 5, 11]  # First, middle, last</code></p> </li> <li> <p>Late Layers: Focus on high-level features    <code>python    feature_layers = [8, 9, 10, 11]  # Last 4 layers</code></p> </li> </ol>"},{"location":"vanilla_distillation/#example-usage_1","title":"Example Usage","text":"<pre><code>from finetune_cli import LLMFineTuner\n\n# Initialize\nfinetuner = LLMFineTuner(\"gpt2\", \"./distilled_model\")\n\n# Load models\nfinetuner.load_model(method=\"feature_distillation\")\nfinetuner.load_teacher_model(\"gpt2-large\")\n\n# Setup feature distillation\nfinetuner.setup_feature_distillation(\n    temperature=2.0,\n    alpha=0.3,  # Lower alpha for feature focus\n    feature_layers=[0, 3, 6, 9, 11]  # Key layers\n)\n\n# Train\nfinetuner.train(dataset, num_epochs=5)\n</code></pre> <p>CLI Usage:</p> <pre><code>python finetune_cli.py\n\n# Select method: 5 (Feature Distillation)\n# Student model: gpt2\n# Teacher model: gpt2-large\n# Temperature: 2.0\n# Alpha: 0.3\n</code></pre>"},{"location":"vanilla_distillation/#comparison-table","title":"\ud83d\udcca Comparison Table","text":"Aspect Vanilla Distillation Feature Distillation Complexity Low Medium Training Speed Fast Slower (2-3x overhead) Memory Usage Low Higher (stores hidden states) Performance Good Better Teacher-Student Gap Works with large gaps Better with similar architectures Hyperparameter Tuning Easier (2 params) More complex (3+ params) Use Case General compression Maximum performance Implementation Simple More involved"},{"location":"vanilla_distillation/#performance-comparison-typical","title":"Performance Comparison (Typical)","text":"Method Student Size Performance Retained Training Time Memory Vanilla Distillation 50% of teacher 85-90% 1x 1.2x Feature Distillation 50% of teacher 90-95% 2x 1.5x No Distillation 50% of teacher 75-80% 1x 1x"},{"location":"vanilla_distillation/#configuration-guide","title":"\u2699\ufe0f Configuration Guide","text":""},{"location":"vanilla_distillation/#choosing-temperature","title":"Choosing Temperature","text":"<p>General Guidelines:</p> Temperature Use Case Example 1.0-1.5 Similar architectures, small gap GPT-2 \u2192 GPT-2-small 2.0-3.0 Moderate gap (recommended) GPT-2-medium \u2192 GPT-2 4.0-6.0 Large gap GPT-2-large \u2192 GPT-2 7.0-10.0 Very different architectures BERT \u2192 DistilBERT <p>Tuning Strategy: 1. Start with T=2.0 2. If student struggles: increase to 3.0-4.0 3. If overfitting: decrease to 1.5 4. Monitor validation loss to find optimal value</p>"},{"location":"vanilla_distillation/#choosing-alpha","title":"Choosing Alpha","text":"<p>Decision Matrix:</p> Teacher Quality Task Difficulty Recommended Alpha Strong Easy 0.2-0.3 Strong Hard 0.4-0.5 Moderate Easy 0.5-0.6 Moderate Hard 0.6-0.7 Weak Any 0.7-0.9 <p>Rules of Thumb: - Strong teacher + abundant data: Lower alpha (0.2-0.4) - Weak teacher or noisy data: Higher alpha (0.6-0.8) - Balanced approach: \u03b1 = 0.5 - Feature distillation: Use 0.2-0.4 (focus on features)</p>"},{"location":"vanilla_distillation/#teacher-student-pairing","title":"Teacher-Student Pairing","text":"<p>Recommended Compression Ratios:</p> Teacher Student Compression Method Expected Performance GPT-2 Medium (355M) GPT-2 (124M) 2.9x Vanilla 85-90% GPT-2 Large (774M) GPT-2 Medium (355M) 2.2x Feature 90-95% GPT-2 XL (1.5B) GPT-2 Large (774M) 1.9x Feature 92-97% LLaMA-7B LLaMA-3B 2.3x Feature 88-93% <p>Key Principles: - Moderate gap (2-4x compression) works best - Too small gap: Minimal benefit - Too large gap: Performance drop - Architecture similarity: Important for feature distillation</p>"},{"location":"vanilla_distillation/#use-cases","title":"\ud83d\udca1 Use Cases","text":""},{"location":"vanilla_distillation/#use-case-1-mobile-deployment","title":"Use Case 1: Mobile Deployment","text":"<p>Scenario: Deploy GPT-2-large quality on mobile devices</p> <p>Solution: Vanilla Distillation</p> <pre><code># Teacher: GPT-2-large (774M params)\n# Student: GPT-2 (124M params)\n# Result: 6.2x smaller, 85% performance\nfinetuner.setup_vanilla_distillation(temperature=3.0, alpha=0.4)\n</code></pre> <p>Benefits: - 6x faster inference - Fits in mobile memory - Maintains most capabilities</p>"},{"location":"vanilla_distillation/#use-case-2-production-api","title":"Use Case 2: Production API","text":"<p>Scenario: Reduce inference costs for high-volume API</p> <p>Solution: Feature Distillation</p> <pre><code># Teacher: GPT-2-XL (1.5B params)\n# Student: GPT-2-large (774M params)\n# Result: 2x smaller, 95% performance\nfinetuner.setup_feature_distillation(\n    temperature=2.0,\n    alpha=0.3,\n    feature_layers=[0, 4, 8, 12, 16, 20, 23]\n)\n</code></pre> <p>Benefits: - 2x throughput - 50% cost reduction - Minimal quality loss</p>"},{"location":"vanilla_distillation/#use-case-3-edge-ai","title":"Use Case 3: Edge AI","text":"<p>Scenario: IoT device with 512MB RAM</p> <p>Solution: Aggressive Vanilla Distillation</p> <pre><code># Teacher: GPT-2-medium (355M)\n# Student: DistilGPT-2 (82M)\n# Result: 4.3x smaller, fits in 512MB\nfinetuner.setup_vanilla_distillation(temperature=4.0, alpha=0.5)\n</code></pre> <p>Benefits: - Runs on constrained hardware - Offline inference - 70-80% performance retained</p>"},{"location":"vanilla_distillation/#use-case-4-domain-adaptation","title":"Use Case 4: Domain Adaptation","text":"<p>Scenario: Transfer medical knowledge from large model</p> <p>Solution: Feature Distillation with domain data</p> <pre><code># Teacher: GPT-2-large fine-tuned on medical data\n# Student: GPT-2 trained with distillation\nfinetuner.setup_feature_distillation(\n    temperature=2.5,\n    alpha=0.4,\n    feature_layers=[0, 5, 11]  # Key medical knowledge layers\n)\n</code></pre> <p>Benefits: - Captures domain-specific representations - Smaller model with specialized knowledge - Better than training student from scratch</p>"},{"location":"vanilla_distillation/#best-practices","title":"\ud83c\udf93 Best Practices","text":""},{"location":"vanilla_distillation/#1-teacher-model-selection","title":"1. Teacher Model Selection","text":"<p>Do: - \u2705 Use a model 2-4x larger than student - \u2705 Ensure teacher is well-trained on target task - \u2705 Use similar architecture families when possible - \u2705 Verify teacher performance before distillation</p> <p>Don't: - \u274c Use poorly trained teachers (will transfer mistakes) - \u274c Use teachers with vastly different architectures for feature distillation - \u274c Skip teacher validation - \u274c Use compression ratios &gt; 10x</p>"},{"location":"vanilla_distillation/#2-hyperparameter-tuning","title":"2. Hyperparameter Tuning","text":"<p>Tuning Order: 1. Temperature: Start with 2.0, adjust based on loss convergence 2. Alpha: Start with 0.5, tune based on validation performance 3. Feature Layers: Start with auto-selection, refine if needed</p> <p>Grid Search Example:</p> <pre><code># Try these combinations\nconfigs = [\n    {'temp': 2.0, 'alpha': 0.3},\n    {'temp': 2.0, 'alpha': 0.5},\n    {'temp': 3.0, 'alpha': 0.4},\n    {'temp': 4.0, 'alpha': 0.5},\n]\n</code></pre>"},{"location":"vanilla_distillation/#3-training-strategy","title":"3. Training Strategy","text":"<p>Recommended Approach: 1. Pre-train student: Regular training first (optional) 2. Distillation: Apply distillation for refinement 3. Fine-tune: Additional epochs on hard examples 4. Evaluation: Test on held-out set</p> <p>Training Schedule:</p> <pre><code># Phase 1: Standard training (optional)\nfinetuner.train(dataset, epochs=3, method=\"standard\")\n\n# Phase 2: Distillation\nfinetuner.setup_vanilla_distillation(temp=2.0, alpha=0.5)\nfinetuner.train(dataset, epochs=5)\n\n# Phase 3: Fine-tuning\nfinetuner.train(hard_examples, epochs=2, learning_rate=1e-5)\n</code></pre>"},{"location":"vanilla_distillation/#4-validation-and-testing","title":"4. Validation and Testing","text":"<p>Metrics to Track: - Student Loss: Should converge below teacher's - KL Divergence: Should decrease over time - Task Performance: Compare student vs teacher on validation - Inference Speed: Measure actual speedup</p> <p>Validation Strategy:</p> <pre><code># Regular validation during training\nval_scores_student = evaluate(student_model, val_set)\nval_scores_teacher = evaluate(teacher_model, val_set)\n\nprint(f\"Performance retention: {val_scores_student / val_scores_teacher * 100:.1f}%\")\n</code></pre>"},{"location":"vanilla_distillation/#5-common-pitfalls","title":"5. Common Pitfalls","text":"Pitfall Symptom Solution Too high temperature Student loss doesn't converge Lower to 2.0-3.0 Too low alpha Ignores ground truth Increase to 0.5-0.7 Wrong layers Poor feature transfer Use auto-selection or key layers Undertrained teacher Student learns errors Train teacher thoroughly first Overfitting Good train, poor val Increase regularization, lower alpha"},{"location":"vanilla_distillation/#examples","title":"\ud83d\udcda Examples","text":""},{"location":"vanilla_distillation/#example-1-basic-vanilla-distillation","title":"Example 1: Basic Vanilla Distillation","text":"<pre><code>from finetune_cli import LLMFineTuner\n\n# Setup\nfinetuner = LLMFineTuner(\"gpt2\", \"./vanilla_distilled\")\n\n# Load models\nfinetuner.load_model(method=\"vanilla_distillation\")\nfinetuner.load_teacher_model(\"gpt2-medium\")\n\n# Load dataset\ndataset = finetuner.load_dataset_from_source(\"./data.jsonl\", num_samples=10000)\ntokenized, _ = finetuner.prepare_dataset(dataset, max_length=512)\n\n# Configure vanilla distillation\nfinetuner.setup_vanilla_distillation(\n    temperature=2.0,\n    alpha=0.5\n)\n\n# Train\nfinetuner.train(\n    tokenized,\n    num_epochs=5,\n    batch_size=8,\n    learning_rate=2e-4\n)\n\nprint(\"\u2705 Vanilla distillation complete!\")\n</code></pre>"},{"location":"vanilla_distillation/#example-2-advanced-feature-distillation","title":"Example 2: Advanced Feature Distillation","text":"<pre><code>from finetune_cli import LLMFineTuner\n\n# Setup\nfinetuner = LLMFineTuner(\"gpt2\", \"./feature_distilled\")\n\n# Load models\nfinetuner.load_model(method=\"feature_distillation\")\nfinetuner.load_teacher_model(\"gpt2-large\")\n\n# Load dataset\ndataset = finetuner.load_dataset_from_source(\n    \"wikitext\",\n    dataset_config=\"wikitext-2-raw-v1\",\n    num_samples=20000\n)\ntokenized, _ = finetuner.prepare_dataset(dataset, max_length=512)\n\n# Configure feature distillation\nfinetuner.setup_feature_distillation(\n    temperature=2.5,\n    alpha=0.3,  # Focus more on features\n    feature_layers=[0, 2, 4, 6, 8, 10, 11]  # Key layers\n)\n\n# Train with smaller learning rate\nfinetuner.train(\n    tokenized,\n    num_epochs=8,\n    batch_size=4,\n    learning_rate=1e-4  # Lower LR for feature matching\n)\n\nprint(\"\u2705 Feature distillation complete!\")\n</code></pre>"},{"location":"vanilla_distillation/#example-3-comparison-study","title":"Example 3: Comparison Study","text":"<pre><code>from finetune_cli import LLMFineTuner\n\n# Compare vanilla vs feature distillation\nmethods = [\n    (\"vanilla\", {\"temperature\": 2.0, \"alpha\": 0.5}),\n    (\"feature\", {\"temperature\": 2.0, \"alpha\": 0.3, \"feature_layers\": None})\n]\n\nresults = {}\n\nfor method_name, config in methods:\n    print(f\"\\n{'='*50}\")\n    print(f\"Training with {method_name} distillation\")\n    print(f\"{'='*50}\")\n\n    finetuner = LLMFineTuner(\"gpt2\", f\"./distilled_{method_name}\")\n    finetuner.load_model(method=f\"{method_name}_distillation\")\n    finetuner.load_teacher_model(\"gpt2-medium\")\n\n    # Setup distillation\n    if method_name == \"vanilla\":\n        finetuner.setup_vanilla_distillation(**config)\n    else:\n        finetuner.setup_feature_distillation(**config)\n\n    # Train and evaluate\n    finetuner.train(dataset, num_epochs=5)\n    scores = finetuner.benchmark(test_prompts, use_finetuned=True)\n    results[method_name] = scores\n\n# Compare results\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARISON: Vanilla vs Feature Distillation\")\nprint(\"=\"*70)\nfor metric in results[\"vanilla\"]:\n    vanilla_score = results[\"vanilla\"][metric]\n    feature_score = results[\"feature\"][metric]\n    improvement = (feature_score - vanilla_score) / vanilla_score * 100\n    print(f\"{metric}: Vanilla={vanilla_score:.4f}, Feature={feature_score:.4f}, \u0394={improvement:+.2f}%\")\n</code></pre>"},{"location":"vanilla_distillation/#advanced-topics","title":"\ud83d\udd2c Advanced Topics","text":""},{"location":"vanilla_distillation/#multi-teacher-distillation","title":"Multi-Teacher Distillation","text":"<p>Combine knowledge from multiple teachers:</p> <pre><code># Pseudo-code (would require custom implementation)\nteacher1 = load_model(\"gpt2-medium\")\nteacher2 = load_model(\"opt-350m\")\n\n# Average teacher outputs\ncombined_logits = 0.5 * teacher1_logits + 0.5 * teacher2_logits\n</code></pre>"},{"location":"vanilla_distillation/#progressive-distillation","title":"Progressive Distillation","text":"<p>Distill in stages:</p> <pre><code># Stage 1: Large \u2192 Medium\ndistill(teacher=\"gpt2-xl\", student=\"gpt2-large\")\n\n# Stage 2: Medium \u2192 Small\ndistill(teacher=\"gpt2-large\", student=\"gpt2-medium\")\n\n# Stage 3: Small \u2192 Tiny\ndistill(teacher=\"gpt2-medium\", student=\"gpt2\")\n</code></pre>"},{"location":"vanilla_distillation/#task-specific-distillation","title":"Task-Specific Distillation","text":"<p>Focus on specific capabilities:</p> <pre><code># Distill only summarization capability\nfinetuner.setup_feature_distillation(\n    feature_layers=[8, 9, 10, 11],  # High-level reasoning layers\n    alpha=0.2  # Heavy distillation focus\n)\n</code></pre>"},{"location":"vanilla_distillation/#performance-benchmarks","title":"\ud83d\udcca Performance Benchmarks","text":"<p>Typical Results (GPT-2 family):</p> Teacher Student Method ROUGE-L Perplexity Compression GPT-2-medium GPT-2 None 0.28 45.2 2.9x GPT-2-medium GPT-2 Vanilla 0.34 38.7 2.9x GPT-2-medium GPT-2 Feature 0.36 36.4 2.9x GPT-2-large GPT-2-medium Vanilla 0.41 32.1 2.2x GPT-2-large GPT-2-medium Feature 0.43 29.8 2.2x"},{"location":"vanilla_distillation/#summary","title":"\ud83c\udfaf Summary","text":""},{"location":"vanilla_distillation/#quick-decision-guide","title":"Quick Decision Guide","text":"<p>Use Vanilla Distillation when: - \u2705 You need fast training - \u2705 Memory is limited - \u2705 Teacher and student are very different - \u2705 You want simple implementation</p> <p>Use Feature Distillation when: - \u2705 You want maximum performance - \u2705 Architectures are similar - \u2705 You can afford longer training - \u2705 Intermediate features matter</p>"},{"location":"vanilla_distillation/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Distillation reduces model size while maintaining performance</li> <li>Temperature controls softness (start with 2.0)</li> <li>Alpha balances distillation vs ground truth (start with 0.5 for vanilla, 0.3 for feature)</li> <li>Feature distillation often outperforms vanilla but is more complex</li> <li>Teacher quality is critical - train teachers thoroughly</li> </ol> <p>Last Updated: 2025-01-29 Version: 2.0.0 Framework: LLM Fine-Tuning CLI Extended Edition</p>"}]}