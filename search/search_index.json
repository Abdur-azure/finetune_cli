{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udd16 Finetune CLI","text":"<p>A comprehensive command-line tool for fine-tuning Large Language Models using LoRA (Low-Rank Adaptation), with automatic ROUGE benchmarking and HuggingFace integration.</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>This tool simplifies the process of fine-tuning large language models by providing an interactive CLI interface with built-in benchmarking capabilities. Whether you're working with local datasets or HuggingFace repositories, this tool handles the complexity of LoRA configuration, training, and evaluation.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfaf LoRA Fine-tuning: Efficient parameter-efficient fine-tuning with automatic target module detection</li> <li>\ud83d\udcca Auto-benchmarking: ROUGE score comparison before and after training to measure improvements</li> <li>\ud83d\udd0d Smart Dataset Loading: Automatically detects text columns and handles multiple data formats</li> <li>\ud83d\udcc1 Flexible Data Sources: Support for local files (JSON, JSONL, CSV, TXT) and HuggingFace datasets</li> <li>\ud83c\udf9b\ufe0f Selective Loading: Load specific files from large repositories to optimize memory usage</li> <li>\ud83d\ude80 HuggingFace Integration: Push fine-tuned models directly to HuggingFace Hub</li> <li>\ud83e\udde0 Auto-detection: Automatically identifies target modules for any model architecture</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install dependencies\npip install -r requirements.txt\n\n# Run the interactive CLI\npython finetune_cli.py\n</code></pre> <p>The tool will guide you through:</p> <ol> <li>Model selection from HuggingFace</li> <li>Dataset loading and preparation</li> <li>Pre-training benchmark</li> <li>LoRA configuration</li> <li>Training process</li> <li>Post-training evaluation</li> <li>Optional upload to HuggingFace Hub</li> </ol>"},{"location":"#why-use-this-tool","title":"Why Use This Tool?","text":"<ul> <li>Simplified Workflow: No need to write complex training scripts</li> <li>Best Practices Built-in: Automatically handles tokenization, padding, and data collation</li> <li>Memory Efficient: LoRA reduces memory requirements significantly</li> <li>Reproducible: Consistent configuration and benchmarking across experiments</li> <li>Educational: Learn fine-tuning concepts through interactive prompts</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the Installation Guide to set up your environment, then follow the Usage Guide to start fine-tuning your first model.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Installation: Setup instructions and prerequisites</li> <li>Usage Guide: Detailed walkthrough of all features</li> <li>Configuration: Understanding LoRA parameters and training settings</li> <li>API Reference: Technical documentation of core classes and methods</li> <li>Examples: Common use cases and recipes</li> <li>Troubleshooting: Solutions to common issues</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>This tool is actively maintained and open for contributions. If you encounter any issues or have suggestions, please open an issue on GitHub.</p>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete reference for the <code>LLMFineTuner</code> class and its methods.</p>"},{"location":"api/#llmfinetuner-class","title":"LLMFineTuner Class","text":"<p>The main class for fine-tuning language models with LoRA.</p>"},{"location":"api/#constructor","title":"Constructor","text":"<pre><code>LLMFineTuner(model_name: str, output_dir: str = \"./finetuned_model\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>model_name</code> (str): HuggingFace model identifier (e.g., \"gpt2\", \"facebook/opt-125m\")</li> <li><code>output_dir</code> (str, optional): Directory to save fine-tuned model. Default: \"./finetuned_model\"</li> </ul> <p>Attributes:</p> <ul> <li><code>model_name</code> (str): Name of the base model</li> <li><code>output_dir</code> (str): Output directory path</li> <li><code>device</code> (str): Device for training (\"cuda\" or \"cpu\")</li> <li><code>tokenizer</code> (AutoTokenizer): HuggingFace tokenizer instance</li> <li><code>model</code> (AutoModelForCausalLM): Base model instance</li> <li><code>peft_model</code> (PeftModel): LoRA-adapted model instance</li> </ul> <p>Example:</p> <pre><code>from finetune_cli import LLMFineTuner\n\nfinetuner = LLMFineTuner(\n    model_name=\"gpt2\",\n    output_dir=\"./my_model\"\n)\n</code></pre>"},{"location":"api/#methods","title":"Methods","text":""},{"location":"api/#load_model","title":"load_model()","text":"<p>Load the base model and tokenizer from HuggingFace.</p> <pre><code>def load_model() -&gt; None\n</code></pre> <p>Returns: None</p> <p>Side Effects:</p> <ul> <li>Initializes <code>self.tokenizer</code></li> <li>Initializes <code>self.model</code></li> <li>Sets pad_token if not present</li> </ul> <p>Example:</p> <pre><code>finetuner = LLMFineTuner(\"gpt2\")\nfinetuner.load_model()\n</code></pre> <p>Notes:</p> <ul> <li>Automatically uses FP16 on CUDA devices</li> <li>Sets device_map=\"auto\" for multi-GPU support</li> <li>Uses low_cpu_mem_usage for efficient loading</li> </ul>"},{"location":"api/#load_dataset_from_source","title":"load_dataset_from_source()","text":"<p>Load dataset from local file or HuggingFace Hub.</p> <pre><code>def load_dataset_from_source(\n    dataset_source: str,\n    dataset_config: Optional[str] = None,\n    split: str = \"train\",\n    num_samples: Optional[int] = None,\n    data_files: Optional[str] = None\n) -&gt; Dataset\n</code></pre> <p>Parameters:</p> <ul> <li><code>dataset_source</code> (str): Local file path or HuggingFace dataset name</li> <li><code>dataset_config</code> (str, optional): Dataset configuration/subset name</li> <li><code>split</code> (str): Dataset split to load. Default: \"train\"</li> <li><code>num_samples</code> (int, optional): Limit number of samples to load</li> <li><code>data_files</code> (str, optional): Specific files to load from repository</li> </ul> <p>Returns: <code>Dataset</code> object</p> <p>Supported Formats:</p> <ul> <li>Local: <code>.json</code>, <code>.jsonl</code>, <code>.csv</code>, <code>.txt</code></li> <li>HuggingFace: Any public dataset</li> </ul> <p>Example:</p> <pre><code># Load local file\ndataset = finetuner.load_dataset_from_source(\n    dataset_source=\"./data.jsonl\",\n    num_samples=1000\n)\n\n# Load HuggingFace dataset\ndataset = finetuner.load_dataset_from_source(\n    dataset_source=\"wikitext\",\n    dataset_config=\"wikitext-2-raw-v1\",\n    split=\"train\",\n    num_samples=5000\n)\n\n# Load specific file from large repo\ndataset = finetuner.load_dataset_from_source(\n    dataset_source=\"HuggingFaceH4/ultrachat_200k\",\n    data_files=\"data/train_sft-00000-of-00004.parquet\",\n    num_samples=2000\n)\n</code></pre>"},{"location":"api/#detect_text_columns","title":"detect_text_columns()","text":"<p>Automatically detect text columns in a dataset.</p> <pre><code>def detect_text_columns(dataset: Dataset) -&gt; List[str]\n</code></pre> <p>Parameters:</p> <ul> <li><code>dataset</code> (Dataset): Dataset to analyze</li> </ul> <p>Returns: List of column names containing text data</p> <p>Detection Strategy:</p> <ol> <li>Checks for common text column names</li> <li>Inspects data types of columns</li> <li>Returns all string-type columns</li> </ol> <p>Common Names Detected:</p> <ul> <li>text, content, input, output</li> <li>prompt, response, instruction</li> <li>question, answer</li> </ul> <p>Example:</p> <pre><code>dataset = finetuner.load_dataset_from_source(\"./data.jsonl\")\ntext_cols = finetuner.detect_text_columns(dataset)\nprint(f\"Found columns: {text_cols}\")\n# Output: Found columns: ['prompt', 'response']\n</code></pre>"},{"location":"api/#prepare_dataset","title":"prepare_dataset()","text":"<p>Tokenize and prepare dataset for training.</p> <pre><code>def prepare_dataset(\n    dataset: Dataset,\n    text_columns: Optional[List[str]] = None,\n    max_length: int = 512\n) -&gt; Tuple[Dataset, List[str]]\n</code></pre> <p>Parameters:</p> <ul> <li><code>dataset</code> (Dataset): Raw dataset to prepare</li> <li><code>text_columns</code> (List[str], optional): Columns to use. Auto-detects if None</li> <li><code>max_length</code> (int): Maximum sequence length. Default: 512</li> </ul> <p>Returns: Tuple of (tokenized_dataset, text_columns_used)</p> <p>Processing Steps:</p> <ol> <li>Auto-detects text columns if not provided</li> <li>Combines multiple columns if present</li> <li>Tokenizes with truncation and padding</li> <li>Removes original columns</li> </ol> <p>Example:</p> <pre><code>dataset = finetuner.load_dataset_from_source(\"./data.jsonl\")\n\n# Auto-detect columns\ntokenized, cols = finetuner.prepare_dataset(dataset, max_length=512)\n\n# Manual column specification\ntokenized, cols = finetuner.prepare_dataset(\n    dataset,\n    text_columns=[\"prompt\", \"response\"],\n    max_length=256\n)\n</code></pre>"},{"location":"api/#get_target_modules","title":"get_target_modules()","text":"<p>Automatically detect target modules for LoRA based on model architecture.</p> <pre><code>def get_target_modules() -&gt; Union[List[str], str]\n</code></pre> <p>Returns: List of module names or \"all-linear\" as fallback</p> <p>Detection Patterns:</p> <ul> <li>GPT-2 style: <code>[\"c_attn\", \"c_proj\"]</code></li> <li>Transformer style: <code>[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]</code></li> <li>Alternative: <code>[\"query\", \"value\", \"key\", \"dense\"]</code></li> </ul> <p>Example:</p> <pre><code>finetuner.load_model()\nmodules = finetuner.get_target_modules()\nprint(f\"Detected modules: {modules}\")\n# Output: Detected modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj']\n</code></pre> <p>Notes:</p> <ul> <li>Automatically called by <code>setup_lora()</code></li> <li>Fallback to \"all-linear\" if no pattern matches</li> </ul>"},{"location":"api/#setup_lora","title":"setup_lora()","text":"<p>Configure and apply LoRA to the model.</p> <pre><code>def setup_lora(\n    r: int = 8,\n    lora_alpha: int = 32,\n    lora_dropout: float = 0.1,\n    target_modules: Optional[List[str]] = None\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>r</code> (int): LoRA rank. Default: 8</li> <li><code>lora_alpha</code> (int): LoRA alpha scaling. Default: 32</li> <li><code>lora_dropout</code> (float): Dropout probability. Default: 0.1</li> <li><code>target_modules</code> (List[str], optional): Modules to apply LoRA. Auto-detects if None</li> </ul> <p>Returns: None</p> <p>Side Effects:</p> <ul> <li>Creates <code>self.peft_model</code> with LoRA adapters</li> <li>Prints trainable parameter statistics</li> </ul> <p>Example:</p> <pre><code>finetuner.load_model()\n\n# Default configuration\nfinetuner.setup_lora()\n\n# Custom configuration\nfinetuner.setup_lora(\n    r=16,\n    lora_alpha=64,\n    lora_dropout=0.15,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n</code></pre> <p>Output:</p> <pre><code>trainable params: 294,912 || all params: 124,439,808 || trainable%: 0.2370\n</code></pre>"},{"location":"api/#train","title":"train()","text":"<p>Train the model with LoRA adapters.</p> <pre><code>def train(\n    train_dataset: Dataset,\n    num_epochs: int = 3,\n    batch_size: int = 4,\n    learning_rate: float = 2e-4\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>train_dataset</code> (Dataset): Tokenized training dataset</li> <li><code>num_epochs</code> (int): Number of training epochs. Default: 3</li> <li><code>batch_size</code> (int): Per-device batch size. Default: 4</li> <li><code>learning_rate</code> (float): Learning rate. Default: 2e-4</li> </ul> <p>Returns: None</p> <p>Side Effects:</p> <ul> <li>Trains the model</li> <li>Saves checkpoints to output_dir</li> <li>Saves final model and tokenizer</li> </ul> <p>Training Configuration:</p> <ul> <li>Gradient accumulation: 4 steps</li> <li>FP16: Enabled on CUDA</li> <li>Logging: Every 10 steps</li> <li>Save strategy: Per epoch</li> </ul> <p>Example:</p> <pre><code>finetuner.load_model()\ndataset = finetuner.load_dataset_from_source(\"./data.jsonl\")\ntokenized, _ = finetuner.prepare_dataset(dataset)\nfinetuner.setup_lora()\n\nfinetuner.train(\n    train_dataset=tokenized,\n    num_epochs=3,\n    batch_size=8,\n    learning_rate=2e-4\n)\n</code></pre>"},{"location":"api/#benchmark","title":"benchmark()","text":"<p>Benchmark model performance using ROUGE scores.</p> <pre><code>def benchmark(\n    test_prompts: List[str],\n    use_finetuned: bool = False\n) -&gt; Dict[str, float]\n</code></pre> <p>Parameters:</p> <ul> <li><code>test_prompts</code> (List[str]): List of prompts to evaluate</li> <li><code>use_finetuned</code> (bool): Use fine-tuned model. Default: False (uses base model)</li> </ul> <p>Returns: Dictionary with ROUGE scores</p> <p>Metrics Computed:</p> <ul> <li>ROUGE-1: Unigram overlap</li> <li>ROUGE-2: Bigram overlap</li> <li>ROUGE-L: Longest common subsequence</li> </ul> <p>Example:</p> <pre><code>test_prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks.\",\n    \"Define artificial intelligence.\"\n]\n\n# Benchmark base model\nbase_scores = finetuner.benchmark(test_prompts, use_finetuned=False)\n\n# After training\nfinetuned_scores = finetuner.benchmark(test_prompts, use_finetuned=True)\n\n# Compare\nfor metric in base_scores:\n    improvement = (finetuned_scores[metric] - base_scores[metric]) / base_scores[metric] * 100\n    print(f\"{metric}: {improvement:+.2f}% improvement\")\n</code></pre> <p>Output Format:</p> <pre><code>{\n    'rouge1': 0.3245,\n    'rouge2': 0.2134,\n    'rougeL': 0.2987\n}\n</code></pre>"},{"location":"api/#upload_to_huggingface","title":"upload_to_huggingface()","text":"<p>Upload fine-tuned model to HuggingFace Hub.</p> <pre><code>def upload_to_huggingface(\n    repo_name: str,\n    token: Optional[str] = None,\n    create_new: bool = False,\n    private: bool = False\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>repo_name</code> (str): Repository name (format: \"username/repo-name\")</li> <li><code>token</code> (str, optional): HuggingFace API token. Uses cached login if None</li> <li><code>create_new</code> (bool): Create new repository. Default: False</li> <li><code>private</code> (bool): Make repository private. Default: False</li> </ul> <p>Returns: None</p> <p>Requirements:</p> <ul> <li>Valid HuggingFace token with write permissions</li> <li>Trained model in output_dir</li> </ul> <p>Example:</p> <pre><code># After training\nfinetuner.upload_to_huggingface(\n    repo_name=\"myusername/gpt2-finetuned\",\n    token=\"hf_xxxxxxxxxxxxx\",\n    create_new=True,\n    private=False\n)\n</code></pre> <p>Successful Upload:</p> <pre><code>\u2705 Model uploaded successfully to: https://huggingface.co/myusername/gpt2-finetuned\n</code></pre>"},{"location":"api/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/#complete-training-pipeline","title":"Complete Training Pipeline","text":"<pre><code>from finetune_cli import LLMFineTuner\n\n# Initialize\nfinetuner = LLMFineTuner(\"gpt2\", \"./my_model\")\n\n# Load model\nfinetuner.load_model()\n\n# Load and prepare data\ndataset = finetuner.load_dataset_from_source(\n    \"./data.jsonl\",\n    num_samples=5000\n)\ntokenized, cols = finetuner.prepare_dataset(dataset, max_length=512)\n\n# Pre-training benchmark\ntest_prompts = [\"Sample prompt 1\", \"Sample prompt 2\"]\nbase_scores = finetuner.benchmark(test_prompts, use_finetuned=False)\n\n# Setup LoRA\nfinetuner.setup_lora(r=8, lora_alpha=32, lora_dropout=0.1)\n\n# Train\nfinetuner.train(tokenized, num_epochs=3, batch_size=4, learning_rate=2e-4)\n\n# Post-training benchmark\nfinetuned_scores = finetuner.benchmark(test_prompts, use_finetuned=True)\n\n# Upload\nfinetuner.upload_to_huggingface(\n    \"username/model-name\",\n    create_new=True\n)\n</code></pre>"},{"location":"api/#loading-fine-tuned-model","title":"Loading Fine-tuned Model","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Load LoRA adapters\nmodel = PeftModel.from_pretrained(base_model, \"./my_model\")\n\n# Inference\nprompt = \"Hello, how are you?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>"},{"location":"api/#merging-adapters-optional","title":"Merging Adapters (Optional)","text":"<pre><code>from peft import PeftModel\n\n# Load model with adapters\nmodel = PeftModel.from_pretrained(base_model, \"./my_model\")\n\n# Merge adapters into base model\nmerged_model = model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(\"./merged_model\")\ntokenizer.save_pretrained(\"./merged_model\")\n</code></pre>"},{"location":"api/#helper-functions","title":"Helper Functions","text":""},{"location":"api/#get_user_input","title":"get_user_input()","text":"<p>Interactive prompt with optional default value.</p> <pre><code>def get_user_input(prompt: str, default: Optional[str] = None) -&gt; str\n</code></pre> <p>Example:</p> <pre><code>model_name = get_user_input(\"Enter model name\", \"gpt2\")\n# Prompt: Enter model name [gpt2]:\n</code></pre>"},{"location":"api/#type-definitions","title":"Type Definitions","text":"<pre><code>from typing import Optional, Dict, List, Tuple\nfrom datasets import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>The tool includes comprehensive error handling:</p> <pre><code>try:\n    finetuner.train(dataset)\nexcept RuntimeError as e:\n    if \"out of memory\" in str(e):\n        print(\"Reduce batch size or sequence length\")\n    raise\nexcept KeyboardInterrupt:\n    print(\"Training interrupted\")\n    sys.exit(0)\n</code></pre>"},{"location":"api/#next-steps","title":"Next Steps","text":"<ul> <li>See Usage Guide for CLI workflow</li> <li>Check Examples for practical use cases</li> <li>Review Configuration for parameter tuning</li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>This guide explains all configuration parameters and how to optimize them for your use case.</p>"},{"location":"configuration/#lora-parameters","title":"LoRA Parameters","text":"<p>LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds trainable rank decomposition matrices to existing weights.</p>"},{"location":"configuration/#rank-r","title":"Rank (r)","text":"<p>The rank of the low-rank matrices added to model layers.</p> <p>What it controls: The capacity of the adapter to learn new patterns.</p> <p>Values and Use Cases:</p> Rank Trainable Params Memory Use Case 4 ~0.1-0.5M Low Quick experiments, simple tasks 8 ~0.5-2M Moderate General purpose, balanced performance 16 ~2-8M Higher Complex tasks, significant adaptation 32 ~8-32M High Maximum quality, specialized domains <p>Choosing rank:</p> <pre><code># Simple classification or entity extraction\nr = 4\n\n# General text generation or summarization\nr = 8\n\n# Complex reasoning or domain adaptation\nr = 16\n\n# Specialized medical/legal/technical domains\nr = 32\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Higher rank: Better adaptation, handles complex patterns</li> <li>\u274c Higher rank: More memory, longer training, risk of overfitting</li> </ul>"},{"location":"configuration/#alpha","title":"Alpha (\u03b1)","text":"<p>Scaling factor applied to LoRA weights.</p> <p>What it controls: The influence of LoRA updates relative to pre-trained weights.</p> <p>Formula: <code>scaling = alpha / r</code></p> <p>Recommended values:</p> <ul> <li>Standard: <code>alpha = 2 \u00d7 r</code> (e.g., r=8, alpha=16)</li> <li>Conservative: <code>alpha = r</code> (less aggressive updates)</li> <li>Aggressive: <code>alpha = 4 \u00d7 r</code> (stronger adaptation)</li> </ul> <p>Examples:</p> <pre><code># Conservative (maintains more of base model)\nr = 8, alpha = 8    # scaling = 1.0\n\n# Standard (recommended)\nr = 8, alpha = 16   # scaling = 2.0\n\n# Aggressive (stronger fine-tuning)\nr = 8, alpha = 32   # scaling = 4.0\n</code></pre> <p>When to adjust:</p> <ul> <li>Increase alpha if model isn't adapting enough</li> <li>Decrease alpha if model forgets pre-trained knowledge</li> </ul>"},{"location":"configuration/#dropout","title":"Dropout","text":"<p>Regularization technique to prevent overfitting.</p> <p>What it controls: Probability of randomly disabling LoRA parameters during training.</p> <p>Values:</p> <ul> <li><code>0.0</code>: No dropout (risk of overfitting on small datasets)</li> <li><code>0.05</code>: Light regularization (large, diverse datasets)</li> <li><code>0.1</code>: Standard regularization (general purpose)</li> <li><code>0.2</code>: Heavy regularization (small or noisy datasets)</li> </ul> <p>Choosing dropout:</p> <pre><code># Large dataset (&gt; 50k samples), clean data\ndropout = 0.05\n\n# Medium dataset (5k-50k samples)\ndropout = 0.1\n\n# Small dataset (&lt; 5k samples) or noisy data\ndropout = 0.2\n</code></pre>"},{"location":"configuration/#target-modules","title":"Target Modules","text":"<p>Specifies which model layers to apply LoRA to.</p> <p>Auto-detection: The tool automatically identifies optimal target modules.</p> <p>Common patterns:</p> <pre><code># Attention layers only (memory efficient)\n[\"q_proj\", \"v_proj\"]\n\n# Full attention (recommended)\n[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n# Attention + MLP (maximum adaptation)\n[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"fc1\", \"fc2\"]\n</code></pre> <p>Manual override (advanced):</p> <p>You can modify the code to specify custom targets:</p> <pre><code>target_modules = [\"q_proj\", \"v_proj\"]  # Attention queries and values only\n</code></pre>"},{"location":"configuration/#training-parameters","title":"Training Parameters","text":""},{"location":"configuration/#number-of-epochs","title":"Number of Epochs","text":"<p>Complete passes through the training dataset.</p> <p>Guidelines by dataset size:</p> Dataset Size Recommended Epochs &lt; 1,000 samples 5-10 1,000-5,000 3-7 5,000-50,000 3-5 &gt; 50,000 1-3 <p>Signs of:</p> <ul> <li>Underfitting: Loss still decreasing, ROUGE scores improving</li> <li> <p>Solution: Increase epochs</p> </li> <li> <p>Overfitting: Training loss decreases but validation loss increases</p> </li> <li>Solution: Decrease epochs, increase dropout</li> </ul>"},{"location":"configuration/#batch-size","title":"Batch Size","text":"<p>Number of samples processed before updating model weights.</p> <p>Memory constraints:</p> GPU VRAM Model Size Max Batch Size 8GB Small (&lt; 500M params) 2-4 12GB Small-Medium 4-8 16GB Medium (1-3B params) 4-8 24GB Large (7B params) 8-16 <p>Effective batch size with gradient accumulation:</p> <pre><code># Config in training_args\nper_device_train_batch_size = 4\ngradient_accumulation_steps = 4\n# Effective batch size = 4 \u00d7 4 = 16\n</code></pre> <p>Tips:</p> <ul> <li>Start with batch_size=4 and adjust based on memory</li> <li>Smaller batches = more frequent updates, noisier gradients</li> <li>Larger batches = more stable gradients, better generalization</li> </ul>"},{"location":"configuration/#learning-rate","title":"Learning Rate","text":"<p>Step size for weight updates.</p> <p>Common values:</p> Learning Rate Use Case 1e-5 Very conservative, large models 5e-5 Conservative, stable training 1e-4 Moderate, good starting point 2e-4 Standard for LoRA (recommended) 5e-4 Aggressive, small models 1e-3 Very aggressive, risk of instability <p>Learning rate schedule:</p> <p>The tool uses a constant learning rate. For advanced use, you can modify to use:</p> <ul> <li>Linear decay</li> <li>Cosine decay</li> <li>Warmup + decay</li> </ul> <p>Signs of poor learning rate:</p> <ul> <li>Too high: Loss oscillates or diverges, NaN values</li> <li> <p>Solution: Reduce by 50% (e.g., 2e-4 \u2192 1e-4)</p> </li> <li> <p>Too low: Loss decreases very slowly</p> </li> <li>Solution: Increase by 2x (e.g., 1e-4 \u2192 2e-4)</li> </ul>"},{"location":"configuration/#maximum-sequence-length","title":"Maximum Sequence Length","text":"<p>Maximum number of tokens per training sample.</p> <p>Choosing max length:</p> <pre><code># Short texts (tweets, titles, Q&amp;A)\nmax_length = 128\n\n# Medium texts (paragraphs, summaries)\nmax_length = 512\n\n# Long texts (articles, documents)\nmax_length = 1024\n\n# Very long texts (research papers)\nmax_length = 2048\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Longer sequences: Better context understanding</li> <li>\u274c Longer sequences: Quadratic memory increase, slower training</li> </ul> <p>Memory impact:</p> <pre><code>Memory \u221d batch_size \u00d7 max_length\u00b2\n</code></pre> <p>Doubling max_length quadruples memory usage!</p>"},{"location":"configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"configuration/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Simulate larger batch sizes without more memory.</p> <p>In the code (line 222):</p> <pre><code>gradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n</code></pre> <p>Calculation:</p> <pre><code>Effective Batch Size = batch_size \u00d7 gradient_accumulation_steps \u00d7 num_gpus\n</code></pre>"},{"location":"configuration/#mixed-precision-fp16","title":"Mixed Precision (FP16)","text":"<p>Use 16-bit floating point for faster training and less memory.</p> <p>Automatically enabled when CUDA is available:</p> <pre><code>fp16 = self.device == \"cuda\"  # Line 228\n</code></pre> <p>Benefits:</p> <ul> <li>50% less memory</li> <li>2-3x faster training</li> <li>Minimal accuracy loss</li> </ul>"},{"location":"configuration/#model-quantization","title":"Model Quantization","text":"<p>For very large models, you can enable quantization:</p> <pre><code># Add to model loading (line 59)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_8bit=True,  # Quantize to 8-bit\n    device_map=\"auto\"\n)\n</code></pre>"},{"location":"configuration/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"configuration/#recipe-1-quick-experimentation","title":"Recipe 1: Quick Experimentation","text":"<pre><code>Samples: 1000\nMax Length: 256\nLoRA r: 4\nLoRA alpha: 16\nDropout: 0.1\nEpochs: 3\nBatch Size: 4\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Testing ideas, rapid iteration</p>"},{"location":"configuration/#recipe-2-balanced-quality","title":"Recipe 2: Balanced Quality","text":"<pre><code>Samples: 10000\nMax Length: 512\nLoRA r: 8\nLoRA alpha: 32\nDropout: 0.1\nEpochs: 3\nBatch Size: 8\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Production models, general tasks</p>"},{"location":"configuration/#recipe-3-maximum-quality","title":"Recipe 3: Maximum Quality","text":"<pre><code>Samples: 50000+\nMax Length: 1024\nLoRA r: 16\nLoRA alpha: 64\nDropout: 0.1\nEpochs: 3\nBatch Size: 8\nLearning Rate: 1e-4\n</code></pre> <p>Best for: Specialized domains, publication-quality results</p>"},{"location":"configuration/#recipe-4-memory-constrained","title":"Recipe 4: Memory-Constrained","text":"<pre><code>Samples: 5000\nMax Length: 256\nLoRA r: 4\nLoRA alpha: 16\nDropout: 0.1\nEpochs: 5\nBatch Size: 2\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Limited GPU memory (&lt; 8GB)</p>"},{"location":"configuration/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Start conservative: Use lower rank, smaller batch, fewer epochs</li> <li>Monitor metrics: Watch loss curves and ROUGE scores</li> <li>Iterate gradually: Increase one parameter at a time</li> <li>Save checkpoints: Keep best performing configurations</li> <li>Profile memory: Use <code>nvidia-smi</code> to track GPU usage</li> </ol>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>See practical Examples</li> <li>Understand Troubleshooting common issues</li> <li>Explore API Reference for programmatic usage</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Practical examples for common fine-tuning scenarios.</p>"},{"location":"examples/#example-1-fine-tune-gpt-2-on-custom-dialogue-data","title":"Example 1: Fine-tune GPT-2 on Custom Dialogue Data","text":""},{"location":"examples/#scenario","title":"Scenario","text":"<p>You have a JSONL file with conversational data and want to create a chatbot.</p>"},{"location":"examples/#dataset-format-dialoguejsonl","title":"Dataset Format (<code>dialogue.jsonl</code>)","text":"<pre><code>{\"prompt\": \"Hello, how are you?\", \"response\": \"I'm doing great! How can I help you today?\"}\n{\"prompt\": \"What's the weather like?\", \"response\": \"I don't have access to weather data, but you can check weather.com\"}\n</code></pre>"},{"location":"examples/#configuration","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: gpt2\nOutput directory: ./dialogue_model\nDataset path: ./dialogue.jsonl\nLimit samples: yes\nNumber of samples: 5000\nMax sequence length: 256\nLoRA r: 8\nLoRA alpha: 32\nLoRA dropout: 0.1\nEpochs: 5\nBatch size: 8\nLearning rate: 2e-4\nUpload to HuggingFace: no\n</code></pre>"},{"location":"examples/#expected-results","title":"Expected Results","text":"<pre><code>\ud83d\udcca PERFORMANCE COMPARISON\nMetric       Base Model      Fine-tuned      Improvement\nROUGE1       0.1823          0.3245          +78.03%\nROUGE2       0.0912          0.2134          +133.99%\nROUGEL       0.1645          0.2987          +81.58%\n</code></pre>"},{"location":"examples/#using-the-fine-tuned-model","title":"Using the Fine-tuned Model","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"./dialogue_model\")\n\n# Generate response\nprompt = \"Hello, how are you?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n</code></pre>"},{"location":"examples/#example-2-domain-adaptation-for-medical-text","title":"Example 2: Domain Adaptation for Medical Text","text":""},{"location":"examples/#scenario_1","title":"Scenario","text":"<p>Adapt a model to medical terminology and clinical notes.</p>"},{"location":"examples/#dataset","title":"Dataset","text":"<p>Using HuggingFace medical dataset.</p>"},{"location":"examples/#configuration_1","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: facebook/opt-350m\nOutput directory: ./medical_model\nDataset name: medical_meadow_medical_flashcards\nDataset config: default\nSplit: train\nLimit samples: yes\nNumber of samples: 20000\nMax sequence length: 512\nLoRA r: 16\nLoRA alpha: 64\nLoRA dropout: 0.15\nEpochs: 3\nBatch size: 4\nLearning rate: 1e-4\nUpload to HuggingFace: yes\nRepo name: myusername/opt-medical-350m\n</code></pre>"},{"location":"examples/#why-these-settings","title":"Why These Settings?","text":"<ul> <li>Higher rank (16): Medical domain requires learning specialized terminology</li> <li>Higher alpha (64): Stronger adaptation to domain-specific patterns</li> <li>More dropout (0.15): Medical text can be noisy, prevent overfitting</li> <li>Lower learning rate (1e-4): Conservative to preserve general knowledge</li> </ul>"},{"location":"examples/#example-3-summarization-task","title":"Example 3: Summarization Task","text":""},{"location":"examples/#scenario_2","title":"Scenario","text":"<p>Fine-tune for news article summarization.</p>"},{"location":"examples/#dataset-format-summariescsv","title":"Dataset Format (<code>summaries.csv</code>)","text":"<pre><code>article,summary\n\"Long article text here...\",\"Brief summary here...\"\n\"Another article...\",\"Another summary...\"\n</code></pre>"},{"location":"examples/#configuration_2","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: google/flan-t5-base\nOutput directory: ./summarization_model\nDataset path: ./summaries.csv\nLimit samples: yes\nNumber of samples: 15000\nMax sequence length: 1024\nLoRA r: 8\nLoRA alpha: 32\nLoRA dropout: 0.1\nEpochs: 3\nBatch size: 4\nLearning rate: 2e-4\n</code></pre>"},{"location":"examples/#data-preparation-tips","title":"Data Preparation Tips","text":"<p>The tool auto-detects columns. For best results:</p> <ol> <li>Name columns clearly: <code>article</code>, <code>text</code>, <code>summary</code>, <code>content</code></li> <li>Clean your data: Remove HTML tags, special characters</li> <li>Balance length: Keep articles similar length when possible</li> </ol>"},{"location":"examples/#example-4-code-generation","title":"Example 4: Code Generation","text":""},{"location":"examples/#scenario_3","title":"Scenario","text":"<p>Fine-tune on code examples for Python code generation.</p>"},{"location":"examples/#dataset_1","title":"Dataset","text":"<p>Using HuggingFace code dataset with specific file selection.</p>"},{"location":"examples/#configuration_3","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: Salesforce/codegen-350M-mono\nOutput directory: ./code_model\nDataset name: codeparrot/github-code\nDataset config: python\nLoad specific file: yes\nFile path: train-00000-of-00200.parquet\nNumber of samples: 10000\nMax sequence length: 512\nLoRA r: 8\nLoRA alpha: 32\nLoRA dropout: 0.05\nEpochs: 2\nBatch size: 8\nLearning rate: 2e-4\n</code></pre>"},{"location":"examples/#why-these-settings_1","title":"Why These Settings?","text":"<ul> <li>Lower dropout (0.05): Code has consistent structure, less noise</li> <li>Fewer epochs (2): Code datasets are large, less overfitting risk</li> <li>Specific file selection: Avoids loading entire 200-file dataset</li> </ul>"},{"location":"examples/#example-5-question-answering","title":"Example 5: Question Answering","text":""},{"location":"examples/#scenario_4","title":"Scenario","text":"<p>Fine-tune on SQuAD-style Q&amp;A data.</p>"},{"location":"examples/#configuration_4","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: distilbert-base-uncased\nOutput directory: ./qa_model\nDataset name: squad\nDataset config: plain_text\nSplit: train\nLimit samples: yes\nNumber of samples: 30000\nMax sequence length: 384\nLoRA r: 8\nLoRA alpha: 32\nLoRA dropout: 0.1\nEpochs: 3\nBatch size: 16\nLearning rate: 3e-4\n</code></pre>"},{"location":"examples/#inference-example","title":"Inference Example","text":"<pre><code>from transformers import pipeline\nfrom peft import PeftModel, AutoModelForQuestionAnswering\n\n# Load fine-tuned model\nbase_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\nmodel = PeftModel.from_pretrained(base_model, \"./qa_model\")\n\n# Create QA pipeline\nqa = pipeline(\"question-answering\", model=model, tokenizer=\"distilbert-base-uncased\")\n\n# Ask question\ncontext = \"Paris is the capital of France. It is known for the Eiffel Tower.\"\nquestion = \"What is the capital of France?\"\nresult = qa(question=question, context=context)\nprint(result['answer'])  # \"Paris\"\n</code></pre>"},{"location":"examples/#example-6-multi-language-fine-tuning","title":"Example 6: Multi-language Fine-tuning","text":""},{"location":"examples/#scenario_5","title":"Scenario","text":"<p>Fine-tune multilingual model for specific languages.</p>"},{"location":"examples/#configuration_5","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: xlm-roberta-base\nOutput directory: ./multilingual_model\nDataset name: Helsinki-NLP/tatoeba\nDataset config: eng-spa\nSplit: train\nLimit samples: yes\nNumber of samples: 25000\nMax sequence length: 128\nLoRA r: 16\nLoRA alpha: 32\nLoRA dropout: 0.1\nEpochs: 4\nBatch size: 8\nLearning rate: 2e-4\n</code></pre>"},{"location":"examples/#example-7-sentiment-analysis","title":"Example 7: Sentiment Analysis","text":""},{"location":"examples/#scenario_6","title":"Scenario","text":"<p>Adapt model for sentiment classification.</p>"},{"location":"examples/#dataset-format-sentimentjsonl","title":"Dataset Format (<code>sentiment.jsonl</code>)","text":"<pre><code>{\"text\": \"This product is amazing!\", \"sentiment\": \"positive\"}\n{\"text\": \"Terrible experience, would not recommend.\", \"sentiment\": \"negative\"}\n</code></pre>"},{"location":"examples/#configuration_6","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: roberta-base\nOutput directory: ./sentiment_model\nDataset path: ./sentiment.jsonl\nLimit samples: yes\nNumber of samples: 10000\nMax sequence length: 256\nLoRA r: 4\nLoRA alpha: 16\nLoRA dropout: 0.1\nEpochs: 5\nBatch size: 16\nLearning rate: 3e-4\n</code></pre>"},{"location":"examples/#why-these-settings_2","title":"Why These Settings?","text":"<ul> <li>Lower rank (4): Sentiment is relatively simple classification</li> <li>Higher batch size (16): Shorter sequences allow larger batches</li> <li>More epochs (5): Smaller dataset benefits from more iterations</li> </ul>"},{"location":"examples/#example-8-instruction-following","title":"Example 8: Instruction Following","text":""},{"location":"examples/#scenario_7","title":"Scenario","text":"<p>Fine-tune on instruction-response pairs.</p>"},{"location":"examples/#dataset_2","title":"Dataset","text":"<p>Using large instruction dataset with selective loading.</p>"},{"location":"examples/#configuration_7","title":"Configuration","text":"<pre><code>python finetune_cli.py\n\n# Interactive prompts:\nModel name: EleutherAI/pythia-410m\nOutput directory: ./instruction_model\nDataset name: HuggingFaceH4/ultrachat_200k\nLoad specific file: yes\nFile path: data/train_sft-00000-of-00004.parquet\nNumber of samples: 15000\nMax sequence length: 1024\nLoRA r: 16\nLoRA alpha: 64\nLoRA dropout: 0.1\nEpochs: 2\nBatch size: 4\nLearning rate: 1e-4\nUpload to HuggingFace: yes\nRepo name: myusername/pythia-instruction-410m\nPrivate: no\n</code></pre>"},{"location":"examples/#best-practices-from-examples","title":"Best Practices from Examples","text":""},{"location":"examples/#dataset-size-guidelines","title":"Dataset Size Guidelines","text":"<ul> <li>Experiments: 1,000-5,000 samples</li> <li>Development: 5,000-20,000 samples</li> <li>Production: 20,000+ samples</li> </ul>"},{"location":"examples/#model-selection-tips","title":"Model Selection Tips","text":"<ol> <li>Start small: Test with GPT-2 or OPT-125M</li> <li>Match architecture: Use decoder models (GPT) for generation</li> <li>Consider license: Check model licensing for commercial use</li> <li>Resource awareness: Larger models need more VRAM</li> </ol>"},{"location":"examples/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ol> <li>\u274c Training on too few samples (&lt; 500)</li> <li>\u274c Using max_length longer than needed (wastes memory)</li> <li>\u274c Setting rank too high for simple tasks (overfitting)</li> <li>\u274c Forgetting to validate results before uploading</li> <li>\u274c Not monitoring GPU memory usage</li> </ol>"},{"location":"examples/#performance-optimization","title":"Performance Optimization","text":"<pre><code># For faster iteration during development:\n- Use smaller model variants\n- Limit samples to 1000-5000\n- Reduce max_length\n- Use rank 4-8\n\n# For production quality:\n- Use full dataset\n- Increase rank to 16\n- Train for more epochs\n- Validate on held-out test set\n</code></pre>"},{"location":"examples/#recipes-summary","title":"Recipes Summary","text":"Use Case Model Rank Samples Max Length Chatbot GPT-2 8 5k 256 Medical OPT-350M 16 20k 512 Summarization FLAN-T5 8 15k 1024 Code Gen CodeGen 8 10k 512 QA DistilBERT 8 30k 384 Multilingual XLM-R 16 25k 128 Sentiment RoBERTa 4 10k 256 Instructions Pythia 16 15k 1024"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration Options</li> <li>Check Troubleshooting for common issues</li> <li>Review API Reference for programmatic usage</li> </ul>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the LLM Fine-Tuning CLI Tool, ensure you have the following:</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>GPU: CUDA-capable GPU (optional but highly recommended)</li> <li>Minimum 8GB VRAM for small models (GPT-2, OPT-125M)</li> <li>16GB+ VRAM recommended for larger models</li> <li>RAM: 16GB minimum, 32GB recommended</li> <li>Storage: 10GB+ free space for model downloads and checkpoints</li> </ul>"},{"location":"installation/#software-dependencies","title":"Software Dependencies","text":"<ul> <li>CUDA Toolkit 11.8+ (for GPU acceleration)</li> <li>pip package manager</li> <li>Git (for cloning the repository)</li> </ul>"},{"location":"installation/#installation-steps","title":"Installation Steps","text":""},{"location":"installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/Abdur-azure/finetune_cli.git\ncd finetune_cli\n</code></pre>"},{"location":"installation/#2-create-virtual-environment-recommended","title":"2. Create Virtual Environment (Recommended)","text":"<p>Using <code>venv</code>:</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre> <p>Using <code>conda</code>:</p> <pre><code>conda create -n finetune python=3.10\nconda activate finetune\n</code></pre>"},{"location":"installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>This will install all required packages:</p> <ul> <li><code>torch&gt;=2.0.0</code> - PyTorch deep learning framework</li> <li><code>transformers&gt;=4.35.0</code> - HuggingFace Transformers library</li> <li><code>datasets&gt;=2.14.0</code> - HuggingFace Datasets library</li> <li><code>peft&gt;=0.7.0</code> - Parameter-Efficient Fine-Tuning library</li> <li><code>rouge-score&gt;=0.1.2</code> - ROUGE metric for evaluation</li> <li><code>huggingface-hub&gt;=0.19.0</code> - HuggingFace Hub integration</li> <li><code>accelerate&gt;=0.24.0</code> - Distributed training utilities</li> <li>Additional utilities (tqdm, pandas, sentencepiece, protobuf)</li> </ul>"},{"location":"installation/#4-verify-installation","title":"4. Verify Installation","text":"<p>Test that everything is installed correctly:</p> <pre><code>python -c \"import torch; print(f'PyTorch: {torch.__version__}')\"\npython -c \"import transformers; print(f'Transformers: {transformers.__version__}')\"\npython -c \"import torch; print(f'CUDA Available: {torch.cuda.is_available()}')\"\n</code></pre> <p>Expected output:</p> <pre><code>PyTorch: 2.x.x\nTransformers: 4.x.x\nCUDA Available: True  # or False if no GPU\n</code></pre>"},{"location":"installation/#gpu-setup-optional-but-recommended","title":"GPU Setup (Optional but Recommended)","text":""},{"location":"installation/#nvidia-gpu-with-cuda","title":"NVIDIA GPU with CUDA","text":"<ol> <li>Install NVIDIA drivers for your GPU</li> <li>Install CUDA Toolkit (11.8 or later):</li> <li>Download from NVIDIA CUDA Downloads</li> <li>Verify CUDA installation:</li> </ol> <pre><code>nvcc --version\nnvidia-smi\n</code></pre>"},{"location":"installation/#pytorch-with-cuda","title":"PyTorch with CUDA","text":"<p>If you need a specific CUDA version, install PyTorch separately:</p> <pre><code># For CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# For CUDA 12.1\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"installation/#huggingface-setup-optional","title":"HuggingFace Setup (Optional)","text":"<p>To upload models to HuggingFace Hub, you'll need an account and token:</p> <ol> <li>Create account at HuggingFace</li> <li>Generate token at Settings &gt; Access Tokens</li> <li>Login via CLI:</li> </ol> <pre><code>huggingface-cli login\n</code></pre> <p>Or set environment variable:</p> <pre><code>export HUGGING_FACE_HUB_TOKEN=\"your_token_here\"\n</code></pre>"},{"location":"installation/#troubleshooting-installation","title":"Troubleshooting Installation","text":""},{"location":"installation/#issue-cuda-out-of-memory","title":"Issue: CUDA Out of Memory","text":"<p>Solution: Install CPU-only PyTorch or use a smaller model:</p> <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"installation/#issue-module-not-found-errors","title":"Issue: Module Not Found Errors","text":"<p>Solution: Upgrade pip and reinstall dependencies:</p> <pre><code>pip install --upgrade pip\npip install --upgrade -r requirements.txt\n</code></pre>"},{"location":"installation/#issue-slow-installation","title":"Issue: Slow Installation","text":"<p>Solution: Use a different PyPI mirror:</p> <pre><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre>"},{"location":"installation/#issue-permission-denied","title":"Issue: Permission Denied","text":"<p>Solution: Install in user space:</p> <pre><code>pip install --user -r requirements.txt\n</code></pre>"},{"location":"installation/#docker-installation-alternative","title":"Docker Installation (Alternative)","text":"<p>For a containerized environment:</p> <pre><code># Build image\ndocker build -t finetune-cli .\n\n# Run container\ndocker run -it --gpus all -v $(pwd):/workspace finetune-cli\n</code></pre> <p>Create a <code>Dockerfile</code>:</p> <pre><code>FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime\n\nWORKDIR /workspace\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"finetune_cli.py\"]\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete, proceed to the Usage Guide to start fine-tuning your first model.</p>"},{"location":"installation/#updating","title":"Updating","text":"<p>To update to the latest version:</p> <pre><code>git pull origin main\npip install --upgrade -r requirements.txt\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Solutions to common issues when using the fine-tuning tool.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#issue-cuda-not-available","title":"Issue: CUDA Not Available","text":"<p>Symptoms:</p> <pre><code>CUDA Available: False\nDevice: cpu\n</code></pre> <p>Causes:</p> <ol> <li>No NVIDIA GPU</li> <li>CUDA drivers not installed</li> <li>PyTorch installed without CUDA support</li> </ol> <p>Solutions:</p> <p>Check GPU:</p> <pre><code>nvidia-smi\n</code></pre> <p>Reinstall PyTorch with CUDA:</p> <pre><code>pip uninstall torch\npip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre>"},{"location":"troubleshooting/#issue-module-not-found-error","title":"Issue: Module Not Found Error","text":"<p>Symptoms:</p> <pre><code>ModuleNotFoundError: No module named 'peft'\n</code></pre> <p>Solution:</p> <pre><code>pip install --upgrade -r requirements.txt\n</code></pre> <p>If issue persists:</p> <pre><code>pip install peft transformers datasets --upgrade\n</code></pre>"},{"location":"troubleshooting/#issue-version-conflicts","title":"Issue: Version Conflicts","text":"<p>Symptoms:</p> <pre><code>ERROR: pip's dependency resolver...\n</code></pre> <p>Solution:</p> <p>Create fresh virtual environment:</p> <pre><code>python -m venv fresh_env\nsource fresh_env/bin/activate  # Windows: fresh_env\\Scripts\\activate\npip install -r requirements.txt\n</code></pre>"},{"location":"troubleshooting/#memory-issues","title":"Memory Issues","text":""},{"location":"troubleshooting/#issue-cuda-out-of-memory-oom","title":"Issue: CUDA Out of Memory (OOM)","text":"<p>Symptoms:</p> <pre><code>RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB\n</code></pre> <p>Solutions (try in order):</p> <p>1. Reduce batch size:</p> <pre><code>Batch size: 2  # Instead of 4 or 8\n</code></pre> <p>2. Reduce max sequence length:</p> <pre><code>Max length: 256  # Instead of 512 or 1024\n</code></pre> <p>3. Reduce LoRA rank:</p> <pre><code>LoRA r: 4  # Instead of 8 or 16\n</code></pre> <p>4. Limit number of samples:</p> <pre><code>Number of samples: 1000  # For testing\n</code></pre> <p>5. Enable gradient checkpointing (edit code):</p> <pre><code># Add to model loading (line 59)\nmodel.gradient_checkpointing_enable()\n</code></pre> <p>6. Use smaller model:</p> <pre><code>Model name: gpt2  # Instead of gpt2-medium or gpt2-large\n</code></pre> <p>Memory calculation formula:</p> <pre><code>Required VRAM \u2248 batch_size \u00d7 max_length\u00b2 \u00d7 model_size / 1e9 GB\n</code></pre>"},{"location":"troubleshooting/#issue-cpu-out-of-memory","title":"Issue: CPU Out of Memory","text":"<p>Symptoms:</p> <pre><code>MemoryError: Unable to allocate array\n</code></pre> <p>Solutions:</p> <p>1. Limit dataset size:</p> <pre><code>Number of samples: 5000\n</code></pre> <p>2. Use streaming for large datasets:</p> <p>Modify code to add streaming:</p> <pre><code>dataset = load_dataset(dataset_source, split=split, streaming=True)\n</code></pre> <p>3. Increase system swap:</p> <pre><code># Linux\nsudo fallocate -l 16G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n</code></pre>"},{"location":"troubleshooting/#training-issues","title":"Training Issues","text":""},{"location":"troubleshooting/#issue-loss-not-decreasing","title":"Issue: Loss Not Decreasing","text":"<p>Symptoms:</p> <pre><code>Epoch 1: Loss 3.45\nEpoch 2: Loss 3.44\nEpoch 3: Loss 3.43\n</code></pre> <p>Causes &amp; Solutions:</p> <p>1. Learning rate too low:</p> <pre><code>Learning rate: 2e-4  # Increase from 1e-4\n</code></pre> <p>2. Model frozen: Check that LoRA is properly applied:</p> <pre><code>\ud83c\udfaf Setting up LoRA configuration...\ntrainable params: X || all params: Y || trainable%: Z\n</code></pre> <p>3. Insufficient training:</p> <pre><code>Epochs: 5  # Increase from 3\nSamples: 10000  # Increase from 1000\n</code></pre> <p>4. Data quality issues: - Check dataset has meaningful text - Verify columns are correctly detected - Ensure no empty or null values</p>"},{"location":"troubleshooting/#issue-loss-diverging-nan","title":"Issue: Loss Diverging (NaN)","text":"<p>Symptoms:</p> <pre><code>Epoch 1: Loss 2.34\nEpoch 2: Loss 12.45\nEpoch 3: Loss NaN\n</code></pre> <p>Causes &amp; Solutions:</p> <p>1. Learning rate too high:</p> <pre><code>Learning rate: 1e-4  # Reduce from 5e-4 or 1e-3\n</code></pre> <p>2. Gradient explosion:</p> <p>Add gradient clipping (edit code):</p> <pre><code># In TrainingArguments (line 223)\nmax_grad_norm=1.0\n</code></pre> <p>3. Data issues: - Remove extreme outliers - Check for special characters causing issues - Normalize text inputs</p>"},{"location":"troubleshooting/#issue-overfitting","title":"Issue: Overfitting","text":"<p>Symptoms:</p> <ul> <li>Training loss decreases but validation would increase</li> <li>ROUGE scores decrease on new data</li> <li>Model outputs repetitive text</li> </ul> <p>Solutions:</p> <p>1. Increase dropout:</p> <pre><code>LoRA dropout: 0.2  # Increase from 0.1\n</code></pre> <p>2. Reduce epochs:</p> <pre><code>Epochs: 2  # Reduce from 5\n</code></pre> <p>3. Add more training data:</p> <pre><code>Number of samples: 20000  # Increase from 5000\n</code></pre> <p>4. Reduce model capacity:</p> <pre><code>LoRA r: 4  # Reduce from 8 or 16\n</code></pre> <p>5. Early stopping (edit code):</p> <pre><code># Add to TrainingArguments\nearly_stopping_patience=2\n</code></pre>"},{"location":"troubleshooting/#dataset-issues","title":"Dataset Issues","text":""},{"location":"troubleshooting/#issue-no-text-columns-detected","title":"Issue: No Text Columns Detected","text":"<p>Symptoms:</p> <pre><code>ValueError: No text columns found in dataset\n</code></pre> <p>Solutions:</p> <p>Check dataset structure:</p> <pre><code>print(dataset.column_names)\nprint(dataset[0])\n</code></pre> <p>Manual column specification (edit code around line 120):</p> <pre><code>text_columns = [\"my_text_column\", \"my_content_column\"]\ntokenized_dataset, _ = finetuner.prepare_dataset(dataset, text_columns=text_columns)\n</code></pre>"},{"location":"troubleshooting/#issue-dataset-too-large","title":"Issue: Dataset Too Large","text":"<p>Symptoms: - Slow loading - Memory issues - Long preprocessing</p> <p>Solutions:</p> <p>1. Use selective file loading:</p> <pre><code>Load specific file: yes\nFile path: train-00000-of-00100.parquet  # Load only one shard\n</code></pre> <p>2. Limit samples aggressively:</p> <pre><code>Number of samples: 5000\n</code></pre> <p>3. Use streaming mode:</p> <p>Modify dataset loading:</p> <pre><code>dataset = load_dataset(dataset_source, streaming=True)\ndataset = dataset.take(num_samples)\n</code></pre>"},{"location":"troubleshooting/#issue-column-names-not-recognized","title":"Issue: Column Names Not Recognized","text":"<p>Symptoms:</p> <p>Tool doesn't detect your text columns properly.</p> <p>Common column names recognized:</p> <ul> <li><code>text</code>, <code>content</code>, <code>input</code>, <code>output</code></li> <li><code>prompt</code>, <code>response</code>, <code>instruction</code></li> <li><code>question</code>, <code>answer</code>, <code>summary</code></li> </ul> <p>Solution:</p> <p>Rename your columns or modify detection logic (line 103):</p> <pre><code>common_names = ['text', 'content', 'your_column_name']\n</code></pre>"},{"location":"troubleshooting/#model-issues","title":"Model Issues","text":""},{"location":"troubleshooting/#issue-model-not-found","title":"Issue: Model Not Found","text":"<p>Symptoms:</p> <pre><code>OSError: Can't find model 'xyz'\n</code></pre> <p>Solution:</p> <p>Verify model exists: - Check HuggingFace Models - Ensure exact name match (case-sensitive)</p> <p>Common model names:</p> <pre><code>\u2705 gpt2\n\u2705 facebook/opt-125m\n\u2705 EleutherAI/pythia-410m\n\u274c GPT-2 (wrong case)\n\u274c opt-125m (missing organization)\n</code></pre>"},{"location":"troubleshooting/#issue-model-architecture-not-supported","title":"Issue: Model Architecture Not Supported","text":"<p>Symptoms:</p> <pre><code>Target modules not found\nLoRA cannot be applied\n</code></pre> <p>Solution:</p> <p>Check supported architectures:</p> <ul> <li>\u2705 GPT-2, GPT-Neo, GPT-J</li> <li>\u2705 OPT, BLOOM, LLaMA</li> <li>\u2705 T5, FLAN-T5</li> <li>\u274c BERT (requires different task type)</li> </ul> <p>Manual target module specification:</p> <p>Find module names:</p> <pre><code>for name, module in model.named_modules():\n    print(name)\n</code></pre> <p>Specify manually in setup_lora call.</p>"},{"location":"troubleshooting/#issue-tokenizer-warnings","title":"Issue: Tokenizer Warnings","text":"<p>Symptoms:</p> <pre><code>Token indices sequence length is longer than specified maximum\n</code></pre> <p>Solution:</p> <p>This is informational. To suppress:</p> <pre><code>Max sequence length: 512  # Match your typical text length\n</code></pre> <p>Or truncate more aggressively.</p>"},{"location":"troubleshooting/#upload-issues","title":"Upload Issues","text":""},{"location":"troubleshooting/#issue-authentication-failed","title":"Issue: Authentication Failed","text":"<p>Symptoms:</p> <pre><code>HTTPError: 401 Client Error: Unauthorized\n</code></pre> <p>Solutions:</p> <p>1. Check token: - Get new token: https://huggingface.co/settings/tokens - Ensure \"Write\" permission enabled</p> <p>2. Login via CLI:</p> <pre><code>huggingface-cli login\n</code></pre> <p>3. Set environment variable:</p> <pre><code>export HUGGING_FACE_HUB_TOKEN=\"hf_xxxxxxxxxxxxx\"\n</code></pre>"},{"location":"troubleshooting/#issue-repository-already-exists","title":"Issue: Repository Already Exists","text":"<p>Symptoms:</p> <pre><code>HTTPError: 409 Conflict\n</code></pre> <p>Solutions:</p> <p>1. Use existing repository:</p> <pre><code>Create new repository: no\n</code></pre> <p>2. Choose different name:</p> <pre><code>Repo name: username/new-model-name-v2\n</code></pre> <p>3. Delete old repository: - Go to repository settings on HuggingFace - Delete repository - Try again</p>"},{"location":"troubleshooting/#issue-upload-failed","title":"Issue: Upload Failed","text":"<p>Symptoms:</p> <pre><code>Error uploading model: Connection timeout\n</code></pre> <p>Solutions:</p> <p>1. Check internet connection</p> <p>2. Retry upload: The tool supports resumable uploads.</p> <p>3. Manual upload:</p> <pre><code>huggingface-cli upload username/repo-name ./finetuned_model\n</code></pre>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#issue-training-too-slow","title":"Issue: Training Too Slow","text":"<p>Symptoms:</p> <ul> <li>&lt; 1 iteration/second</li> <li>Hours for small datasets</li> </ul> <p>Solutions:</p> <p>1. Use GPU: Verify CUDA is enabled:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> <p>2. Reduce sequence length:</p> <pre><code>Max length: 256  # Instead of 512 or 1024\n</code></pre> <p>3. Increase batch size:</p> <pre><code>Batch size: 8  # If memory allows\n</code></pre> <p>4. Use mixed precision: Automatically enabled on GPU (FP16).</p> <p>5. Reduce dataset size for testing:</p> <pre><code>Number of samples: 1000\n</code></pre>"},{"location":"troubleshooting/#issue-poor-fine-tuning-results","title":"Issue: Poor Fine-tuning Results","text":"<p>Symptoms:</p> <ul> <li>ROUGE scores barely improve</li> <li>Model outputs generic responses</li> </ul> <p>Solutions:</p> <p>1. Increase model capacity:</p> <pre><code>LoRA r: 16  # Increase from 8\nLoRA alpha: 64  # Increase from 32\n</code></pre> <p>2. Train longer:</p> <pre><code>Epochs: 5  # Increase from 3\n</code></pre> <p>3. Check data quality: - Ensure diverse, high-quality examples - Remove duplicates - Balance dataset</p> <p>4. Use larger base model:</p> <pre><code>Model name: facebook/opt-1.3b  # Instead of opt-125m\n</code></pre> <p>5. Increase training data:</p> <pre><code>Number of samples: 20000  # Instead of 5000\n</code></pre>"},{"location":"troubleshooting/#debugging-tips","title":"Debugging Tips","text":""},{"location":"troubleshooting/#enable-verbose-logging","title":"Enable Verbose Logging","text":"<pre><code>export TRANSFORMERS_VERBOSITY=debug\nexport PEFT_VERBOSITY=debug\npython finetune_cli.py\n</code></pre>"},{"location":"troubleshooting/#monitor-gpu-usage","title":"Monitor GPU Usage","text":"<pre><code># Real-time monitoring\nwatch -n 1 nvidia-smi\n\n# Log to file\nnvidia-smi --query-gpu=timestamp,memory.used,memory.free,utilization.gpu --format=csv -l 1 &gt; gpu_log.csv\n</code></pre>"},{"location":"troubleshooting/#check-model-size","title":"Check Model Size","text":"<pre><code>from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nprint(f\"Params: {model.num_parameters() / 1e6:.1f}M\")\n</code></pre>"},{"location":"troubleshooting/#validate-dataset","title":"Validate Dataset","text":"<pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"your_dataset\")\nprint(dataset)\nprint(dataset[0])\nprint(dataset.column_names)\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If issues persist:</p> <ol> <li>Check logs: Review error messages carefully</li> <li>Search issues: GitHub Issues</li> <li>Open new issue: Include:</li> <li>Error message</li> <li>Configuration used</li> <li>System info (GPU, Python version)</li> <li>Steps to reproduce</li> </ol>"},{"location":"troubleshooting/#common-error-messages-reference","title":"Common Error Messages Reference","text":"Error Likely Cause Quick Fix CUDA OOM Memory exceeded Reduce batch size NaN loss Learning rate too high Reduce learning rate No text columns Column names not recognized Check dataset structure 401 Unauthorized Invalid HF token Re-login to HuggingFace Connection timeout Network issue Retry upload Module not found Missing dependency Reinstall requirements Model not found Wrong model name Check spelling"},{"location":"troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>Review Configuration Guide for optimization</li> <li>Check Examples for working configurations</li> <li>See API Reference for programmatic usage</li> </ul>"},{"location":"usage/","title":"Configuration Guide","text":"<p>This guide explains all configuration parameters and how to optimize them for your use case.</p>"},{"location":"usage/#lora-parameters","title":"LoRA Parameters","text":"<p>LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds trainable rank decomposition matrices to existing weights.</p>"},{"location":"usage/#rank-r","title":"Rank (r)","text":"<p>The rank of the low-rank matrices added to model layers.</p> <p>What it controls: The capacity of the adapter to learn new patterns.</p> <p>Values and Use Cases:</p> Rank Trainable Params Memory Use Case 4 ~0.1-0.5M Low Quick experiments, simple tasks 8 ~0.5-2M Moderate General purpose, balanced performance 16 ~2-8M Higher Complex tasks, significant adaptation 32 ~8-32M High Maximum quality, specialized domains <p>Choosing rank:</p> <pre><code># Simple classification or entity extraction\nr = 4\n\n# General text generation or summarization\nr = 8\n\n# Complex reasoning or domain adaptation\nr = 16\n\n# Specialized medical/legal/technical domains\nr = 32\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Higher rank: Better adaptation, handles complex patterns</li> <li>\u274c Higher rank: More memory, longer training, risk of overfitting</li> </ul>"},{"location":"usage/#alpha","title":"Alpha (\u03b1)","text":"<p>Scaling factor applied to LoRA weights.</p> <p>What it controls: The influence of LoRA updates relative to pre-trained weights.</p> <p>Formula: <code>scaling = alpha / r</code></p> <p>Recommended values:</p> <ul> <li>Standard: <code>alpha = 2 \u00d7 r</code> (e.g., r=8, alpha=16)</li> <li>Conservative: <code>alpha = r</code> (less aggressive updates)</li> <li>Aggressive: <code>alpha = 4 \u00d7 r</code> (stronger adaptation)</li> </ul> <p>Examples:</p> <pre><code># Conservative (maintains more of base model)\nr = 8, alpha = 8    # scaling = 1.0\n\n# Standard (recommended)\nr = 8, alpha = 16   # scaling = 2.0\n\n# Aggressive (stronger fine-tuning)\nr = 8, alpha = 32   # scaling = 4.0\n</code></pre> <p>When to adjust:</p> <ul> <li>Increase alpha if model isn't adapting enough</li> <li>Decrease alpha if model forgets pre-trained knowledge</li> </ul>"},{"location":"usage/#dropout","title":"Dropout","text":"<p>Regularization technique to prevent overfitting.</p> <p>What it controls: Probability of randomly disabling LoRA parameters during training.</p> <p>Values:</p> <ul> <li><code>0.0</code>: No dropout (risk of overfitting on small datasets)</li> <li><code>0.05</code>: Light regularization (large, diverse datasets)</li> <li><code>0.1</code>: Standard regularization (general purpose)</li> <li><code>0.2</code>: Heavy regularization (small or noisy datasets)</li> </ul> <p>Choosing dropout:</p> <pre><code># Large dataset (&gt; 50k samples), clean data\ndropout = 0.05\n\n# Medium dataset (5k-50k samples)\ndropout = 0.1\n\n# Small dataset (&lt; 5k samples) or noisy data\ndropout = 0.2\n</code></pre>"},{"location":"usage/#target-modules","title":"Target Modules","text":"<p>Specifies which model layers to apply LoRA to.</p> <p>Auto-detection: The tool automatically identifies optimal target modules.</p> <p>Common patterns:</p> <pre><code># Attention layers only (memory efficient)\n[\"q_proj\", \"v_proj\"]\n\n# Full attention (recommended)\n[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n# Attention + MLP (maximum adaptation)\n[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"fc1\", \"fc2\"]\n</code></pre> <p>Manual override (advanced):</p> <p>You can modify the code to specify custom targets:</p> <pre><code>target_modules = [\"q_proj\", \"v_proj\"]  # Attention queries and values only\n</code></pre>"},{"location":"usage/#training-parameters","title":"Training Parameters","text":""},{"location":"usage/#number-of-epochs","title":"Number of Epochs","text":"<p>Complete passes through the training dataset.</p> <p>Guidelines by dataset size:</p> Dataset Size Recommended Epochs &lt; 1,000 samples 5-10 1,000-5,000 3-7 5,000-50,000 3-5 &gt; 50,000 1-3 <p>Signs of:</p> <ul> <li>Underfitting: Loss still decreasing, ROUGE scores improving</li> <li> <p>Solution: Increase epochs</p> </li> <li> <p>Overfitting: Training loss decreases but validation loss increases</p> </li> <li>Solution: Decrease epochs, increase dropout</li> </ul>"},{"location":"usage/#batch-size","title":"Batch Size","text":"<p>Number of samples processed before updating model weights.</p> <p>Memory constraints:</p> GPU VRAM Model Size Max Batch Size 8GB Small (&lt; 500M params) 2-4 12GB Small-Medium 4-8 16GB Medium (1-3B params) 4-8 24GB Large (7B params) 8-16 <p>Effective batch size with gradient accumulation:</p> <pre><code># Config in training_args\nper_device_train_batch_size = 4\ngradient_accumulation_steps = 4\n# Effective batch size = 4 \u00d7 4 = 16\n</code></pre> <p>Tips:</p> <ul> <li>Start with batch_size=4 and adjust based on memory</li> <li>Smaller batches = more frequent updates, noisier gradients</li> <li>Larger batches = more stable gradients, better generalization</li> </ul>"},{"location":"usage/#learning-rate","title":"Learning Rate","text":"<p>Step size for weight updates.</p> <p>Common values:</p> Learning Rate Use Case 1e-5 Very conservative, large models 5e-5 Conservative, stable training 1e-4 Moderate, good starting point 2e-4 Standard for LoRA (recommended) 5e-4 Aggressive, small models 1e-3 Very aggressive, risk of instability <p>Learning rate schedule:</p> <p>The tool uses a constant learning rate. For advanced use, you can modify to use:</p> <ul> <li>Linear decay</li> <li>Cosine decay</li> <li>Warmup + decay</li> </ul> <p>Signs of poor learning rate:</p> <ul> <li>Too high: Loss oscillates or diverges, NaN values</li> <li> <p>Solution: Reduce by 50% (e.g., 2e-4 \u2192 1e-4)</p> </li> <li> <p>Too low: Loss decreases very slowly</p> </li> <li>Solution: Increase by 2x (e.g., 1e-4 \u2192 2e-4)</li> </ul>"},{"location":"usage/#maximum-sequence-length","title":"Maximum Sequence Length","text":"<p>Maximum number of tokens per training sample.</p> <p>Choosing max length:</p> <pre><code># Short texts (tweets, titles, Q&amp;A)\nmax_length = 128\n\n# Medium texts (paragraphs, summaries)\nmax_length = 512\n\n# Long texts (articles, documents)\nmax_length = 1024\n\n# Very long texts (research papers)\nmax_length = 2048\n</code></pre> <p>Trade-offs:</p> <ul> <li>\u2705 Longer sequences: Better context understanding</li> <li>\u274c Longer sequences: Quadratic memory increase, slower training</li> </ul> <p>Memory impact:</p> <pre><code>Memory \u221d batch_size \u00d7 max_length\u00b2\n</code></pre> <p>Doubling max_length quadruples memory usage!</p>"},{"location":"usage/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"usage/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Simulate larger batch sizes without more memory.</p> <p>In the code (line 222):</p> <pre><code>gradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n</code></pre> <p>Calculation:</p> <pre><code>Effective Batch Size = batch_size \u00d7 gradient_accumulation_steps \u00d7 num_gpus\n</code></pre>"},{"location":"usage/#mixed-precision-fp16","title":"Mixed Precision (FP16)","text":"<p>Use 16-bit floating point for faster training and less memory.</p> <p>Automatically enabled when CUDA is available:</p> <pre><code>fp16 = self.device == \"cuda\"  # Line 228\n</code></pre> <p>Benefits:</p> <ul> <li>50% less memory</li> <li>2-3x faster training</li> <li>Minimal accuracy loss</li> </ul>"},{"location":"usage/#model-quantization","title":"Model Quantization","text":"<p>For very large models, you can enable quantization:</p> <pre><code># Add to model loading (line 59)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_8bit=True,  # Quantize to 8-bit\n    device_map=\"auto\"\n)\n</code></pre>"},{"location":"usage/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"usage/#recipe-1-quick-experimentation","title":"Recipe 1: Quick Experimentation","text":"<pre><code>Samples: 1000\nMax Length: 256\nLoRA r: 4\nLoRA alpha: 16\nDropout: 0.1\nEpochs: 3\nBatch Size: 4\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Testing ideas, rapid iteration</p>"},{"location":"usage/#recipe-2-balanced-quality","title":"Recipe 2: Balanced Quality","text":"<pre><code>Samples: 10000\nMax Length: 512\nLoRA r: 8\nLoRA alpha: 32\nDropout: 0.1\nEpochs: 3\nBatch Size: 8\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Production models, general tasks</p>"},{"location":"usage/#recipe-3-maximum-quality","title":"Recipe 3: Maximum Quality","text":"<pre><code>Samples: 50000+\nMax Length: 1024\nLoRA r: 16\nLoRA alpha: 64\nDropout: 0.1\nEpochs: 3\nBatch Size: 8\nLearning Rate: 1e-4\n</code></pre> <p>Best for: Specialized domains, publication-quality results</p>"},{"location":"usage/#recipe-4-memory-constrained","title":"Recipe 4: Memory-Constrained","text":"<pre><code>Samples: 5000\nMax Length: 256\nLoRA r: 4\nLoRA alpha: 16\nDropout: 0.1\nEpochs: 5\nBatch Size: 2\nLearning Rate: 2e-4\n</code></pre> <p>Best for: Limited GPU memory (&lt; 8GB)</p>"},{"location":"usage/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Start conservative: Use lower rank, smaller batch, fewer epochs</li> <li>Monitor metrics: Watch loss curves and ROUGE scores</li> <li>Iterate gradually: Increase one parameter at a time</li> <li>Save checkpoints: Keep best performing configurations</li> <li>Profile memory: Use <code>nvidia-smi</code> to track GPU usage</li> </ol>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>See practical Examples</li> <li>Understand Troubleshooting common issues</li> <li>Explore API Reference for programmatic usage</li> </ul>"}]}